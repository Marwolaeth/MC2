# Evanescent h {#sec-evanescent-h}

```{r child="../starter.Rmd"}
```

On the radio, I once heard a baseball fanatic describing the path of a home run slammed just inside the left-field post. "Coming off the bat, the ball screamed upwards, passing five stories over the head of the first baseman and still gaining altitude. Then, somewhere over mid-left-field, gravity caught up with the ball, forcing it down faster and faster until it crashed into the cheap seats." A gripping image, perhaps, but wrong about the physics. Gravity doesn't suddenly catch hold of the ball. Even when upward bound, gravity influences the ball to the same extent as it does at the peak of the flight and as the ball falls back down. The vertical velocity of the ball is positive while climbing and negative on descent, but that velocity is steadily changing all through the flight: a smooth, almost linear numerical decrease in velocity from the time the ball leaves the bat to when it lands in the bleachers.

At each instant of time, the ball's vertical velocity has a numerical value in feet-per-second (L T^-1^). That value changes continuously. If $Z(t)$ is the height of the ball at time $t$, and $v_Z(t)$ is the vertical velocity at time $t$, then the slope function $${\cal D}_t Z(t) \equiv \frac{Z(t+h) - Z(t)}{h}$$ tells us the average velocity of the ball over a time interval of $h$.

The "average velocity" is a human construction because $h$ is a human choice. The reality pointed to by Newton is that at each instant in time the ball has a continuously changing velocity. Velocity is the physical reality: an instantaneous quantity. The "average velocity" is merely a concession to the way we measure the velocity, by recording the height at two different times and computing the difference in height divided by the difference in time.

Our measurement of the average velocity gets closer to the instantaneous velocity when we make the time interval $h$ smaller. But how small?

In the decades after Newton and Leibniz published their work inventing calculus, there was much philosophical concern about $h$. Skeptics pointed out the truth that either $h$ is zero or it is not. If it is zero, then the slope function would be $${\cal D}_t Z(t) \equiv \frac{Z(t+0) - Z(t)}{0} = \frac{Z(t) - Z(t)}{0} = \frac{0}{0}\ .$$ This $0/0$ is not a proper arithmetical quantity. On the other hand, if $h$ is not zero, then the slope function is a human construction involving $h$, not a natural one where $h$ plays no role.

```{r echo=FALSE}
#| label: fig-analysis
#| column: margin
#| fig-cap: "The title page of the most famous critique of calculus. The author is George Berkeley."
knitr::include_graphics("www/the_analyst_title_page.png")
```

Newton and Leibniz could not address skeptics except by using suggestive but as yet undefined words. These amount to saying "$h$ vanishes," or "$h$ is an infinitesimal," or "$h$ is a nascent quantity." 



Another good image of $h$ becoming as small as possible comes from University of Oxford mathematician Charles Lutwidge Dodgson (1832-1898).  In *Alice in Wonderland*, Dodgson introduced the character of the Cheshire Cat. 

```{r echo=FALSE}
#| label: fig-cheshire-cat
#| fig-cap: "Vanishing $h$ in the form of the Chesire Cat from *Alice in Wonderland*."
#| fig-cap-location: margin
#| fig-pos: "h"
knitr::include_graphics("www/Cheshire-cat.png")
```

The most famous of the early calculus skeptics, George Berkeley (1685-1753), used decried the vanishing $h$ as "notional shadowy Entities." Another of his critical words, "evanescent," has a more physical meaning. The dictionary definition of evanescent is, "tending to vanish like vapor."^[[Source](https://www.merriam-webster.com/dictionary/evanescent)] In Berkeley's era, the appearance and vanishing of a vapor might well have been a mysterious process and Berkeley used it as a criticism. 

It will therefore be a relief to the student starting out in calculus that Berkeley himself was not skeptical about the calculations used. He wrote in *The Analysis*, "I have no Controversy about your Conclusions, but only about your Logic and Method." 

Further security is provided by the development, by the 1850s, of a workable theory of vanishing $h$ was accepted by the mathematics community. Often, this theory---the "theory of limits"---is taught as the centerpiece of introductory calculus. But in my view, a theory demonstrating that a thing is true should always be subsidiary to the thing itself. For the uses of calculus, the theory of limits is of little direct use.

To give you a sense for the meaning of evanescent $h$ by way of a physical analogy, consider the process of painting.

For most people it's evident that a can of paint contains a liquid which somehow becomes a solid when brushed on a wall or other object. A better way to think about paint is as a "colloid," small solid particles suspended in a liquid. The purpose of the liquid is to keep the solid particles separate, so that the paint can flow and conform to the surface of the object being painted. Spreading out the liquid on the surface leads to rapid evaporation so that only the solid particles remain. Without the liquid, the particles no longer flow. They remain in place.

In the expression $${\cal D}_t p(t) \equiv \frac{p(t+h) - p(t)}{h}$$ 
the quantity $h$ is the liquid and each value of the input is a particle of solid. $h$ separates the solid particles. $p(t+h)$ and $p(t)$ are the values of the function at the slightly separated inputs. Since the inputs are held slightly apart, the function values $p(t+h)$ and $p(t)$ can be distinct and the difference between them, $p(t+h) - p(t)$, can be non-zero. The final result is to remove the $h$ from the difference, that is, divide $p(t+h)-p(t)$ by $h$. By slightly separating the input values, $h$ makes its contribution to the process, but in the end, $h$ evaporates just like the liquid in paint. That's why $h$ is evanescent: eventually it vanishes like vapor. 

## Evanescence algebraically

Let's look at a slope function using evanescent $h$. To start, we'll analyze $f(t) \equiv t^2$, one of our pattern-book functions. By definition, 
$${\cal D}_t f(t) \equiv \frac{f(t+h) - f(t)}{h}\ .$$
We can easily evaluate $f(t+h)$ symbolically:
$$f(t+h) \equiv (t+h)^2 = t^2 + 2 t h + h^2$$
Similarly, we can find the difference $f(t+h) - f(t)$. It is
$$f(t+h) - f(t) = f(t+h) - t^2 = 2 t h + h^2\ .$$
Notice that there is still some liquid (that is, $h$) in the difference. Now we let the difference start to dry, taking out the $h$ by dividing the difference by $h$:
$$\frac{f(t+h) - f(t)}{h} = \frac{2 t h + h^2}{h} = 2 t + h\ .$$
We've got something solid---$2 t$---along with a little bit of liquid $h$ that we can make as small as we wish. At this point, we let it evaporate entirely, to zero.

NOW INTRODUCE $\partial_t$ to replace ${\cal D}_t$.


::: {.column-margin}
Think of $\partial_t f()$ as the **name** of a function, the equivalent of "son" in the old-style of patronymic names where "Johnson" refers to the son of a man named John. The name $\partial_t f()$ tells us how the thus-named function is related to $f()$.
:::

The logic of calculus is not to abruptly set $h=0$, but to treat $h$ as a gradual, ***evanescent h*** that gently gets smaller and smaller. The type of slope function calculated with this (as yet undefined) evanescent h is called a ***derivative*** and corresponds to the instantaneous rate-of-change function. The process of constructing the derivative of a function $f(t)$ is called ***differentiation***. And to help us keep track of things, whenever we construct a derivative of $f(t)$ with respect to $t$, we will name the constructed function $\partial_t f(t)$. The small symbol $\partial$ is a reminder that we are looking only at a small difference, as opposed to the $\cal D$ of the slope operator where $h$ can potentially be not so small.

## A little bit of ...

Recall that the local rate of change of a function  can be written as a ratio of rise-over-run:
$$\partial_t f(t) \equiv \frac{f(t+h) - f(t)}{h}$$ 
where $h$ is the length of the "run." The idea of the ***instantaneous rate of change*** is to make $h$ as small as possible. 

In the very early days of calculus, the vanishing $h$ was described as "evanescent." (Dictionary definition: "tending to vanish like vapor."^[[Source](https://www.merriam-webster.com/dictionary/evanescent)]) 








```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-sig
#| column: margin
#| fig-cap: "The pattern-book sinusoid function. A vertical blue line marks the input $t=0$."
slice_plot(sin(t) ~ t, domain(t=c(-5, 2*pi))) %>%
  gf_labs(title="Sinusoid") %>%
  gf_vline(xintercept=0, color="dodgerblue", alpha=0.5)
```

The sinusoid (`sin()`) and the sigmoid (`pnorm()`). The computer can easily construct the slope functions for the sinusoid and sigmoid, which we'll call `Dsin()` and `Dsigma()` respectively.

```{r}
Dsin   <- makeFun((  sin(t+h) -   sin(t))/h ~ t, h=0.1)
Dsigma <- makeFun((pnorm(t+h) - pnorm(t))/h ~ t, h=0.1)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-sine
#| column: margin
#| fig-cap: "The pattern-book sigmoidal function."
slice_plot(pnorm(t) ~ t, domain(t=c(-5, 2*pi))) %>%
  gf_labs(title="Sigmoid") %>%
  gf_vline(xintercept=0, color="dodgerblue", alpha=0.5)
```

In the tilde expression handed to `makeFun()`, we've identified `t` as the name of the input and gave a "small" default value to the `h` parameter. But R recognizes that both `Dsin()` and `Dsigma()` are functions with two inputs, `t` and `h`, as you can see in the parenthesized argument list for the functions.

```{r}
Dsin
Dsigma
```

This is a nuisance, since when using the slope functions we will always need to think about `h`, a number that we'd like to describe simply as "small," but for which we always need to provide a numerical value. A surprisingly important question in the development of calculus is, "What can we do to avoid this nuisance?" To find out, let's look at `Dsin()` and `Dsigma()` for a range of values of `h`, as in @fig-sin-sig-many-h. 

```{r echo=FALSE, fig.show = "hold", warning=FALSE, message=FALSE}
#| label: fig-sin-sig-many-h
#| column: page-right
#| fig-cap: "The slope functions of the sinusoid and sigmoid. Each curve shows the slope function for a particular numerical choice of `h`. Both panels show $h=2, 1, 0.5, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001$."
rain <- rev(hcl.colors(12)[-(1:3)])
Psin <- slice_plot(Dsin(t, h=1) ~ t, domain(t=c(-5, 2*pi)), color=rain[1], label_text="h=1", label_x=0.56) %>%
  slice_plot(Dsin(t, h=2) ~ t, alpha = 1, label_text="h=2", label_x=.58, color=rain[1]) %>%
  slice_plot(Dsin(t, h=0.5) ~ t, alpha = 1, label_text="h=0.5", label_x=.54, color=rain[3]) %>%
  slice_plot(Dsin(t, h=0.1) ~ t, alpha = 1, label_text="h=0.1", label_x=.53, color=rain[4]) %>%
  slice_plot(Dsin(t, h=0.01) ~ t, alpha = 1, label_text="h=0.01", label_x=.51, color=rain[5]) %>%
  slice_plot(Dsin(t, h=0.001) ~ t, alpha = 1, color=rain[6], label_text="h=0.001", label_x=.5) %>%
  slice_plot(Dsin(t, h=0.0001) ~ t, alpha = 1, color=rain[7], label_text="h=0.0001", label_x=.49) %>%
  slice_plot(Dsin(t, h=0.00001) ~ t, alpha = 1, color=rain[8], label_text="h=0.00001", label_x=.48) %>%
  slice_plot(Dsin(t, h=0.000001) ~ t, alpha = 1, color=rain[9], label_text="h=0.000001", label_x=.44) %>%
  gf_labs(title="Slope functions of sinusoid") %>%
  gf_vline(xintercept=0, color="dodgerblue", alpha=0.5)
Psigma <- slice_plot(Dsigma(t, h=1) ~ t, domain(t=c(-5, 2*pi)), label_text="h=1", label_x=.28, color= rain[2]) %>%
  slice_plot(Dsigma(t, h=2) ~ t, alpha = 1, label_text="h=2", label_x=.24, color=rain[1]) %>%
  slice_plot(Dsigma(t, h=0.5) ~ t, alpha = 1, label_text="h=0.5", label_x=.54, color=rain[3]) %>%
  slice_plot(Dsigma(t, h=0.1) ~ t, alpha = 1, label_text="h=0.1", label_x=.53, color=rain[4]) %>%
  slice_plot(Dsigma(t, h=0.01) ~ t, alpha = 1, label_text="h=0.01", label_x=.51, color=rain[5]) %>%
  slice_plot(Dsigma(t, h=0.001) ~ t, alpha = 1, color=rain[6], label_text="h=0.001", label_x=.5) %>%
  slice_plot(Dsigma(t, h=0.0001) ~ t, alpha = 1, color=rain[7], label_text="h=0.0001", label_x=.49) %>%
  slice_plot(Dsigma(t, h=0.00001) ~ t, alpha = 1, color=rain[8], label_text="h=0.00001", label_x=.48) %>%
  slice_plot(Dsigma(t, h=0.000001) ~ t, alpha = 1, color=rain[9], label_text="h=0.000001", label_x=.44) %>%
  gf_labs(title="Slope functions of sigmoid") %>%
  gf_vline(xintercept=0, color="dodgerblue", alpha=0.5)
gridExtra::grid.arrange(Psin, Psigma, ncol=2)
```

Some observations from this numerical experiment:

1. As $h$ gets very small, the slope function doesn't depend on the exact value of $h$. As you can see in @fig-sin-sig-many-h, the graphs of the functions with the smallest $h$ (blue), with labels near the top of the graph) lie on top of one another.

    This will provide a way for us, eventually, to discard $h$ so that the slope function will not need an $h$ argument.
    
2. For small $h$, we have $\partial_t \sin(t) = \sin(t + \pi/2) = \cos(t)$. That is, taking the slope function of a sinusoid gives another sinusoid, shifted left by $\pi/2$ from the original. Or, in plain words, for small $h$ the cosine is the slope function of the sine.
3. For small $h$, we have $\partial_t \pnorm(t) = \dnorm(t)$. That is, for small $h$ the gaussian function is the slope function of the sigmoid $\dnorm()$ function.

You can confirm these last two statements by comparison with the original functions, especially the alignment of the peaks of the slope functions with respect to the peak of the sinusoid and the half-way point of the sigmoid. 

::: {.why  data-latex=""}
Here you use $t$ as the name of the input and $\partial_t$ as the notation for differentiation. Previously in this block you used $x$ as the input name and $\partial_x$ for differentiation. Are they the same? 

Mathematically, the name of the input makes no difference whatsoever. We could call it $x$ or $t$ or $y$ or Josephina. What's important is that the name be used consistently on the left and right sides of $\equiv$, and that the derivative symbol $\partial$ has a subscript that identifies ***the with-respect-to*** input. All these are the same statement mathematically: 

$$\partial_x\, x = 1\ \ \ \ \partial_t\, t = 1\ \ \ \ \partial_y\, y = 1\ \ \ \ \partial_\text{Josephina} \text{Josephina} = 1$$
Admittedly, the last one is hard to read.

When we look at derivatives of functions with multiple inputs we will need to be thoughtful about our choice of the with-respect-to input. But we want you to get used to seeing different input names used for differentiation. 

:::

Now consider the slope functions of the logarithm and exponential functions.

```{r echo=FALSE, fig.show = "hold", warning=FALSE, message=FALSE}
#| label: fig-log-exp-many-h
#| column: page-right
#| fig-cap: "The slope functions of the logarithm and exponential."
rain <- rev(hcl.colors(12)[-(1:3)])
Dlog <- makeFun((log(x+h) - log(x))/h ~ x)
Dexp <- makeFun((exp(x+h) - exp(x))/h ~ x)
Pa <- slice_plot(Dlog(t, h=1) ~ t, domain(t=c(0.01, 2)), color=rain[2], label_text="h=1", label_x=0.8) %>%
  slice_plot(Dlog(t, h=2) ~ t, domain(t=c(0.01, 2)), alpha = 1, label_text="h=2", label_x=.90, color=rain[1]) %>%
  slice_plot(Dlog(t, h=0.5) ~ t, domain(t=c(0.02, 2)), alpha = 1, label_text="h=0.5", label_x=.7, color=rain[3]) %>%
  slice_plot(Dlog(t, h=0.1) ~ t, alpha = 1, label_text="h=0.1", label_x=.6, color=rain[4]) %>%
  slice_plot(Dlog(t, h=0.01) ~ t, alpha = 1, label_text="h=0.01", label_x=.5, color=rain[5]) %>%
  slice_plot(Dlog(t, h=0.001) ~ t, domain(t=c(0.05, 2)), alpha = 1, color=rain[6], label_text="h=0.001", label_x=.4) %>%
  slice_plot(Dlog(t, h=0.0001) ~ t, domain(t=c(0.1, 2)), alpha = 1, color=rain[7], label_text="h=0.0001", label_x=.3) %>%
  slice_plot(Dlog(t, h=0.00001) ~ t, domain(t=c(0.125, 2)), alpha = 1, color=rain[8], label_text="h=0.00001", label_x=.2) %>%
  slice_plot(Dlog(t, h=0.000001) ~ t, domain(t=c(0.125, 2)),alpha = 1, color=rain[9], label_text="h=0.000001", label_x=.1) %>%
  gf_labs(title="Slope functions of logarithm") %>%
  gf_vline(xintercept=0, color="dodgerblue", alpha=0.5) %>%
  gf_lims(y=c(0,8))
Pb <- slice_plot(Dexp(t, h=1) ~ t, domain(t=c(-2, 2)), label_text="h=1", label_x=.4, color= rain[2]) %>%
  slice_plot(Dexp(t, h=2) ~ t, alpha = 1, label_text="h=2", label_x=.3, color=rain[1]) %>%
  slice_plot(Dexp(t, h=0.5) ~ t, alpha = 1, label_text="h=0.5", label_x=.5, color=rain[3]) %>%
  slice_plot(Dexp(t, h=0.1) ~ t, alpha = 1, label_text="h=0.1", label_x=.55, color=rain[4]) %>%
  slice_plot(Dexp(t, h=0.01) ~ t, alpha = 1, label_text="h=0.01", label_x=.6, color=rain[5]) %>%
  slice_plot(Dexp(t, h=0.001) ~ t, alpha = 1, color=rain[6], label_text="h=0.001", label_x=.65) %>%
  slice_plot(Dexp(t, h=0.0001) ~ t, alpha = 1, color=rain[7], label_text="h=0.0001", label_x=.75) %>%
  slice_plot(Dexp(t, h=0.00001) ~ t, alpha = 1, color=rain[8], label_text="h=0.00001", label_x=.85) %>%
  slice_plot(Dexp(t, h=0.000001) ~ t, alpha = 1, color=rain[9], label_text="h=0.000001", label_x=.9) %>%
  gf_labs(title="Slope functions of exponential") %>%
  gf_vline(xintercept=0, color="dodgerblue", alpha=0.5) %>%
  gf_lims(y=c(0,8))
gridExtra::grid.arrange(Pa, Pb, ncol=2)
```

These numerical experiments with the logarithm and exponential functions are more evidence that, as $h$ gets small, the slope function doesn't depend strongly on $h$. And, we find that:

- For small $h$, the slope function of the logarithm is a power-law function: $\partial_t \ln(t) = \frac{1}{t}$.
- For small $h$, the slope function of the exponential is the exponential itself: $\partial_t e^x = e^x$.

You can confirm these by evaluating the slope function of the exponential at $t=0$ and $t=1$, and the slope function of the logarithm at $t= 2, 1, 1/2, 1/4, 1/8.$

"Small" and "zero," although related, are different. In constructing a derivative, we use smaller and smaller $h$, but never zero. Let's see what happens if instead of evanescent h, we use zero h. For example, we can use the slope function `Dsin()` and `Dsigma()` that we created earlier. Setting $h$ to zero does not give a result that is the instantaneous rate of change of anything: 

```{r}
Dsin(t=1, h=0)
Dsigma(t=0, h=0)
```

In `NaN`, you can hear the echo of your fourth-grade teacher reminding you that it is illegal to divide by zero.

Think of evanescent $h$ as the vapor in the definition of "evanescent": "tending to vanish like vapor." This vapor is like the solvent in paint. You don't want the solvent once the paint is on the wall; wet paint is a nuisance. But getting the paint from the container to the wall absolutely needs the solvent.  

We used the solvent $h$ earlier in the chapter in the numerical experiments that led us to the derivatives of the pattern-book functions, for instance $\partial_x e^x = e^x$ or $\partial_x \sin(x) = \cos(x)$.  In Section @sec-h-free-rules, we'll construct an $h$-free theory of differentiation, reducing the process to a set of algebraic rules in which $h$ never appears. With this as our goal, let's continue using $h$ for a while to find some additional useful facts about derivatives. 


## Notations for differentiation

There are several traditional notations for differentiation of a function named $f()$. Here's a list of some of them, along with the name associated with each: 

- Leibnitz: $\frac{df}{dx}$
- Partial: $\frac{\partial f}{\partial x}$
- Euler: $D_x f$
- Newton (or "dot"): $\dot{f}$
- Lagrange (or "prime"): $f'$
- One-line: $\partial_x f$ (This hybrid of partial and Euler notation, will be the main differential notation used in this book.)

::: {.intheworld  data-latex=""}
It is a fact of mathematical and scientific life that varied notations are used for differentiation. This reflects historical precedence and, to be honest, nationalistic European politics of the 18th century. To make sense of mathematical writing in the many areas in which calculus is used, you have to recognize all of them for what they are. Your skill will be enhanced if you also memorize the names of the different styles. It's not all that different from the pattern in English of having multiple words for the same sort of object, for instance: car, automobile, junker, ride, wheels, crate, jalopy, limo, motor car, horseless carriage.  

In the days when carriages where pulled by horses, the phrase "horseless carriage" made a useful distinction. Today, when horses are rarely seen on the road, it makes sense to trim down the notation to its essentials: ~~horseless~~ **car**~~iage~~. Think of $\partial_x$ as this sort of minification of older notations.^[Yes, ["minification" is a word!](https://en.wikipedia.org/wiki/Minification_(programming))]   
:::

You have likely seen the $f'$ notation if you've studied calculus before. $f'$ is admirably concise but is only viable in a narrow circumstance: functions that take a single input. What $f'$ leaves out is a means to specify a crucial aspect of differentiation, the **with-respect-to input**. The general situation for differentiation involves functions of one or more variables, for example, $g(x, y, z)$. For such functions, you need to specify which is the with-respect-to input. So, for instance, we can differentiate $g()$ three different ways, each way incrementing one or another of the three inputs: 



$$\partial_x g(x, y, z) \equiv \frac{g(x+h, y, z) - g(x, y, z)}{h}$$

$$\partial_y g(x, y, z) \equiv \frac{g(x, y+h, z) - g(x, y, z)}{h}$$
$$\partial_z g(x, y, z) \equiv \frac{g(x, y, z+h) - g(x, y, z)}{h}$$ 


At this point in your studies, you haven't seen why you might choose to differentiate a function with respect to one input or another. That will come as you progress through calculus. But we want to set you up with a notation that won't narrow your options. This book mainly use2 the one-line notation, $\partial_x f$, but it means the same as the Leibnitz and Partial notations, which are much more widely used in textbooks. 
 

Both the Leibnitz and Partial notations are explicit in identifying the function and the with-respect-to input. For example, using the Partial differentiation notation, the three ways of differentiating our example function $g(x, y, z)$ are labeled : 

$$\frac{\partial f}{\partial x},\ \ \ \frac{\partial f}{\partial y},\ \ \text{and}\ \ \frac{\partial f}{\partial z}$$

Our R/mosaic computer differentiation is longer but explicit:
```r
g <- makeFun(__formula__ ~ x & y & z) # define a function
dx_g <- D(g(x, y, z) ~ x)
dy_g <- D(g(x, y, z) ~ y)
dz_g <- D(g(x, y, z) ~ z)
```
The names assigned to the result of the `D()` operator can be any names you like. What's nice about `dx_g` and the others is that it mimics the math notation $\partial_x g()$.

Notice that the R/mosaic operator for differentiation is named `D()` and that it is a function. It follows the same pattern as `makeFun()` or `slice_plot()` or `contour_plot()`: the first argument is a tilde expression, for instance `g(x, y, z) ~ x`, which identifies the mathematical function to work with (`g()`) and the name of the with-respect-to input to that function. The R/mosaic notation makes it clear that differentiation is an ***operation*** on a function. The `D()` operator takes a function as input and produces as output **another function**. We've seen similar behavior with, say, `slice_plot()`, which takes a function as input and produces graphics as output. Both `D()` and `slice_plot()` need to know the identity of the with-respect-to input as well as the function to work with. That's why both pieces of input are packaged into a tilde expression. 

::: {.why  data-latex=""}
We're calling R/mosaic `D()` an ***operator*** rather than a ***function***. The reason is purely for communication with other people. There are so many "functions" in a calculus course that we thought it would be helpful to distinguish between the kinds of functions that take quantities as input and produce a quantity as output and the functions that take a *function* as input and produce a *function* as output.^[It's pretty easy to see in an expression like $f(x,y)$ why we call $f()$ a function. But an expression like $3+2$ also involves a function of two inputs. We just write the name of the function (`+`) in between the two inputs. This is called ***infix*** notation.] Both sorts are called "functions" in R terminology. But a sentence like, "Differentiation is a function that takes a function as input and produces a function as output," true though it be, is dizzying. 
:::




