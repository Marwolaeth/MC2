# Constructing derivatives {#sec-computing-derivs}

```{r child="../starter.Rmd"}
```

This chapter shows you how to use the computer to construct the derivative of any function. As you will see, this is easy because the task of constructing derivatives is well suited to the computer. [There are functions where the derivative cannot be meaningfully defined. Examples are the absolute-value function or the Heaviside function which we introduced when discussing piecewise functions in Chapter @sec-piecewise-intro.  In Chapter @sec-cont-and-smooth we will consider the pathological cases and show you how to spot them at a glance.]{.aside} 

We'll demonstrate two methods:

1. ***Symbolic differentiation***, which transforms an algebraic formula for a function into a corresponding algebraic formula for the derivative.

2. ***Finite-difference*** methods that use a "small"---but not evanescent---$h$.

Later, in Chapter @sec-cont-and-smooth, you will learn the algorithms used by the computer in constructing a symbolic derivative. One reason for teaching you to do with paper and pencil the simpler sorts of problems that the computer does perfectly is similar to the reasons you learned basic arithmetic by hand even though a calculator can perform the task more reliably. Another reason to learn to carry out symbolic differentiation on your own is to enable you to follow textbook or classroom uses of formulas which often involve differentiation. 

## Why differentiate?

Before showing you the easy computer-based methods for constructing the derivative of a function, it's good to provide some motivation: Why is differentiation such a common operation in so many fields of study and application?

A primary reason lies in the laws of physics. In its modern form, calculus was invented specifically to be able to describe precisely the nature of motion. Newton's second law relating force to acceleration is built on derivatives. If the position of a moving object as a function of time is $x(t)$, then the velocity is $v(t) \equiv \partial_t x(t)$ and the acceleration is $a(t) \equiv \partial_t v(t) = \partial_{tt} x(t)$. Derivatives are also central to the expression of more modern forms of physics such as quantum theory and general relativity.

Many relationships encountered in the everyday or technical worlds are more understandable if framed in terms of derivatives. For instance,

- Electrical power is the rate of change with respect to time of electrical energy.
- Birth rate is one component of the rate of change with respect to time of population. (The others are the death rate and the rates immigration and emigration.)
- Interest, as in bank interest or credit card interest, is the rate of change with respect to time of assets. 
- Inflation is the rate of change with respect to time of prices.
- Disease incidence is one component of the rate of change with respect to time of disease prevalence. (The other components are death or recovery from disease.)
- Force is the rate of change with respect to position of energy.
- Deficit (as in spending deficits) is the change with respect to time of debt.

It's common to know something about one of these function-and-derivative pairs, but to need to translate this knowledge into the form of the other. Many modeling situations call for putting together different components of change to reveal how some other quantity of interest will change. For example, modeling the financial viability of retirement programs such as the US Social Security involves looking at the changing age structure of the population, the returns on investment, the changing cost of living, and so on. In Block @sec-dynamics-part, we'll use derivatives explicitly to construct models of systems, such as an outbreak of disease, with many changing parts.

Derivatives also play an important role in design. They play an important role in the construction and representation of smooth curves, such as a robot's track or the body of a car. (See Chapter @sec-splines.) Control systems that work to stabilize the flight of a plane or regulate the speed and spacing of cars are based on derivatives. The notion of "stability" itself is defined in terms of derivatives. (See Chapter @sec-equilibria.) Algorithms for optimizing design choices also often make use of derivatives. (See Chapter @sec-optimization-and-constraint.)

::: {.intheworld  data-latex=""}
Economics as a field makes considerable use of concepts of calculus---particularly first and second derivatives, the subjects of this Block---although the names used are peculiar to economics, for instance, "elasticity", "marginal returns" and "diminishing marginal returns."

The origins of modern economics, especially the theory of the free market, are attributed to a book published in 1776, *The Wealth of Nations*. The author, Adam Smith (1723-1790), lays out dozens of relationships between different quantities --- wages, labor, stock, interest, prices, profits, and coinage among others. Yet despite the invention of calculus a century before *Wealth of Nations*, the book does not make any use of calculus.

Consider this characteristic statement in *Wealth of Nations*:

>  *The market price of every particular commodity is regulated by the proportion between the quantity which is brought to market, and the demand of those who are willing to pay the natural price of the commodity.*

Without calculus and the ideas of functions and their derivatives, Smith was not able to think about prices in a modern way where price is shaped by demand and supply. Instead, for Smith, each item has a "natural price": a fixed quantity that depends on the amount of labor used to produce the item. Nowadays, we understand that productivity changes as new methods of production and new inventions are introduced. But Smith lived near the end of a centuries-long period of ***static*** economies. Transportation, agriculture, manufacture, population were all much as they had been for the past 500 years or longer. James Watt's steam engine was introduced only in 1776 and it would be decades before it was adapted to the myriad uses of steam power that characterized the 19th century. The cotton gin (1793), labor-saving agricultural machines such as the McCormick reaper (1831), the assembly line (1901), and the many other innovations of industry all lay in the future when Smith was writing *Wealth of Nations*. 


```{r echo=FALSE}
#| label: cournot-demand
#| fig-cap: "Demand as a *function* of price, as first published by Antoine-Augustin Cournot in 1836."
#| column: margin
knitr::include_graphics("www/cournot-demand-curve.png")
```


It took the industrial revolution and nearly a century of intellectual development before economics had to and could embrace the rapid changes in the production process. In this dynamical view, supply and demand are not seen as mere quantities, but as ***functions*** of which price is the major input. The tradition in economics is to use the word "curve" instead of "function," giving us the phrases "supply curve" and "demand curve." Many students starting out in economics conceive of supply and demand as fixed quantities. Making the transition from quantity to function, that is, between a single amount and a relationship between amounts, is a core challenge to those learning economics. 

:::




## Symbolic differentiation

The R/mosaic function `D()` takes a formula for a function and produces the derivative. It uses the same sort of tilde expression used by `makeFun()` or `contour_plot()` or the other R/mosaic tools.  For instance,

```{r}
D(t * sin(t) ~ t)
```

If you prefer, you can use `makeFun()` to define a function, then hand that function to `D()` for differentiation.

```{r}
myf <- makeFun(sqrt(y * pnorm(1 + x^2, mean=2, sd=3)) ~ x & y)
dx_myf <- D(myf(x, y) ~ x, y=3)
dx_myf
```

In the right side of the tilde expression handed off to `D()` names the with-respect-to input. This is similar to the tilde expressions used in plotting, which name the inputs that form the graphics domain. But it contrasts with the tilde expressions in `makeFun()`, where the right-hand side specifies the order in which you want the inputs to appear.

::: {.example data-latex=""}
Needless to say, `D()` knows the rules for the derivatives of the pattern-book functions introduced in Section @sec-d-pattern-book. For instance,

```{r}
D(sin(t) ~ t)
D(log(x) ~ x)
D(exp(x) ~ x)
D(x^2 ~ x)
```
:::


## Finite-quotient derivatives

Whenever you have a formula amenable to the construction of a symbolic derivative, that's what you should use. Finite-quotient derivatives are useful in those situation where you don't have such a formula. The calculation is simple but has a weakness that points out the advantages of the evanescent-$h$ approach.

For a function $f(x)$ and a "small," non-zero number $h$, the finite-quotient approximates the derivative with this formula:

$$\partial_x f(x) \approx \frac{f(x+h) - f(x-h)}{2h}\ .$$
To demonstrate, let's construct the finite-quotient approximation to $\partial_x \sin(x)$. Since we already know the symbolic derivative---it is $\partial_x \sin(x) = \cos(x)$---there's no genuinely practical purpose for this demonstration. Still, it can serve to confirm the symbolic rule.

We'll call the finite-quotient approximation `fq_sin()` and use `makeFun()` to construct it:

```{r}
fq_sin <- makeFun((sin(x+h)- sin(x-h))/(2*h) ~ x, h=0.01)
```

Notice that `fq_sin()` has a parameter, `h`. In the above line we have set a default value of `h = 0.01`. That may look to you like a "small" value for $h$ but, as we'll see, it is not enough to just to propose a small value, you need to confirm that it is genuinely small in the particular context in which it is being used.

To confirm that `fq_sin()` is an approximation to the genuine $\partial_x \sin(x)$, we exploit our knowledge that $\partial_x \sin(x) = \cos(x)$. @fig-confirm-fq-sin plots out the difference between the the approximation and the genuine derivative.

```{r}
#| label: fig-confirm-fq-sin
#| fig-cap: "Comparing `fq_sin()` to $\\partial_x \\sin(x)$ for two values of $h$."
slice_plot(fq_sin(x, h=0.01) - cos(x) ~ x, domain(x=-10:10)) %>%
  slice_plot(fq_sin(x, h=0.001) - cos(x) ~ x, color="magenta") %>%
  gf_labs(y="Error from true value.")
```

You'll need to look carefully at the scale of the vertical axis in @fig-confirm-fq-sin to see what's happening. For $h=0.01$, `fq_sin()` is not exactly the same as `cos()`, but it is close, always being within $\pm$0.00017. For many purposes, this would be accurate enough. But not for all purposes. We can make the approximation better by using a smaller $h$. For instance, the $h=0.001$ version of `fq_sin()` is accurate to within $\pm$0.0000017.

In practical use, one employs the finite-quotient method in those cases where one does *not already know* the exact derivative function. This would be the case, for example, if the function is a sound wave recorded in the form of an MP3 audio file.

In such situations, a practical way to determine what is a small $h$ is to pick one based on your understanding of the situation. For example, much of what we perceive of sound involves mixtures of sinusoids with periods longer than one-two-thousandth of a second, so you might start with $h$ of 0.002 seconds. Use this guess about $h$ to construct a candidate finite-quotient approximation. Then, construct another candidate using a smaller `h`, say, 0.0002 seconds. If the two candidates are a close match to one another, then you have confirmed that your choice of $h$ is adequate.

It's tempting to think that the approximation gets better and better as `h` is made smaller. But that's not true for computer calculations. The reason is that quantities on the computer have only a limited precision: about 15 digits. To illustrate, let's calculate a simple quantity, $(\sqrt{3})^2 - 3$. Mathematically, this quantity is exactly zero. But on the computer, it is small in magnitude, but not exactly zero:

```{r}
sqrt(3)^2 - 3
```

We can see this sort of loss of precision at work if we make `h` very small in the finite-quotient approximation to $\partial_x \sin(x)$. In @fig-too-small-h we are using `h = 0.000000000001`. The result is unsatisfactory.

```{r}
#| label: fig-too-small-h
#| fig-cap: "In computer calculations, using too small an `h` leads to a loss of accuracy in the finite-quotient approximation."
#| column: margin
slice_plot(fq_sin(x, h=0.000000000001) - cos(x) ~ x, domain(x=-10:10)) %>%
  slice_plot(fq_sin(x, h=0.0000000000001) - cos(x) ~ x, color="magenta") %>%
  gf_labs(y="Error from true value.")
```

## Second and higher-order derivatives

Many applications call for differentiating a derivative or even differentiating the derivative of a derivative. In English, such phrases are hard to read. They are much simpler using mathematical notation.

- $f(x)$ a function
- $\partial_x f(x)$ the derivative of $f(x)$ 
- $\partial_x \partial_x f(x)$, the ***second derivative*** of $f(x)$, usually written even more concisely as $\partial_{xx}f f(x)$.

There are third-order derivatives, fourth-order, and on up. These are not encountered as often as first- and second-order derivatives.

To compute a second-order derivative $\partial_{xx} f(x)$, first differentiate $f(x)$ to produce $\partial_x f(x)$. Then, still using the techniques described earlier in this chapter, differentiate $\partial_x f(x)$. 

There's a short-cut for constructing high-order derivatives using `D()` in a single step. On the right-hand side of the tilde expression, list the with-respect-to name repeatedly. For instance:

- The second derivative $\partial_{xx} \sin(x)$:

```{r}
D(sin(x) ~ x & x)
```

- The third derivative $\partial_{xxx} \ln(x)$:

```{r}
D(log(x) ~ x & x & x)
```

::: {.example data-latex=""}
Physics students learn a formula for the position of an object in free fall dropped from a height $x_0$ and at an initial velocity $v_0$:
$$ x(t) \equiv -\frac{1}{2} g t^2 + v_0 t + x_0\ .$$
The acceleration of the object is the second derivative $\partial_{tt} x(t)$. Use `D()` to find the object's acceleration.

The second derivative of $x(t)$ with respect to $t$ is:

```{r warning=FALSE}
D(0.5*g*t^2 + v0*t + x0 ~ t & t)
```

The acceleration does not depend on $t$; it is the constant $g$. No wonder $g$ is called "gravitational acceleration."

:::

TURN THESE INTO EXERCISES

BUT THINK WHERE TO PUT THE LIST OF DIFFERENTIATION NOTATIONS.

## Notations for differentiation

There are several traditional notations for differentiation of a function named $f()$. Here's a list of some of them, along with the name associated with each: 

- Leibnitz: $\frac{df}{dx}$
- Partial: $\frac{\partial f}{\partial x}$
- Euler: $D_x f$
- Newton (or "dot"): $\dot{f}$
- Lagrange (or "prime"): $f'$
- One-line: $\partial_x f$ (This hybrid of partial and Euler notation, will be the main differential notation used in this book.)

::: {.intheworld  data-latex=""}
It is a fact of mathematical and scientific life that varied notations are used for differentiation. This reflects historical precedence and, to be honest, nationalistic European politics of the 18th century. To make sense of mathematical writing in the many areas in which calculus is used, you have to recognize all of them for what they are. Your skill will be enhanced if you also memorize the names of the different styles. It's not all that different from the pattern in English of having multiple words for the same sort of object, for instance: car, automobile, junker, ride, wheels, crate, jalopy, limo, motor car, horseless carriage.  

In the days when carriages where pulled by horses, the phrase "horseless carriage" made a useful distinction. Today, when horses are rarely seen on the road, it makes sense to trim down the notation to its essentials: ~~horseless~~ **car**~~iage~~. Think of $\partial_x$ as this sort of minification of older notations.^[Yes, ["minification" is a word!](https://en.wikipedia.org/wiki/Minification_(programming))]   
:::

You have likely seen the $f'$ notation if you've studied calculus before. $f'$ is admirably concise but is only viable in a narrow circumstance: functions that take a single input. What $f'$ leaves out is a means to specify a crucial aspect of differentiation, the **with-respect-to input**. The general situation for differentiation involves functions of one or more variables, for example, $g(x, y, z)$. For such functions, you need to specify which is the with-respect-to input. So, for instance, we can differentiate $g()$ three different ways, each way incrementing one or another of the three inputs: 



$$\partial_x g(x, y, z) \equiv \frac{g(x+h, y, z) - g(x, y, z)}{h}$$

$$\partial_y g(x, y, z) \equiv \frac{g(x, y+h, z) - g(x, y, z)}{h}$$
$$\partial_z g(x, y, z) \equiv \frac{g(x, y, z+h) - g(x, y, z)}{h}$$ 


At this point in your studies, you haven't seen why you might choose to differentiate a function with respect to one input or another. That will come as you progress through calculus. But we want to set you up with a notation that won't narrow your options. This book mainly use2 the one-line notation, $\partial_x f$, but it means the same as the Leibnitz and Partial notations, which are much more widely used in textbooks. 
 

Both the Leibnitz and Partial notations are explicit in identifying the function and the with-respect-to input. For example, using the Partial differentiation notation, the three ways of differentiating our example function $g(x, y, z)$ are labeled : 

$$\frac{\partial f}{\partial x},\ \ \ \frac{\partial f}{\partial y},\ \ \text{and}\ \ \frac{\partial f}{\partial z}$$

Our R/mosaic computer differentiation is longer but explicit:
```r
g <- makeFun(__formula__ ~ x & y & z) # define a function
dx_g <- D(g(x, y, z) ~ x)
dy_g <- D(g(x, y, z) ~ y)
dz_g <- D(g(x, y, z) ~ z)
```
The names assigned to the result of the `D()` operator can be any names you like. What's nice about `dx_g` and the others is that it mimics the math notation $\partial_x g()$.

Notice that the R/mosaic operator for differentiation is named `D()` and that it is a function. It follows the same pattern as `makeFun()` or `slice_plot()` or `contour_plot()`: the first argument is a tilde expression, for instance `g(x, y, z) ~ x`, which identifies the mathematical function to work with (`g()`) and the name of the with-respect-to input to that function. The R/mosaic notation makes it clear that differentiation is an ***operation*** on a function. The `D()` operator takes a function as input and produces as output **another function**. We've seen similar behavior with, say, `slice_plot()`, which takes a function as input and produces graphics as output. Both `D()` and `slice_plot()` need to know the identity of the with-respect-to input as well as the function to work with. That's why both pieces of input are packaged into a tilde expression. 

::: {.why  data-latex=""}
We're calling R/mosaic `D()` an ***operator*** rather than a ***function***. The reason is purely for communication with other people. There are so many "functions" in a calculus course that we thought it would be helpful to distinguish between the kinds of functions that take quantities as input and produce a quantity as output and the functions that take a *function* as input and produce a *function* as output.^[It's pretty easy to see in an expression like $f(x,y)$ why we call $f()$ a function. But an expression like $3+2$ also involves a function of two inputs. We just write the name of the function (`+`) in between the two inputs. This is called ***infix*** notation.] Both sorts are called "functions" in R terminology. But a sentence like, "Differentiation is a function that takes a function as input and produces a function as output," true though it be, is dizzying. 
:::











```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-sig
#| column: margin
#| fig-cap: "The pattern-book sinusoid function. A vertical blue line marks the input $t=0$."
slice_plot(sin(t) ~ t, domain(t=c(-5, 2*pi))) %>%
  gf_labs(title="Sinusoid") %>%
  gf_vline(xintercept=0, color="dodgerblue", alpha=0.5)
```

The sinusoid (`sin()`) and the sigmoid (`pnorm()`). The computer can easily construct the slope functions for the sinusoid and sigmoid, which we'll call `Dsin()` and `Dsigma()` respectively.

```{r}
Dsin   <- makeFun((  sin(t+h) -   sin(t))/h ~ t, h=0.1)
Dsigma <- makeFun((pnorm(t+h) - pnorm(t))/h ~ t, h=0.1)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-sine
#| column: margin
#| fig-cap: "The pattern-book sigmoidal function."
slice_plot(pnorm(t) ~ t, domain(t=c(-5, 2*pi))) %>%
  gf_labs(title="Sigmoid") %>%
  gf_vline(xintercept=0, color="dodgerblue", alpha=0.5)
```

In the tilde expression handed to `makeFun()`, we've identified `t` as the name of the input and gave a "small" default value to the `h` parameter. But R recognizes that both `Dsin()` and `Dsigma()` are functions with two inputs, `t` and `h`, as you can see in the parenthesized argument list for the functions.

```{r}
Dsin
Dsigma
```

This is a nuisance, since when using the slope functions we will always need to think about `h`, a number that we'd like to describe simply as "small," but for which we always need to provide a numerical value. A surprisingly important question in the development of calculus is, "What can we do to avoid this nuisance?" To find out, let's look at `Dsin()` and `Dsigma()` for a range of values of `h`, as in @fig-sin-sig-many-h. 

```{r echo=FALSE, fig.show = "hold", warning=FALSE, message=FALSE}
#| label: fig-sin-sig-many-h
#| column: page-right
#| fig-cap: "The slope functions of the sinusoid and sigmoid. Each curve shows the slope function for a particular numerical choice of `h`. Both panels show $h=2, 1, 0.5, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001$."
rain <- rev(hcl.colors(12)[-(1:3)])
Psin <- slice_plot(Dsin(t, h=1) ~ t, domain(t=c(-5, 2*pi)), color=rain[1], label_text="h=1", label_x=0.56) %>%
  slice_plot(Dsin(t, h=2) ~ t, alpha = 1, label_text="h=2", label_x=.58, color=rain[1]) %>%
  slice_plot(Dsin(t, h=0.5) ~ t, alpha = 1, label_text="h=0.5", label_x=.54, color=rain[3]) %>%
  slice_plot(Dsin(t, h=0.1) ~ t, alpha = 1, label_text="h=0.1", label_x=.53, color=rain[4]) %>%
  slice_plot(Dsin(t, h=0.01) ~ t, alpha = 1, label_text="h=0.01", label_x=.51, color=rain[5]) %>%
  slice_plot(Dsin(t, h=0.001) ~ t, alpha = 1, color=rain[6], label_text="h=0.001", label_x=.5) %>%
  slice_plot(Dsin(t, h=0.0001) ~ t, alpha = 1, color=rain[7], label_text="h=0.0001", label_x=.49) %>%
  slice_plot(Dsin(t, h=0.00001) ~ t, alpha = 1, color=rain[8], label_text="h=0.00001", label_x=.48) %>%
  slice_plot(Dsin(t, h=0.000001) ~ t, alpha = 1, color=rain[9], label_text="h=0.000001", label_x=.44) %>%
  gf_labs(title="Slope functions of sinusoid") %>%
  gf_vline(xintercept=0, color="dodgerblue", alpha=0.5)
Psigma <- slice_plot(Dsigma(t, h=1) ~ t, domain(t=c(-5, 2*pi)), label_text="h=1", label_x=.28, color= rain[2]) %>%
  slice_plot(Dsigma(t, h=2) ~ t, alpha = 1, label_text="h=2", label_x=.24, color=rain[1]) %>%
  slice_plot(Dsigma(t, h=0.5) ~ t, alpha = 1, label_text="h=0.5", label_x=.54, color=rain[3]) %>%
  slice_plot(Dsigma(t, h=0.1) ~ t, alpha = 1, label_text="h=0.1", label_x=.53, color=rain[4]) %>%
  slice_plot(Dsigma(t, h=0.01) ~ t, alpha = 1, label_text="h=0.01", label_x=.51, color=rain[5]) %>%
  slice_plot(Dsigma(t, h=0.001) ~ t, alpha = 1, color=rain[6], label_text="h=0.001", label_x=.5) %>%
  slice_plot(Dsigma(t, h=0.0001) ~ t, alpha = 1, color=rain[7], label_text="h=0.0001", label_x=.49) %>%
  slice_plot(Dsigma(t, h=0.00001) ~ t, alpha = 1, color=rain[8], label_text="h=0.00001", label_x=.48) %>%
  slice_plot(Dsigma(t, h=0.000001) ~ t, alpha = 1, color=rain[9], label_text="h=0.000001", label_x=.44) %>%
  gf_labs(title="Slope functions of sigmoid") %>%
  gf_vline(xintercept=0, color="dodgerblue", alpha=0.5)
gridExtra::grid.arrange(Psin, Psigma, ncol=2)
```

Some observations from this numerical experiment:

1. As $h$ gets very small, the slope function doesn't depend on the exact value of $h$. As you can see in @fig-sin-sig-many-h, the graphs of the functions with the smallest $h$ (blue), with labels near the top of the graph) lie on top of one another.

    This will provide a way for us, eventually, to discard $h$ so that the slope function will not need an $h$ argument.
    
2. For small $h$, we have $\partial_t \sin(t) = \sin(t + \pi/2) = \cos(t)$. That is, taking the slope function of a sinusoid gives another sinusoid, shifted left by $\pi/2$ from the original. Or, in plain words, for small $h$ the cosine is the slope function of the sine.
3. For small $h$, we have $\partial_t \pnorm(t) = \dnorm(t)$. That is, for small $h$ the gaussian function is the slope function of the sigmoid $\dnorm()$ function.

You can confirm these last two statements by comparison with the original functions, especially the alignment of the peaks of the slope functions with respect to the peak of the sinusoid and the half-way point of the sigmoid. 

::: {.why  data-latex=""}
Here you use $t$ as the name of the input and $\partial_t$ as the notation for differentiation. Previously in this block you used $x$ as the input name and $\partial_x$ for differentiation. Are they the same? 

Mathematically, the name of the input makes no difference whatsoever. We could call it $x$ or $t$ or $y$ or Josephina. What's important is that the name be used consistently on the left and right sides of $\equiv$, and that the derivative symbol $\partial$ has a subscript that identifies ***the with-respect-to*** input. All these are the same statement mathematically: 

$$\partial_x\, x = 1\ \ \ \ \partial_t\, t = 1\ \ \ \ \partial_y\, y = 1\ \ \ \ \partial_\text{Josephina} \text{Josephina} = 1$$
Admittedly, the last one is hard to read.

When we look at derivatives of functions with multiple inputs we will need to be thoughtful about our choice of the with-respect-to input. But we want you to get used to seeing different input names used for differentiation. 

:::

Now consider the slope functions of the logarithm and exponential functions.

```{r echo=FALSE, fig.show = "hold", warning=FALSE, message=FALSE}
#| label: fig-log-exp-many-h
#| column: page-right
#| fig-cap: "The slope functions of the logarithm and exponential."
rain <- rev(hcl.colors(12)[-(1:3)])
Dlog <- makeFun((log(x+h) - log(x))/h ~ x)
Dexp <- makeFun((exp(x+h) - exp(x))/h ~ x)
Pa <- slice_plot(Dlog(t, h=1) ~ t, domain(t=c(0.01, 2)), color=rain[2], label_text="h=1", label_x=0.8) %>%
  slice_plot(Dlog(t, h=2) ~ t, domain(t=c(0.01, 2)), alpha = 1, label_text="h=2", label_x=.90, color=rain[1]) %>%
  slice_plot(Dlog(t, h=0.5) ~ t, domain(t=c(0.02, 2)), alpha = 1, label_text="h=0.5", label_x=.7, color=rain[3]) %>%
  slice_plot(Dlog(t, h=0.1) ~ t, alpha = 1, label_text="h=0.1", label_x=.6, color=rain[4]) %>%
  slice_plot(Dlog(t, h=0.01) ~ t, alpha = 1, label_text="h=0.01", label_x=.5, color=rain[5]) %>%
  slice_plot(Dlog(t, h=0.001) ~ t, domain(t=c(0.05, 2)), alpha = 1, color=rain[6], label_text="h=0.001", label_x=.4) %>%
  slice_plot(Dlog(t, h=0.0001) ~ t, domain(t=c(0.1, 2)), alpha = 1, color=rain[7], label_text="h=0.0001", label_x=.3) %>%
  slice_plot(Dlog(t, h=0.00001) ~ t, domain(t=c(0.125, 2)), alpha = 1, color=rain[8], label_text="h=0.00001", label_x=.2) %>%
  slice_plot(Dlog(t, h=0.000001) ~ t, domain(t=c(0.125, 2)),alpha = 1, color=rain[9], label_text="h=0.000001", label_x=.1) %>%
  gf_labs(title="Slope functions of logarithm") %>%
  gf_vline(xintercept=0, color="dodgerblue", alpha=0.5) %>%
  gf_lims(y=c(0,8))
Pb <- slice_plot(Dexp(t, h=1) ~ t, domain(t=c(-2, 2)), label_text="h=1", label_x=.4, color= rain[2]) %>%
  slice_plot(Dexp(t, h=2) ~ t, alpha = 1, label_text="h=2", label_x=.3, color=rain[1]) %>%
  slice_plot(Dexp(t, h=0.5) ~ t, alpha = 1, label_text="h=0.5", label_x=.5, color=rain[3]) %>%
  slice_plot(Dexp(t, h=0.1) ~ t, alpha = 1, label_text="h=0.1", label_x=.55, color=rain[4]) %>%
  slice_plot(Dexp(t, h=0.01) ~ t, alpha = 1, label_text="h=0.01", label_x=.6, color=rain[5]) %>%
  slice_plot(Dexp(t, h=0.001) ~ t, alpha = 1, color=rain[6], label_text="h=0.001", label_x=.65) %>%
  slice_plot(Dexp(t, h=0.0001) ~ t, alpha = 1, color=rain[7], label_text="h=0.0001", label_x=.75) %>%
  slice_plot(Dexp(t, h=0.00001) ~ t, alpha = 1, color=rain[8], label_text="h=0.00001", label_x=.85) %>%
  slice_plot(Dexp(t, h=0.000001) ~ t, alpha = 1, color=rain[9], label_text="h=0.000001", label_x=.9) %>%
  gf_labs(title="Slope functions of exponential") %>%
  gf_vline(xintercept=0, color="dodgerblue", alpha=0.5) %>%
  gf_lims(y=c(0,8))
gridExtra::grid.arrange(Pa, Pb, ncol=2)
```

These numerical experiments with the logarithm and exponential functions are more evidence that, as $h$ gets small, the slope function doesn't depend strongly on $h$. And, we find that:

- For small $h$, the slope function of the logarithm is a power-law function: $\partial_t \ln(t) = \frac{1}{t}$.
- For small $h$, the slope function of the exponential is the exponential itself: $\partial_t e^x = e^x$.

You can confirm these by evaluating the slope function of the exponential at $t=0$ and $t=1$, and the slope function of the logarithm at $t= 2, 1, 1/2, 1/4, 1/8.$

"Small" and "zero," although related, are different. In constructing a derivative, we use smaller and smaller $h$, but never zero. Let's see what happens if instead of evanescent h, we use zero h. For example, we can use the slope function `Dsin()` and `Dsigma()` that we created earlier. Setting $h$ to zero does not give a result that is the instantaneous rate of change of anything: 

```{r}
Dsin(t=1, h=0)
Dsigma(t=0, h=0)
```

In `NaN`, you can hear the echo of your fourth-grade teacher reminding you that it is illegal to divide by zero.

Think of evanescent $h$ as the vapor in the definition of "evanescent": "tending to vanish like vapor." This vapor is like the solvent in paint. You don't want the solvent once the paint is on the wall; wet paint is a nuisance. But getting the paint from the container to the wall absolutely needs the solvent.  

We used the solvent $h$ earlier in the chapter in the numerical experiments that led us to the derivatives of the pattern-book functions, for instance $\partial_x e^x = e^x$ or $\partial_x \sin(x) = \cos(x)$.  In Section @sec-h-free-rules, we'll construct an $h$-free theory of differentiation, reducing the process to a set of algebraic rules in which $h$ never appears. With this as our goal, let's continue using $h$ for a while to find some additional useful facts about derivatives. 



## Acceleration (rename to "Second increments"????)

Having worked out a theory of slope functions, Newton was ready to express the laws of motion in continuous time. He did this by denoting position as $x(t)$. He defined velocity and force in terms of slope functions of position and the "quantity of matter," which we call "mass."  

- Velocity is the slope function of position: $v(t) \equiv {\cal D}_t x(t)$.
- Net force is the slope function of velocity times mass: $F(t) \equiv m {\cal D}_t v(t) = m {\cal D}_{tt} x(t)$ 

To take mass out of the formulation, we give a name specifically to the slope function of velocity: ***acceleration***. 

- Acceleration is the slope function of velocity: $a(t) \equiv {\cal D}_t v(t) = {\cal D}_{tt} x(t)$.

Having defined acceleration, we can express net force as mass times acceleration. This is ***Newton's Second Law of Motion***.

::: {.why  data-latex=""}
We used **net force** as the quantity we related to mass and the slope function of velocity. There are different sources of forces that add up and can cancel out. Famously, Newton formulated the ***law of universal gravitation***, which ascribed the force between masses as proportional to the product of the two masses and inversely proportional to the square of the distance between them. But a mass on a table is subject to no net force, since the table pushes back (push = force) on the mass to cancel out the force due to gravity. "Net force" takes such cancellation into account. 
:::








## Exercises


<!-- Drill

`r Znotes:::MC_counter$reset(labels="roman")`


```{r drill-Pattern-book-derivs-1, echo=FALSE, results='markup'}
askMC(
  prompt = r"(Which pattern-book function is the **derivative** of the sigmoid function pnorm()? That is, $${\large \text{pnorm}(x)}  \underset{\scriptsize \text{anti-diff}}{{\stackrel{\text{diff}}{\ \ \ \ {\Huge\rightleftharpoons}\ \ \ }}}  {\LARGE ?}$$  )",
r"(Reciprocal $1/x$)" = r"(The anti-derivative of the reciprocal is the natural logarithm.)",
  r"(Exponential $e^x$)" = r"(The anti-derivative of the exponential is the exponential.)",
  r"(Sinusoid $\sin(x)$)" = r"(The anti-derivative of a sinusoid is a shifted sinusoid.)",
  r"(+Gaussian dnorm(x)+)" = r"(The anti-derivative of the gaussian is the sigmoid.)",
  r"(Constant $1$)" = r"(The anti-derivative of the constant function is the identity function $x$.)",
  random_answer_order=TRUE
)
```



```{r drill-Pattern-book-derivs-2, echo=FALSE, results='markup'}
askMC(
  prompt = r"(Which pattern-book function is the **anti-derivative** of the reciprocal $1/x$? That is, $${\LARGE ?}  \underset{\scriptsize \text{anti-diff}}{{\stackrel{\text{diff}}{\ \ \ \ {\Huge\rightleftharpoons}\ \ \ }}}  {\large \frac{1}{x}}$$<br> <br> NOTE: Differentiation produces a "child" function from a "parent" function. The child is the derivative of the parent. Putting the relationship the other way, the parent is the **anti-derivative** of the child. "Derivative" and "anti-derivative" are two ways of looking at the same relationship between a pair of functions. So, if $f(x)$ is the derivative of $F(x)$, then $F(x)$ is the anti-derivative of $f(x)$. )",
r"(Reciprocal $1/x$)" = r"(The derivative of the reciprocal is $- x^{-2}$.)",
  r"(+Logarithm $\ln(x)$+)" = r"(The derivative of the logarithm is the reciprocal $1/x$.)",
  r"(Sinusoid $\sin(x)$)" = r"(The derivative of a sinusoid is a shifted sinusoid.)",
  r"(Gaussian $\text{dnorm(x)}$)" = r"(The derivative of the gaussian is $- x \text{dnorm}(x)$, which is NOT one of our pattern book functions..)",
  r"(Constant $1$)" = r"(The derivative of the constant function is the zero function.)",
  random_answer_order=TRUE
)
```



```{r drill-Pattern-book-derivs-3, echo=FALSE, results='markup'}
askMC(
  prompt = r"(Which pattern-book function is the **anti-derivative** of the gaussian $\text{dnorm()}$? That is, $${\LARGE ?}  \underset{\scriptsize \text{anti-diff}}{{\stackrel{\text{diff}}{\ \ \ \ {\Huge\rightleftharpoons}\ \ \ }}}  {\large \text{dnorm}(x)}$$<br> <br> NOTE: Differentiation produces a "child" function from a "parent" function. The child is the derivative of the parent. Putting the relationship the other way, the parent is the **anti-derivative** of the child. "Derivative" and "anti-derivative" are two ways of looking at the same relationship between a pair of functions. So, if $f(x)$ is the derivative of $F(x)$, then $F(x)$ is the anti-derivative of $f(x)$. In other words: $${\large F(x)}  \underset{\scriptsize \text{anti-diff}}{{\stackrel{\text{diff}}{\ \ \ \ {\Huge\rightleftharpoons}\ \ \ }}}  {\Large f(x)}$$ )",
r"(Reciprocal $1/x$)" = r"(The derivative of the reciprocal is $- x^{-2}$.)",
  r"(Logarithm $\ln(x)$)" = r"(The derivative of the logarithm is the reciprocal $1/x$.)",
  r"(+Sigmoid $\text{pnorm(x)}$+)" = r"(The derivative of the sigmoid is a gaussian.)",
  r"(Gaussian $\text{dnorm(x)}$)" = r"(The derivative of the gaussian is $- x \text{dnorm}(x)$, which is NOT one of our pattern book functions..)",
  r"(Constant $1$)" = r"(The derivative of the constant function is the zero function.)",
  random_answer_order=TRUE
)
```



```{r drill-Pattern-book-derivs-4, echo=FALSE, results='markup'}
askMC(
  prompt = r"(What is the **derivative** of the power-law function $x^p$?i That is, $${\Large x^p}  \underset{\scriptsize \text{anti-diff}}{{\stackrel{\text{diff}}{\ \ \ \ {\Huge\rightleftharpoons}\ \ \ }}}  {\LARGE ?}$$   )",
r"($p\, x^p$)" = r"( )",
  r"($(p-1)\, x^p$)" = r"( )",
  r"(+$p\, x^{p-1}$+)" = r"( )",
  r"($(p-1)\, x^{p-1}$)" = r"( )",
  r"($\frac{1}{p} x^{p+1}$)" = r"( )",
  random_answer_order=TRUE
)
```



```{r drill-Pattern-book-derivs-5, echo=FALSE, results='markup'}
askMC(
  prompt = r"(There are two pattern-book functions whose **second** derivative is proportional to the function itself. Which are they?  )",
r"(+Exponential and sinusoid+)" = r"( )",
  r"(Exponential and sigmoid)" = r"(The second derivative of the sigmoid is $-x\, \text{dnorm}(x)$.)",
  r"(Exponential and logarithm)" = r"(The second derivative of the logarithm is $-x^{-2}$)",
  r"(Sinusoid and gaussian)" = r"(The second derivative of the gaussian is far from obvious at first glance: $(x^2 - 1) \text{dnorm}(x)$. Don't freak out; you aren't expected to memorize this one!)",
  random_answer_order=TRUE
)
```



```{r drill-Pattern-book-derivs-6, echo=FALSE, results='markup'}
askMC(
  prompt = r"(What is the **derivative** of $t^5$ with respect to $t$? That is, $${\Large t^5}  \underset{\scriptsize \text{anti-diff}}{{\stackrel{\text{diff}}{\ \ \ \ {\Huge\rightleftharpoons}\ \ \ }}}  {\Large ?}$$)",
r"(+$5 t^4$+)" = r"( )",
  r"($4 t^5$)" = r"(Remember, $\partial_t t^p = p\, t^{p-1}$)",
  r"($\frac{1}{5} t^4$)" = r"( )",
  r"($\frac{1}{4} t^5$)" = r"( )",
  random_answer_order=TRUE
)
```



```{r drill-Pattern-book-derivs-7, echo=FALSE, results='markup'}
askMC(
  prompt = r"(What is $\partial_x x^2$?  )",
r"($2 x$)" = r"(As per the power-law rule for differentiation.)",
  r"(+$2$+)" = r"( )",
  r"($2 x^2$)" = r"( )",
  r"($2/x$)" = r"( )",
  random_answer_order=TRUE
)
```



```{r drill-Pattern-book-derivs-8, echo=FALSE, results='markup'}
askMC(
  prompt = r"(What is $\partial_t \sin(x)$   )",
r"($\cos(x)$)" = r"(Tricked you! We asked about the partial with respect to $t$, not the partial with respect to $x$. There's no $t$ in $\sin(x)$. So far as $t$ is concerned, $\sin(x)$ is a constant.)",
  r"(+0+)" = r"( )",
  r"($-\sin(x)$)" = r"(Tricked you! We asked about the partial with respect to $t$, not the partial with respect to $x$. There's no $t$ in $\sin(x)$. So far as $t$ is concerned, $\sin(x)$ is a constant.)",
  r"($-\cos(x)$)" = r"(Tricked you! We asked about the partial with respect to $t$, not the partial with respect to $x$. There's no $t$ in $\sin(x)$. So far as $t$ is concerned, $\sin(x)$ is a constant.)",
  random_answer_order=TRUE
)
```



```{r drill-Pattern-book-derivs-9, echo=FALSE, results='markup'}
askMC(
  prompt = r"(Suppose you know only this one fact about $f(x)$, that $\partial_{xx}\, f(7.3) = 1.6$. Which of these statements must be true?  )",
  r"($f(x)$ is increasing at $x=7.3$.)" = r"(The first derivative $\partial_x f(x)$ would tell you about whether the function is increasing or decreasing. The second derivative $\partial_{xx} f(x)$ tells you about something else.)",
  r"($f(x)$ is concave up and decreasing at $x=7.3$)" = r"(Second derivatives tell you about concavity, but not about whether the function is increasing or decreasing.)",
  r"(+$f(x)$ is concave up at $x=7.3$+)" = r"(The 2nd derivative tells you **only** about the concavity of a function. It has nothing to say about the value of the slope.)",
  r"($f(x)$ is concave up at $x=7.3$, but eventually it will become concave down.)" = r"(In a graph of a function, the second derivative corresponds to concavity. But knowing only the numerical value for the second derivative at a single input value does not tell you what the function is doing anywhere else.)",
  random_answer_order=TRUE
)
```



```{r drill-Pattern-book-derivs-10, echo=FALSE, results='markup'}
askMC(
  prompt = r"(If $f(x)$ is discontinuous at $x=5$, can it possibly be **smooth** at $x=6$?  )",
r"(+Yes+)" = r"(Discontinuity of a function at one input doesn't tell you what the function is doing at another input.)",
  r"(No)" = r"(We told you something about $f(x)$ at $x=5$. Just knowing that doesn't tell you about $f(x)$ at other inputs.)",
  random_answer_order=TRUE
)
```



```{r drill-Pattern-book-derivs-11, echo=FALSE, results='markup'}
askMC(
  prompt = r"(If $g(x)$ is discontinuous at $x=1$, what will be the value of $\partial_x g(x)$ at $x=1$?   )",
r"(Depends on how big the gap is at the discontinuity.)" = r"( )",
  r"(0)" = r"( )",
  r"($1/x$)" = r"( )",
  r"(+The derivative isn't defined at a discontinuity.+)" = r"( )",
  random_answer_order=FALSE
)
```



```{r drill-Pattern-book-derivs-12, echo=FALSE, results='markup'}
askMC(
  prompt = r"(Which of the following is the correct construction for $\partial_t g(t)$?   )",
r"(+$\lim_{h \rightarrow 0} \frac{g(t + h) - g(t)}{h}$+)" = r"( )",
  r"($\lim_{h \rightarrow 0} \frac{g(t + h) - g(t)}{t}$)" = r"(The division should be by $h$, not by $t$.)",
  r"($\lim_{h \rightarrow 0} \frac{g(t) - g(t+h)}{h}$)" = r"(This will be the $- \partial_t g(t)$.)",
  r"($\lim_{x \rightarrow 0} \frac{g(t + h) - g(t)}{h}$)" = r"($x$ has nothing to do with it, so $\lim_{x \rightarrow 0}$ means nothing in this context.)",
  random_answer_order=TRUE
)
```



```{r drill-Pattern-book-derivs-13, echo=FALSE, results='markup'}
askMC(
  prompt = r"(Which of these is a reasonable definition of a derivative?   )",
r"(+A derivative is a function whose value tells, for any input, the instantaneous rate of change of the function from which it was derived.+)" = r"( )",
  r"(A derivative is the slope of a function.)" = r"(For functions that aren't straight lines, it doesn't mean anything to speak of "the slope" because the slope varies from place to place. Always remember that the derivative of a function is another **function**.)",
  r"(A derivative is a function whose value tells, for any input, the instantaneous change of the function from which it was derived.)" = r"(Not a bad start but a crucial word was left out. It should say, "the instantaneous **rate** of change". "Instantaneous" and "rate" go hand in hand.)",
  random_answer_order=TRUE
)
```



```{r drill-Pattern-book-derivs-14, echo=FALSE, results='markup'}
askMC(
  prompt = r"(Which one of these is **not** the derivative of a pattern-book function?   )",
r"(Reciprocal)" = r"(The reciprocal is the derivative of the logarithm.)",
  r"(Zero)" = r"(Zero is the derivative of the constant function.)",
  r"(One)" = r"(The constant function 1 is the derivative of the identity function $x$.)",
  r"(+Sigmoid+)" = r"(There is some function whose derivative is the sigmoid, but it's not a function that we've discussed and it's not much (or ever!?) used in modeling.)",
  random_answer_order=TRUE
)
```



```{r drill-Pattern-book-derivs-15, echo=FALSE, results='markup'}
askMC(
  prompt = r"(Which of the following shapes of functions is **not** allowed? You are **strongly** advised to try to draw each shape.  )",
r"(Increasing and concave up.)" = r"( )",
  r"(Decreasing and concave up.)" = r"( )",
  r"(Increasing and concave down.)" = r"( )",
  r"(Decreasing and concave down.)" = r"( )",
  r"(None of them are allowed.)" = r"( )",
  r"(+All of them are allowed.+)" = r"(The only restriction on mathematical functions is that for any given input, there can be at most one output.)",
  random_answer_order=FALSE
)
```
