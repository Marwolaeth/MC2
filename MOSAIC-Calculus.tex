% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod,
  oneside]{scrreprt}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={MOSAIC Calculus},
  pdfauthor={Daniel Kaplan},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.46,0.14}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{multicol}
\usepackage{afterpage}
\usepackage{cancel}

\newenvironment{why}% 
{% 
\textcolor{blue}{\hrulefill}%
  \par\vspace{.3\baselineskip}%
  \textcolor{blue}{\scshape Why did you?}%
  \par\vspace{\baselineskip}%
}%
{\textcolor{blue}{\hrulefill}}

\newenvironment{takenote}% 
{% 
\textcolor{orange}{\hrulefill}%
  \par\vspace{.3\baselineskip}%
  \textcolor{orange}{\scshape Take note!}%
  \par\vspace{\baselineskip}%
}%
{\textcolor{orange}{\hrulefill}}

\newenvironment{practice}% 
{% 
\textcolor{olive}{\hrulefill}%
  \par\vspace{.3\baselineskip}%
  \textcolor{olive}{\scshape How it's done.}%
  \par\vspace{\baselineskip}%
}%
{\textcolor{olive}{\hrulefill}}

\newenvironment{example}% 
{% 
\textcolor{teal}{\hrulefill}%
  \par\vspace{.3\baselineskip}%
  \textcolor{teal}{\scshape For instance ...}%
  \par\vspace{\baselineskip}%
}%
{\textcolor{teal}{\hrulefill}}


\newenvironment{intheworld}% 
{% 
\textcolor{magenta}{\hrulefill}%
  \par\vspace{.3\baselineskip}%
  \textcolor{magenta}{\scshape Math in the world ...}%
  \par\vspace{\baselineskip}%
}%
{\textcolor{magenta}{\hrulefill}}

\newenvironment{scaffolding}% 
{% 
\textcolor{brown}{\hrulefill}%
  \par\vspace{.3\baselineskip}%
  \textcolor{brown}{\scshape Open an R console and ....}%
  \par\vspace{\baselineskip}%
}%
{\textcolor{brown}{\hrulefill}}

\newenvironment{rmosaic}% 
{% 
\textcolor{brown}{\hrulefill}%
  \par\vspace{.3\baselineskip}%
  \textcolor{brown}{\scshape R/mosaic commands}%
  \par\vspace{\baselineskip}%
}%
{\textcolor{brown}{\hrulefill}}

\newenvironment{madeeasy}% 
{% 
\textcolor{orange}{\hrulefill}%
  \par\vspace{.3\baselineskip}%
  \textcolor{orange}{\scshape Calculus Made Easy}%
  \par\vspace{\baselineskip}%
}%
{\textcolor{orange}{\hrulefill}}


\newenvironment{tip}% 
{% 
\textcolor{teal}{\hrulefill}%
  \par\vspace{.3\baselineskip}%
  \textcolor{teal}{\scshape A Tip!}%
  \par\vspace{\baselineskip}%
}%
{\textcolor{teal}{\hrulefill}}

\newenvironment{underconstruction}% 
{% 
\textcolor{teal}{\hrulefill}%
  \par\vspace{.3\baselineskip}%
  \textcolor{teal}{\scshape UNDER CONSTRUCTION!}%
  \par\vspace{\baselineskip}%
}%
{\textcolor{teal}{\hrulefill}}

\renewcommand{\line}{\text{line}}
\newcommand{\hump}{\text{hump}}
\newcommand{\sigmoid}{\text{sigmoid}}
\newcommand{\recip}{\text{recip}}
\newcommand{\diff}[1]{{\cal D}_#1}
\newcommand{\pnorm}{\text{pnorm}}
\newcommand{\dnorm}{\text{dnorm}}
\let\origvec\vec
\let\origmathit\mathit
\let\orighat\hat
\let\origbar\bar
\renewcommand{\vec}[1]{\overset{{\rule[-1pt]{0mm}{1mm}}\rightharpoonup}{\mathbf{#1}}}
\renewcommand{\bar}[1]{\overset{{\rule[-1pt]{12pt}{.5mm}}}{\mathbf{#1}}}
\renewcommand{\mathit}[1]{\underset{\leftharpoondown}{\overset{{\rightharpoonup}}{\large\mathbf #1}}}
\renewcommand{\hat}[1]{\widehat{\ \mathbf#1\ }}
\newcommand{\len}[1]{{\|{\mathbf #1}\|}}
\newcommand{\tvec}[1]{\overset{\uparrow}{\mathbf #1}}
\newcommand{\tmat}[1]{\overset{\leftrightarrows}{\mathbf #1}}
\newcommand{\perpendicularto}[2]{#1\!\perp\!#2}
\newcommand{\modeledby}[2]{#1\!\sim\!#2}
\newcommand{\CC}[1]{\color{\#648fff}{#1}}
\newcommand{\CE}[1]{\color{\#785ef0}{#1}}
\newcommand{\CA}[1]{\color{\#dc267f}{#1}}
\newcommand{\CB}[1]{\color{\#fe6100}{#1}}
\newcommand{\CD}[1]{\color{\#ffb000}{#1}}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\renewcommand*\contentsname{Table of contents}
\renewcommand*\listfigurename{List of Figures}
\renewcommand*\listtablename{List of Tables}
\renewcommand*\figurename{Figure}
\renewcommand*\tablename{Table}
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{MOSAIC Calculus}
\author{Daniel Kaplan}
\date{4/28/2022}

\begin{document}
\maketitle

\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[borderline west={3pt}{0pt}{shadecolor}, boxrule=0pt, enhanced, frame hidden, interior hidden, sharp corners]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\hypertarget{tips-in-draft}{%
\section*{Tips in draft}\label{tips-in-draft}}
\addcontentsline{toc}{section}{Tips in draft}

\texttt{GIT\_TRACE=1\ git\ push}

or

\texttt{git\ push\ -\/-force\ origin\ main}

\hypertarget{acknowledements}{%
\section*{Acknowledements}\label{acknowledements}}
\addcontentsline{toc}{section}{Acknowledements}

This project was initiated by the Mathematical Sciences department at
the US Air Force Academy. They recognized that a traditional calculus
introduction is ill-suited to the needs of STEM in the 21st century.

Critical support was given by the \href{http://ardifoundation.org}{ARDI
Foundation} which awarded the
\href{http://www.ardifoundation.org/coors-chair/}{Holland H. Coors Chair
in Education Technology} to one of the project members, Daniel Kaplan.
This made possible a year-long residency at USAFA during which time he
was able to work unhindered on this project.

Macalester College, where Kaplan is DeWitt Wallace Professor of
Mathematics, Statistics, and Computer science, was the site where the
overall framework and many of the materials for a STEM-oriented calculus
were developed. Particularly important in the germination were David
Bressoud and Jan Serie, respectively chairs of the Macalester math and
biology departments, as well as Prof.~Thomas Halverson and Prof.~Karen
Saxe, who volunteered to team teach with Kaplan the first prototype
course. Early grant support from the Howard Hughes Medical Foundation
and the Keck Foundation provided the resources to carry the prototype
course to a point of development where it became the entryway to
calculus for Macalester students.

Profs. Randall Pruim (Calvin University) and Nicholas Horton (Amherst
College) were essential collaborators in developing software to support
calculus in R. They and Kaplan formed the core team of Project MOSAIC,
which was supported by the US National Science Foundation (NSF
DUE-0920350).

Joel Kilty and Alex McAllister at Centre College admired the Macalester
course and devoted much work and ingenuity to write a textbook,
\emph{Mathematical Modeling and Applied Calculus} (Oxford Univ. Press),
implementing their own version. Their textbook enabled us to reduce the
use of sketchy notes in the first offering of this course at USAFA.

\part{Differentiation}

This is where I'll explain what the block is about and the overall
goals.

\hypertarget{sec-continuous-change}{%
\chapter{Continuous change}\label{sec-continuous-change}}

For our purposes, a good definition of calculus is

\begin{quote}
\emph{The use of functions to model and explore continuous change}
\end{quote}

In previous chapters we defined and studied functions. Now it is time to
get at the core of calculus, the idea of ``continuous change.''

\hypertarget{mathematics-in-motion}{%
\section{Mathematics in motion}\label{mathematics-in-motion}}

The questions that started it all had to do with motion of planets and
marbles. In more technical language, ``ballistics,'' the science of
balls. There were words to describe speed: fast and slow. There were
words to describe force: strong and weak, heavy and light. And there
were words to describe location and distance: far and near, long and
short, here and there. But what were the relationships among these
things? And how did time fit in, an intangible quantity that had aspects
of location (long and short) and speed (quick and slow)?

Galileo (1564-1642) started the ball rolling.\sidenote{\footnotesize Galileo was not
  aware of Kepler's elliptical theory, even though they lived at the
  same time.} As the son of a musician and music theorist, he had a
sense of musical time, a steady beat of intervals. When a student of
medicine in Pisa, he noted that swinging pendulums kept reliable time,
regardless of the amplitude of their swing. After unintentionally
attending a geometry lecture, he turned to mathematics and natural
philosophy.

Using his newly developed apparatus, the telescope, Galileo's
observations put him on a collision course with the accepted classical
truth about the nature of the planets. Seeking to understand gravity, he
built an apparatus that enabled him accurately to measure the position
in time of a ball rolling down a straight ramp. The belled gates he set
up to mark the ball's passage were spaced evenly in musical time: 1, 2,
3, 4, \ldots. To get this even spacing in time, Galileo found he had to
position the gates unevenly. Defining as 1 the distance of the first
gate from the ball's release point, the gates were at positions 1, 4, 9,
16, \ldots.

\marginnote{\begin{footnotesize}

A re-enactment of Galileo's rolling-ball experiment. The frets on the
ramp are at positions 2 cm, 8 cm, 18 cm, 32 cm, 50 cm, \ldots, that is,
2 cm times 1, 4, 9, 16, 25.

\end{footnotesize}}

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/www/galileo-reinactment.png}

}

\caption{\label{fig-galileo-reinactment}A re-enactment of Galileo's
rolling-ball experiment. The frets on the ramp are at positions 2 cm, 8
cm, 18 cm, 32 cm, 50 cm, \ldots, that is, 2 cm times 1, 4, 9, 16, 25.
Link to video: \url{https://youtu.be/9BQKe9HT1OE?t=9}}

\end{marginfigure}

\begin{table}

\caption{Galileo's observations and their first \& second
increments.}\begin{minipage}[t]{\linewidth}

{\centering 

\begin{longtable}[]{@{}llll@{}}
\toprule
\(t\) & \(x(t)\) & first increment & second increment \\
\midrule
\endhead
0 & 0 & 1 & 2 \\
1 & 1 & 3 & 2 \\
2 & 4 & 5 & 2 \\
3 & 9 & 7 & \\
4 & 16 & & \\
\bottomrule
\end{longtable}

}

\end{minipage}%

\end{table}

Anyone familiar with the squares of the integers can see the pattern in
1, 4, 9, 16, \ldots. To demonstrate the pattern even more clearly,
Galileo took the difference between the successive positions, what we
will call the ``first increment.''

\[\underbrace{1 - 0}_1 \ \ \ \ \ \underbrace{4 - 1}_3\ \ \ \ \ \underbrace{9 - 4}_{5}\ \ \ \ \ \underbrace{16-9}_7\ \ \ \underset{{\Large\strut}\text{first increment}}{\text{}}\]
Next, Galileo repeated the differencing process on the first increment
to produce a ``second increment.''

\[\underbrace{3 - 1}_2 \ \ \ \ \ \underbrace{5 - 3}_2\ \ \ \ \ \underbrace{7 - 5}_{2}\ \ \ \underset{{\Large\strut}\text{second increment}}{\text{}}\]
::: \{.column-margin\} For more about Galileo's measurements, see
Stillman Drake (1986) ``Galileo's physical measurements'' \emph{American
Journal of Physics} \textbf{54}, 302-305
\url{https://doi.org/10.1119/1.14634} :::

The rule established by Galileo's observations for the motion of a ball
rolling down the ramp:

\begin{quote}
The second increment of position is constant.
\end{quote}

\hypertarget{continuous-time}{%
\section{Continuous time}\label{continuous-time}}

Galileo's mathematics of first and second increments was suited to the
discrete-time measurements he was able to make. It would be for Newton
to develop the continuous-time analog of increments.

To start, we can imagine a function \(x(t)\) that gives the position of
the ball at any instant \(t\). With this notation, Galileo's measured
positions were \(x(0), x(1), x(2), x(3), x(4), \ldots\), and the first
increments were \(x(1) - x(0)\), \(x(2) - x(1)\), \(x(3) - x(2)\), and
so on.

But just as position \(x(t)\) is a continuous function of time \(t\),
the first increment can also be written as a continous function:
\[y(t) \equiv x(t+1) - x(t)\ .\] Similarly, there is a second increment
function:
\begin{eqnarray}z(t) & \equiv&  y(t+1) - y(t)\\ & = & \left[x(t+2) - x(t+1)\right] - \left[x(t+1) - x(t)\right] \\ &=& x(t+2) - 2 x(t+1) + x(t)\ .
\end{eqnarray}

The \(+1\) and \(+2\) in the first and second increment functions
correspond to the time elapsed from one belled gate to the next. More
generally, rather than using Galileo's unit of rhythmic time, we can
define the increment functions using a time quantity of our own choice;
we will call it \(h\).

Re-written using \(h\), the first increment becomes
\[y(t) \equiv x(t+h) - x(t)\ .\] The second increment function is
\begin{eqnarray}z(t) & \equiv&  y(t+h) - y(t)\\ & = & \left[x(t+2h) - x(t+h)\right] - \left[x(t+h) - x(t)\right] \\ &=& x(t+2h) - 2 x(t+h) + x(t)\ .
\end{eqnarray}

Evidently, the numerical values (dimension L) of the first and second
increments depend on \(h\), which is a choice made by the experimenter,
not a fact of nature. If the experimenter selects a large \(h\), the
first and second increments will be large.

It would be nice to frame the ballistics theory so that \(h\) does not
appear. Newton's insight amounts to taking two steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Replace the simple difference \(x(t+h) - x(t)\) with a
  \textbf{\emph{rate of change}}, that is: ::: \{.column-margin\} Note
  that we are using the symbol \({\cal D}\_t\) and naming the rate of
  change function \({\cal D}_t y(t)\). Read \({\cal D}_t\) as ``the rate
  of change with respect to \(t\). :::
  \[\text{Rate of change of } x(t): \ \ \ \ {\cal D}_t y(t) \equiv \frac{x(t+h) - x(t)}{h}\]
\end{enumerate}

Likewise, the second increment will become a ``rate of change of a rate
of change,'' a phrase that is easier to understood when written as a
formula:

\marginnote{\begin{footnotesize}

Read \({\cal D}_t {\cal D}_t y(t)\) as ``the rate of change of the rate
of change of \(y(t)\).''

\end{footnotesize}}

\begin{eqnarray}
{\cal D}_t {\cal D}_t y(t)  &\equiv&   {\cal D}_t \left(\strut \frac{{\cal D}_t y(t+h) - {\cal D}_t y(h)}{h}\right) \\
&=& \frac{y(t+h) - y(t)}{h} \\
&=& \frac{\frac{x(t+2h) - x(t+h)}{h}- \frac{x(t+h) - x(t)}{h}}{h}\\
&=& \frac{x(t+2h) - 2 x(t+h) + x(t)}{h^2}\ .
\end{eqnarray}

Admittedly, this complicated expression for the rate-of-change
equivalent of Galileo's second increment hardly looks like an
improvement! And it still depends on \(h\).

This is where the second step of Newton's insight comes in.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Make \(h\) vanishingly small.
\end{enumerate}

In the next chapters, we will look at how these two steps---use rate of
change rather than change and make \(h\) vanishingly small---create
mathematical entities that allowed Newton to extend Galileo's work to
become a universal theory of motion.

\hypertarget{change-relationships}{%
\section{Change relationships}\label{change-relationships}}

As you know, \textbf{\emph{function}} is a mathematical idea used to
represent a relationship between quantities. For instance, the
\textbf{water volume} of a reservoir behind a dam varies with the
seasons and over the years. As a function, water volume is a
relationship between water volume (one quantity) and time (another
quantity). Similarly, the \textbf{flow} in a river feeding the reservoir
has its own relationship with time. In spring, the river may be rushing
with snow-melt, in late summer the river may be dry, but after a summer
downpour the river flow again rises briefly. In other words, river flow
is a function of time.

\textbf{\emph{Differentiation}} is a way of describing a
\emph{relationship between relationships}. The water volume in the
reservoir has a relationship with time. The river flow has a
relationship with time. Those two relationships are themselves related:
the river flow feeds the reservoir and thereby influences the water
volume.

It is not easy to keep straight what's going on in a ``relationship
between relationships.'' Consequently, we need tools such as
differentiation to aid our understanding. For instance, Johannes Kepler
(1572-1630) spent years analyzing the data collected by astronomer Tycho
Brahe (1546-1601). The data showed clearly a relationship between time
and the speed of a planet across the sky. Long-standing wisdom claimed
that there is also a specific relationship between a planet's position
and time. From antiquity, it had been claimed that planets moved in
circular orbits. Kepler worked hard to find the relationship between the
two relationships: speed versus time and position versus time. He was
unsuccessful until he dropped the assumption that planetary orbits are
circular. Testing the hypothesis that orbits are \textbf{elliptical},
Kepler was able to find a simple relationship between speed vs.~time and
position vs.~time.

Building on Kepler's earlier work, Newton hypothesized that planets
might be influenced by the same gravity that pulls an apple to the
ground. It was evident from human experience that gravity has the most
trivial relationship with time: gravity is constant! But Newton could
not find a link between this notion of gravity as a constant and
Kepler's planetary motion as a function of time. Success came when
Newton hypothesized---without any direct evidence from experience---that
gravity is a function of distance. Newton's formulation of the
relationship between relationships--- gravity-as-a-function-of-distance
and orbital-position-as-a-function-of time---became the foundation of
modern science. Newton's theories of gravity, force, and motion created
an extremely complicated chain or reasoning that is still hard to grasp.
Or, more precisely, it is hard to grasp \emph{until} you have the
language for describing relationships between relationships. Newton
invented this language: differentiation. As you learn this language, you
will find it easier to express and understand relationships between
relationships, that is, the mechanisms that account for the
ever-changing quantities around us.

\hypertarget{with-respect-to}{%
\section{With respect to \ldots{}}\label{with-respect-to}}

We've introduced a bit of new notation in the previous section,
\({\cal D}_t\). {\marginnote{\begin{footnotesize}In Chapter
Section~\ref{sec-evanescent-h}, when we carry out the second step of
Newton's program by making \(h\) vanishingly small, we will switch from
the big \({\cal D}\) to a smaller one, \(\partial\), to remind us that
\(h\) has vanished from the picture.\end{footnotesize}}} As mentioned
previously, \({\cal D}\) stands for ``the rate of change of \_\_\_\_.''
In use, you put a function in the slot indicated by \_\_\_\_. Which
function depends on what you want to describe. For instance, the
position of a rolling ball is a function of time: \(x(t)\). ``The rate
of change of \(x(t)\)'' is written \({\cal D}_t x(t)\). This object
\({\cal D}_t x(t)\) is itself a function of time.

Another example: consider a water reservoir fed by a spring and drained
by the water utility to serve its customers. Suppose \(w(t)\) is the
volume of water in the reservoir, a quantity that changes over time.
Then \({\cal D}_t w(t)\) is the rate of change of water volume in the
reservoir. Common sense suggests that the rate of change in water volume
will be positive during a wet season and negative in a drought.

The subscript on \({\cal D}_t\) is the \textbf{\emph{with-respect-to}}
input. To illustrate, suppose that \(h(v, w)\) is the volume of the
harvest from a field as a function of the amount of irrigation water
\(w\) and the amount of fertilizer used during the growing season.
{\marginnote{\begin{footnotesize}Constructing such a function could be
done by collecting data over many years of the harvest, along with the
amount of water and fertilizer used each year.\end{footnotesize}}} As
will be described in Chapter Section~\ref{sec-partial-change}, there are
two different rate-of-change functions associated with \(h()\). One is
the rate of change in harvest volume with respect to \(w\), the other is
the rate change in harvest volume with respect to \(v\). In everyday
language, \({\cal D}_w h(v, w)\) can be used to predict how much the
harvest will change if, next year, the farmer uses less irrigation
water. Similarly, \({\cal D}_v h(v, w)\) can inform a farmer's decision
to reduce costs by using less fertilizer.

Strictly speaking, for functions with just one input the subscript on
\({\cal D}\) isn't needed. Even so, we will always include a subscript,
if only for the sake of forming good habits to serve us when we do
examine rates of change in functions of multiple inputs.

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

\hypertarget{sec-rate-of-change}{%
\chapter{Rate of change}\label{sec-rate-of-change}}

Imagine a car trip along a scenic road such as that shown in
Figure~\ref{fig-stop-and-go}. As the trip proceeds, the position varies;
position is a function of time. The graph's horizontal axis marks the
elapsed time from the start of the trip. The vertical axis denotes the
distance from the starting point.

\begin{figure}

\sidecaption{\label{fig-stop-and-go}The position of an imagined car over
an hour's time.}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/17-rate-of-change_files/figure-pdf/fig-stop-and-go-1.pdf}

}

\end{figure}

At the end of the hour's journey, the car's position has changed by 25
miles from the start. The \emph{rate of change} of the car's position is
a ratio: the change in position divided by the change in time:
\[\frac{25\ \text{miles}}{1\ \text{hour}}\] Writing this as ``25 miles
per hour'' is more compact, with the small word ``per'' doing the job of
reminding that the quantity was produced by dividing change in position
by change in time.

\hypertarget{outputs-versus-rates-of-change}{%
\section{Outputs versus rates of
change}\label{outputs-versus-rates-of-change}}

Using functions to describe the car-trip situation, we can say that
position is a function of time. We will call it \(p(t)\). The input to
the function is time and the output is position.

A \emph{rate of change} for the function can be calculated by choosing
\emph{two different values for time} and evaluating the function at
those times. The evaluation produces \emph{two different values for the
output position}. Calling the two times \(t_0\) and \(t_1\), the
corresponding outputs are \(p(t_0)\) and \(p(t_1)\).

The \textbf{\emph{average rate of change}} of \(p(t)\) over the interval
\(t_0 \leq t \leq t_1\) is \[\frac{p(t_1) - p(t_0)}{t_1 - t_0}\ .\] In
Chapter \textbf{?@sec-dimension-and-units} we saw that subtraction is
legitimate only when the two quantities involved have the same dimension
and units. That is the case here. \(p(t_1)\) and \(p(t_2)\) both have
dimension L and miles as the unit. \(t_1\) and \(t_0\) both have
dimension T and hours as the unit.

The division of \(p(t_1) - p(t_0)\) (dimension L) by \(t_1 - t_0\)
(dimension T) is also dimensionally legitimate. The simple reason is
that division of one quantity by another is \emph{always} dimensionally
legitimate. The division produces a quantity with dimension L/T.

A quantity with dimension L/T is utterly different than a quantity of
dimension L or a quantity of dimension T. In other words, ``25 miles per
hour'' is neither a position nor a time, it is a
\textbf{\emph{velocity}}.

One way to see that velocity is a different kind of quantity than
position or time is that you measure the quantities in different ways.
You might measure position by noting the passage of a mile marker along
the side of the road. You can measure time by reference, say, to your
level of boredom or by checking a clock or watch. Divide change in
position by change in time to get velocity. But you can also sense
velocity directly, by the level of noise in the car or the blurring of
nearby objects along the road.

On a graph, you also measure in different ways changes in the input to a
function and the corresponding changes in output. As always, start by
picking the endpoints of an interval in the domain of the function. As
an example, \textbf{?@fig-stop-and-go-b} marks the endpoints of an
interval with \(\color{magenta}{magenta}\) dots.

\marginnote{\begin{footnotesize}

\end{footnotesize}}

Draw a rectangle connecting the function values at the start and end of
the interval. The change in input is the horizontal extent of the
rectangle. The change in output is the vertical extent of the rectangle.
If ``vertical'' and ``horizontal'' are enough to point out that the two
measures are of different kinds of things, you will be reminded by your
having to use two different scales for the two measurements.

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/17-rate-of-change_files/figure-pdf/fig-car-rose-1.pdf}

}

\caption{\label{fig-car-rose}Adding a scale for slope to the graph.}

\end{marginfigure}

Over the interval marked, the average rate of change of the function is
still another kind of perceived quantity, the ``slope'' of the diagonal
of the rectangle. Unfortunately, graphs do not typically include a scale
for slope, but we have added a scale to Figure~\ref{fig-car-rose}. From
the slope scale, you can easily see that the average rate of change is a
little less than 30 miles per hour.

\hypertarget{slope-at-a-point}{%
\section{Slope at a point}\label{slope-at-a-point}}

With a slope scale, you can dispense with the laborious process shown in
\textbf{?@fig-stop-and-go-b}: marking an interval, drawing a rectangle,
measuring the vertical change, etc. The slope scale lets you read off
the rate of change at a glance: pick a point in the domain, look at the
slope of the function \emph{at that point}, and compare it to the slope
scale.

Perhaps you can see that formally defining an interval isn't an absolute
necessity for defining a slope. Instead, you can perceive slope directly
from a graph, even if it is hard to quantify without a special scale.

\begin{example}
Using the slope scale in Figure~\ref{fig-car-rose2}, estimate the
function's slope at input \(t=0.2\). How does it compare to the slope at
\(t=0.4\)?

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/17-rate-of-change_files/figure-pdf/fig-car-rose2-1.pdf}

}

\caption{\label{fig-car-rose2}Adding a scale for slope to the graph.}

\end{marginfigure}

Place a ruler on the function graph so that the rule touches the graph
at \(t=0.2\). Keeping that point of contact, vary the slope of the ruler
until it neatly aligns with the curve. Now, without changing the slope
of the ruler, slide it over to the slope scale and read the ruler slope
off that scale. The slope is a bit more than 20 miles per hour.

At \(t=0.4\), the function slope is considerably steeper than at
\(t=0.2\), about 60 miles per hour.

\end{example}

The function's slope at a specific input like \(t=0.2\) is called the
\textbf{\emph{instantaneous slope}} and corresponds to the instantaneous
velocity of the car. , you do not have to measure the car's velocity by
reading the change in position over the interval between two distinct
moments in time; you can simply look at the speedometer to get an
instantaneous read-out of the velocity. We will translate instantaneous
rate of change into the language of functions in Chapter
Section~\ref{sec-evanescent-h}.

\hypertarget{sec-slope-function}{%
\section{Slope function}\label{sec-slope-function}}

Figure -Figure~\ref{fig-car-rose2} shows that the rate of change of the
car-position function changes during the trip. In other words, the rate
of change of position is itself a function of time.

In general, for a function \(p(t)\) the rate of change function will be
\[{\cal D}_t p(t) \equiv \frac{p(t+h) - p(t)}{h}\] where \(h\) is the
length of the interval used to compute the rate of change. We will call
this the \textbf{\emph{slope function}} of \(f(t)\).

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/17-rate-of-change_files/figure-pdf/fig-car-rose3-1.pdf}

}

\caption{\label{fig-car-rose3}Showing the changing slope of \(p(t)\) as
a series of segments. The slope scale (in red) indicates the numerical
value of each segment.}

\end{marginfigure}

A fun but unconventional way to display a slope function is to show the
literal slope of \(p(t)\) as a function of \(t\) as in
Figure~\ref{fig-car-rose3}. It represents the value of
\({\cal D}_t p(t)\) as the slope of a little line segment. To read off
the numerical value of the slope, refer to the slope scale drawn in red.
A picture like Figure~\ref{fig-car-rose3} is a good reminder that the
slope function \({\cal D}_t p(t)\) is all about the \emph{slope} of
\(p(t)\) and not at all about the actual value of \(p(t)\).

The conventional way to display a slope function is to show the
numerical value of the slope by the position on the vertical axis, as in
Figure~\ref{fig-car-rose4}. Such a graph is easy to read, but provides
nothing but the axis label to remind you that the scale on the vertical
axis is the \emph{slope} of another function.

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/17-rate-of-change_files/figure-pdf/fig-car-rose4-1.pdf}

}

\caption{\label{fig-car-rose4}Graphing the slope function
\({\cal D}_t p(t)\). The value of the slope of \(p(t)\) can be read from
the vertical axis scale.}

\end{marginfigure}

\hypertarget{sec-tree-harvest-example}{%
\section{Average rate of change}\label{sec-tree-harvest-example}}

In Chapter Section~\ref{sec-evanescent-h}, we will start working with
the \emph{instantaneous rate of change} of a function. That concept is
so important that you will tend to forget there was any such thing as
the ``average rate of change'' over an interval.

Nevertheless, average rate of change can be a useful concept in many
circumstances. To illustrate, Figure~\ref{fig-aver-tree} shows a
simplified model of the amount of usable wood harvestable from a typical
tree in a managed forest of Ponderosa Pine. (You can see some actual
forestry research models
\href{https://www.fs.fed.us/rm/pubs/rmrs_gtr292/1992_milner.pdf}{here}.)
Such a model, even if simplified, can provide useful insight for
forestry planning.

\begin{figure}

\sidecaption{\label{fig-aver-tree}A model, somewhat realistic, of the
amount of useful wood from a Ponderosa Pine as a function of the number
of years from planting to harvest.}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/17-rate-of-change_files/figure-pdf/fig-aver-tree-1.pdf}

}

\end{figure}

The overall pattern in \textbf{?@fig-over-tree} is that the tree
continues to grow until year 50, when it seems to have reached an
equilibrium: perhaps growth goes to zero, or rot balances growth.

If managing a forest for wood production, it seems sensible to try to
get as much wood out of the tree as possible. The maximum volume of wood
occurs, in \textbf{?@fig-over-tree}, at about year 50. Does that mean
that harvesting at year 50 is optimal? If not, when is the best time?
{\marginnote{\begin{footnotesize}Spend a moment thinking about
\textbf{?@fig-over-tree} and draw your own conclusions. Right or wrong,
this will help you understand the argument we are about to
make.\end{footnotesize}}}

Good forestry practices are ``sustainable.'' Forests are managed to be
continually productive rather than subject to a one-time extraction of
value followed by desolation. For sustainability, it is important to
consider the life cycle of the forest. After all, continual productivity
implies that the forest will continue to produce value into the
indefinite future.

One implication of managing for sustainability is that the quantity to
optimize is not the volume of wood from a one-time harvest. Rather, it
is the \textbf{\emph{rate}} (per year) at which wood can be sustainably
extracted from the forest.

Around year 25, the tree adds usable wood at the fastest instantaneous
rate. This might suggest to some that a good time to harvest is near
year 25. But, in fact, it makes no sense to harvest at the time of
maximum rate of growth; why kill the tree when it is being most
productive?

A better quantity to look at for deciding when to harvest is the
\emph{average rate of growth} in the volume of wood. Remember that
``average rate of change'' is the rate of change over an extended
interval. For wood harvesting, the relevant interval is the time from
planting until harvest.

Harvesting at year 25 will give a total change of 600 board feet over 25
years, corresponding to an \emph{average rate of change} of
\(600 \div 25 = 24\ \text{board-feet-per-year}\). But if you wait until
year 35, you will have about 900 board feet, giving an average rate of
change of \(900 \div 35 = 25.7\) board-feet-per-year
(L\textsuperscript{3} T\textsuperscript{-1}).

It is easy to construct a diagram that indicates whether year 35 is best
for the harvest. Recall that our fundamental model of change is the
straight-line function. So we are going to \textbf{\emph{model the
model}} of tree growth as a straight-line function. Like the model in
Figure~\ref{fig-aver-tree}, our straight-line model will start with zero
wood when planted. Furthermore, to be faithful to
Figure~\ref{fig-aver-tree}, we will insist that the straight-line
intersect or touch that curve.

Figure~\ref{fig-aver-tree2} reiterates the Figure~\ref{fig-aver-tree}
model of the tree annotated with several straight-line models that all
give zero harvest-able wood at planting time. Each green line represents
a scenario where harvest occurs at \(t_1\), \(t_2\), etc. From the
perspective of representing the rate of growth per year from planting to
harvest, the straight-line green models do not need to replicate the
actual growth curve. The complexities of the curve are not relevant to
the growth rate. Instead, what's relevant is the slope of a
straight-line model connecting the output at planting time to the output
at harvest time. In contrast, the \(\color{magenta}{\text{magenta}}\)
curve is not a suitable model because it does not match the situation at
any harvest time; it does not touch the curve anywhere after planting!

\begin{figure}

\sidecaption{\label{fig-aver-tree2}Modeling the tree-growth model with
straight lines connecting planting time to various harvest times. The
slope of each line is the average rate of growth for that planting
time.}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/17-rate-of-change_files/figure-pdf/fig-aver-tree2-1.pdf}

}

\end{figure}

Choose a harvest time that produces the steepest possible green segment
to maximize average lumber volume per year. From
Figure~\ref{fig-aver-tree2}, that steepest line glances the growth curve
near year 31 (shown as \(t_3\) in the diagram).

It is best to find the argmax by creating a function that shows
explicitly what one is trying to optimize. (In
Chapter~\ref{sec-optim-and-shape} we will use the name
\textbf{\emph{objective function}} to identify such function.) Here, the
objective function is
\(\text{ave.growth(year)} \equiv \text{volume(year)} / \text{year}\).
See Figure~\ref{fig-aver-tree3}.

\begin{figure}

\sidecaption{\label{fig-aver-tree3}Graph of the average-growth function
ave\_growth(year), constructed by dividing volume(year) by year.}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/17-rate-of-change_files/figure-pdf/fig-aver-tree3-1.pdf}

}

\end{figure}

The graph of ave\_growth(year) makes clear the maximum average growth
from planting to harvest will occur at about year 32.

There is no point waiting until after year 50.

At year 25, the tree is growing as fast as ever. You will get about 600
board feet of lumber.\sidenote{\footnotesize A ``board foot'' is a volume, dimension
  L\textsuperscript{3}. It is a square foot (L\^{}2) times an inch (L).}
Should you harvest at year 25? No! That the tree is growing so fast
means that you will have a lot more wood in years 26, 27, etc. The time
to harvest is when the growth is getting smaller so that it is not worth
waiting an extra year.

\hypertarget{dimension-of-a-rate-of-change}{%
\section{Dimension of a rate of
change}\label{dimension-of-a-rate-of-change}}

The function named \(\partial_t f(t)\) which is the derivative of
\(f(t)\) takes the same input as \(f(t)\); the notation makes that
pretty clear. Let's suppose that \(t\) is time and so the dimension of
the input is \([t] = \text{T}\).

The outputs of the two functions, \(\partial_t f(t)\) and \(f(t)\) will
not, in general, have the same dimension. Why not? Recall that a
derivative is a special case of a slope function, the
\textbf{\emph{instantaneous slope function}}. It is easy to calculate a
slope function:

\[{\cal D}_t f(t) \equiv \frac{f(t+h) - f(t)}{h}\] The dimension of the
quantity \(f(t+h) - f(t)\) must be the same as the dimension of
\(f(t)\); the subtraction would not be possible otherwise. Likewise, the
dimension of \(h\) must be the same as the dimension of \(t\); the
addition \(t+h\) wouldn't make sense otherwise.

\marginnote{\begin{footnotesize}

Keep in mind that the dimension \([f(t+h) - f(t)]\) will be the same as
\([f(t)]\). Why? The result of addition and subtraction will always have
the same dimension as the quantities being combined.

\end{footnotesize}}

Whereas the dimension of the output \(f(t)\) is simply
\(\left[f(t)\right]\), the dimension of the quotient
\(\frac{f(t+h) - f(t)}{h}\) will be different. The output of the
derivative function \(\partial_t f(t)\) will be
\[\left[\partial_t f(t)\right] = \left[f(t)\right] / \left[t\right] .\]

Suppose \(x(t)\) is the position of a car as a function of time \(t\).
Position has dimension L. Time has dimension T. The function
\(\partial_t x(t)\) will have dimension L/T. Familiar units for L/T are
miles-per-hour, which you can recognize as velocity.

Another example: Imagine a function pressure() with that takes altitude
above sea level (in km) and output pressure (in kPa,
``kiloPascal'').\sidenote{\footnotesize Air pressure at sea level is about 100
  kiloPascal.} The derivative function, let's call it
\(\partial_\text{altitude} \text{pressure}()\), also takes an input in
km, but produces an output in kPA per km: a rate.

\hypertarget{exercises-1}{%
\section{Exercises}\label{exercises-1}}

\textbf{Part A} What is the average rate of change of this function on
the interval
\(-5 < t < 5\)?https://raw.githubusercontent.com/dtkaplan/Zdrill/main/inst/rev2/rev2-09.png(Choose
the closest answer.)

0.5\hspace{3em}1\hspace{3em}1.5\hspace{3em}2

\textbf{Part B} Consider the average rate of change of this function on
the interval \(-2 < t < 2\).
https://raw.githubusercontent.com/dtkaplan/Zdrill/main/inst/rev2/rev2-09.png''
Which of these statements best describes that average rate of change.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  very close to 1/2
\item
  slightly less than 1
\item
  slightly greater than 1
\item
  very close to 2
\end{enumerate}

\hypertarget{sec-evanescent-h}{%
\chapter{Evanescent h}\label{sec-evanescent-h}}

On a radio broadcast, a baseball fanatic described the path of a home
run slammed just inside the left-field post: ``Coming off the bat, the
ball screamed upwards, passing five stories over the head of the first
baseman and still gaining altitude. Then, somewhere over mid-left-field,
gravity caught up with the ball, forcing it down faster and faster until
it crashed into the cheap seats.'' A gripping image, perhaps, but wrong
about the physics. Gravity does not suddenly catch hold of the ball.
Gravity influences the ball in the same way at all phases of flight,
upward and downward. The vertical velocity of the ball is positive while
climbing and negative on descent, but that velocity is steadily changing
all through the flight: a smooth, almost linear numerical decrease in
velocity from the time the ball leaves the bat to when it lands in the
bleachers.

At each instant, the ball's vertical velocity has a numerical value in
feet-per-second (L T\textsuperscript{-1}). That value changes
continuously. If \(Z(t)\) is the height of the ball at time \(t\), and
\(v_Z(t)\) is the vertical velocity at time \(t\), then the slope
function \[{\cal D}_t Z(t) \equiv \frac{Z(t+h) - Z(t)}{h}\] tells us the
average velocity of the ball over a time interval of \(h\).

The ``average velocity'' is a human construction because \(h\) is a
human choice. The reality pointed to by Newton is that at each instant
in time the ball has a continuously changing velocity. Velocity is the
physical reality: an instantaneous quantity. The ``average velocity'' is
merely a concession to the way we measure, recording the height at two
different times and computing the difference in height divided by the
difference in time.

Our average velocity measurement gets closer to the instantaneous
velocity when we make the time interval \(h\) smaller. But how small?

In the decades after Newton and Leibniz published their work inventing
calculus, there was much philosophical concern about \(h\). Skeptics
pointed out the truth that either \(h\) is zero or it is not. If it is
zero, then the slope function would be
\[{\cal D}_t Z(t) \equiv \frac{Z(t+0) - Z(t)}{0} = \frac{Z(t) - Z(t)}{0} = \frac{0}{0}\ .\]
This \(0/0\) is not a proper arithmetical quantity. On the other hand,
if \(h\) is not zero, then the slope function is a human construction
involving \(h\), not a natural one where \(h\) plays no role.

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/www/the_analyst_title_page.png}

}

\caption{\label{fig-analysis}The title page of the most famous critique
of calculus. The author is George Berkeley.}

\end{marginfigure}

Newton and Leibniz could not address skeptics except by using suggestive
but as yet undefined words. These amount to saying ``\(h\) vanishes,''
or ``\(h\) is an infinitesimal,'' or ``\(h\) is a nascent quantity.''

Another good image of \(h\) becoming as small as possible comes from
University of Oxford mathematician Charles Lutwidge Dodgson (1832-1898).
In \emph{Alice in Wonderland}, Dodgson introduced the character of the
Cheshire Cat.

\begin{figure}[h]

\sidecaption{\label{fig-cheshire-cat}Vanishing \(h\) in the form of the
Chesire Cat from \emph{Alice in Wonderland}.}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/www/Cheshire-cat.png}

}

\end{figure}

The most famous early calculus skeptic, George Berkeley (1685-1753),
decried the vanishing \(h\) as ``notional shadowy Entities.'' He
described \(h\) as ``evanescent,'' meaning ``tending to vanish like
vapor.''\sidenote{\footnotesize \url{https://www.merriam-webster.com/dictionary/evanescent}}
In Berkeley's era, the appearance and vanishing of a vapor might well
have been a mysterious process. Berkeley used it as a criticism.

Strangely, Berkeley was not skeptical about the calculations used in
calculus. He wrote in \emph{The Analysis}, ``I have no Controversy about
your Conclusions, but only about your Logic and Method.''

Not until 1870 was a rigorous theory of vanishing \(h\) accepted by the
mathematics community. This \textbf{\emph{theory of limits}} is
sometimes taught as the centerpiece of introductory calculus even though
it has little connection to the uses of calculus.

\marginnote{\begin{footnotesize}

Indeed, in 1960 a different rigorous mathematical framework, called
\textbf{\emph{\href{https://en.wikipedia.org/wiki/Hyperreal_number}{non-standard
analysis}}}, was introduced. Both theories provide a set of rules for
allowed manipulations of algebraic expressions involving \(h\): when you
can ignore a term involving \(h\) and when you cannot. Such rules were
used to find the derivatives of basic functions (such as the sinusoid)
and to figure out the correct formulas for function products and
function composition as in Chapter Section~\ref{sec-prod-comp-rules}.
Using such formulas does not require re-iterating the historical
development of them.

\end{footnotesize}}

For a physical metaphor for evanescent \(h\), consider the process of
painting. It is evident that a can of paint contains liquid which
somehow becomes solid when brushed on a wall or other object. A better
way to think about paint is as a ``colloid,'' small solid particles
suspended in a liquid. The purpose of the liquid is to keep the solid
particles separate, so that the paint can flow and conform to the
surface of the object being painted. Spreading out the liquid on the
surface leads to rapid evaporation so that only the solid particles
remain. Without the liquid, the particles no longer flow. They remain in
place.

In the expression \[{\cal D}_t p(t) \equiv \frac{p(t+h) - p(t)}{h}\] the
quantity \(h\) is the liquid and each value of the input is a particle
of solid. \(h\) separates the solid particles. \(p(t+h)\) and \(p(t)\)
are the values of the function at the slightly separated inputs. Since
the inputs are held slightly apart, the function values \(p(t+h)\) and
\(p(t)\) can be distinct and the difference between them,
\(p(t+h) - p(t)\), can be non-zero. The final result is to remove the
\(h\) from the difference, that is, divide \(p(t+h)-p(t)\) by \(h\). By
slightly separating the input values, \(h\) makes its contribution to
the process, but in the end, \(h\) evaporates just like the liquid in
paint. That is why \(h\) is evanescent: eventually it vanishes like
vapor.

\hypertarget{evanescence-algebraically}{%
\section{Evanescence algebraically}\label{evanescence-algebraically}}

Let's look at a slope function using evanescent \(h\). To start, we will
analyze \(f(t) \equiv t^2\), one of our pattern-book functions. By
definition, \[{\cal D}_t f(t) \equiv \frac{f(t+h) - f(t)}{h}\ .\] We can
easily evaluate \(f(t+h)\) symbolically:
\[f(t+h) \equiv (t+h)^2 = t^2 + 2 t h + h^2\] Similarly, we can find the
difference \(f(t+h) - f(t)\). It is
\[f(t+h) - f(t) = f(t+h) - t^2 = 2 t h + h^2\ .\] Notice that there is
still some liquid (that is, \(h\)) in the difference. Now we let the
difference start to dry, taking out the \(h\) by dividing the difference
by \(h\):
\[\frac{f(t+h) - f(t)}{h} = \frac{2 t h + h^2}{h} = 2 t + h\ .\] The
rate of chaange of \(f(t)\) has something solid---\(2 t\)---along with a
little bit of liquid \(h\) that we can leave to evaporate to nothing.

\hypertarget{differentiation}{%
\section{Differentiation}\label{differentiation}}

By this point you should be familiar with the definition of the average
rate of change of \(f(t)\) over an interval from \(t\) to \(t+h\):

\[{\cal D}_t f(t) \equiv \frac{f(t+h) - f(t)}{h}\] To indicate that we
want a rate of change with evanescent \(h\), we add a statement to that
effect:

\[\partial_t f(t) \equiv \lim_{h\rightarrow 0} {\cal D}_t f(t) = \lim_{h\rightarrow0}\frac{f(t+h) - f(t)}{h}\ .\]
A proper mathematical phrasing of \(\lim_{h\rightarrow 0}\) is, ``the
limit as \(h\) goes to zero.'' In terms of the paint metaphor, read
\(\lim_{h\rightarrow 0}\) as ``once applied to the wall, let the paint
dry.''

To save space, write \(\lim_{h\rightarrow 0} {\cal D}_t f(t)\)\in a more
compact way: \(\partial_t f(t)\). We use the small symbol \(\partial\)
as a reminiscence of the role that small \(h\) played in the
construction of \(\partial_t f(t)\).

The function \(\partial_t f(t)\) is called the
\textbf{\emph{derivative}} of the function \(f(t)\). The process of
constructing the derivative of a function is called
\textbf{\emph{differentiation}}. The roots of these two words are not
the same. ``Differentiation'' comes from ``difference,'' a nod to
subtraction as in ``the difference between 4 and 3 is 1.'' In contrast,
``derivative'' comes from ``derive,'' whose dictionary definition is
``obtain something from a specified source,'' as in deriving butter from
cream. ``Derive'' is a general term. But ``derivative'' and
``differentiation'' always refers to a specific form of related to rates
of change.

\hypertarget{sec-d-pattern-book}{%
\section{Derivatives of the pattern book
functions}\label{sec-d-pattern-book}}

The pattern-book functions are so widely used that it is helpful to
memorize facts about their derivatives. Remember that, as always, the
derivative of a function is another function. For every pattern-book
function, the derivative is itself built from a pattern-book functions.
To emphasize this, the list below states the rules in terms of the
\emph{names} of the functions, rather than formulas.

\begin{itemize}
\tightlist
\item
  \(\partial_x\) one(x) \(=\) zero(x)
\item
  \(\partial_x\) identity(x) \(=\) one(x)
\item
  \(\partial_x\) square(x) \(=\) 2 identity(x)
\item
  \(\partial_x\) reciprocal(x) \(=\) -1/square(x)
\item
  \(\partial_x\) log(x) \(=\) reciprocal(x)
\item
  \(\partial_x\) sin(x) \(=\) cos(x)
\item
  \(\partial_x\) exp(x) \(=\) exp(x)
\item
  \(\partial_x\) sigmoid(x) \(=\) gaussian(x)
\item
  \(\partial_x\) gaussian(x) \(=\) - x gaussian(x)
\end{itemize}

\marginnote{\begin{footnotesize}

In applications, the pattern-book functions are parameterized,
e.g.~\(\sin\left(\frac{2\pi}{P} t\right)\). In Chapter
Section~\ref{sec-prod-comp-rules} we will introduce the derivatives of
the parameterized functions.

\end{footnotesize}}

Notice that \(h\) does not appear at all in the table of derivatives.
Instead, to use the derivatives of the pattern-book functions we need
only refer to a list of facts, not the process for discovering those
facts.

\begin{figure}

\sidecaption{\label{fig-pattern-deriv-diag}A diagram showing how
differentiation connects the pattern-book functions to one another.}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/www/pattern-derivs-diagram.png}

}

\end{figure}

\hypertarget{notations-for-differentiation}{%
\section{Notations for
differentiation}\label{notations-for-differentiation}}

There are many notations in wide use for differentiation. In this book,
we will denote differentiation in a manner has a close analogy in
computer notation.

We will write the derivative of \(f(x)\) as \(\partial_x f(x)\). If we
had a function \(g(t)\), with \(t\) being the input name, the derivative
would be \(\partial_t g(t)\). Since there is nothing special about the
name of the input in functions with one input, we could just as well
write the one-input function that is the derivative of \(g()\) with
respect to its input as \(\partial_x g(x)\) or \(\partial_z g(z)\) or
even \(\partial_{zebra} g(zebra)\). For functions with just one input,
the notation skeptic might argue that there is no need for a subscript
on \(\partial\), since it will always match the name of the input to the
function being differentiated.

Early in the history of calculus, mathematician Joseph-Louis Lagrange
(1736-1813) proposed a more compact notation for the derivative of a
function with a single input. Rather than \(\partial_x f(x)\), Lagrange
wrote \(f'\). We pronounce this ``f-prime.'' This notation is still
widely used in calculus textbooks because it is compact. But it is not a
viable notation for functions used in modeling since those functions
often have more than one input.

Earlier than Lagrange, Newton used a very compact notation. The
historian needs to be careful, because Newton did not use the term
``derivative'' nor the term ``function.'' Instead, Newton wrote of
``flowing quantities,'' that is, quantities that change in time. For
Newton, typical names for such flowing quantities were \(x\) and \(y\).
He didn't use the parentheses that we now associate with functions, just
the bare name. Newton used ``fluent'' to name such flowing quantities.
Newton's fluents were more or less what we call today ``functions of
time.'' What we now call ``derivatives,'' Newton called ``fluxions.'' If
\(x\) is a fluent, then Newton wrote \(\dot{x}\) to stand for the
fluxion. This is pronounced ``x-dot.'' Like Lagrange's compact prime
notation, Newton's dot notation is still used, particularly in physics.

The mathematician Gottfried Wilhelm Leibniz (1646-1716) was a
contemporary of Newton. Leibniz developed his own notation for calculus,
which was easier to understand than Newton's. In Leibniz's notation, the
derivative (with respect to \(x\)) of \(f(x)\) was written
\[\frac{df}{dx}\ .\] The little \(d\) stands for ``a little bit of'' or
``a little change in,'' so \(\frac{df}{dx}\) makes clear that the
derivative is a ratio of two little bits. In the denominator, \(dx\)
refers to an infinitesimal change in the value of the input \(x\). In
the numerator, the \(df\) names the corresponding change in the output
of \(f()\) when the input is changed.

Leibniz's notation is by far the most widely used in introductory
calculus. It has many advantages compared to Newton's or Lagrange's
notations. For example, it provides an opportunity to name the
with-respect-to input. It also provides a nice notation for an operation
called ``anti-differentiation'' which we will meet in Part
\textbf{?@sec-accumulation-part}. And many a physics or engineering
student has been taught to treat \(dx\) as if it were a number when
doing algebraic manipulations.

The problem with Leibniz's notation, from the perspective of this book,
is that it does not translate well into computer notation. A statement
like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}\SpecialCharTok{/}\NormalTok{dx }\OtherTok{\textless{}{-}}\NormalTok{ x}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{*}\NormalTok{x}
\end{Highlighting}
\end{Shaded}

is a non-starter since the character \texttt{/} is not allowed in a name
in most computer languages, including R.

For functions with multiple inputs, for instance, \(h(x,y,z)\),
differentiation can be done with respect to any input. Leibniz's
notation might possibly be used to indicate which is the with-respect-to
input; the three derivatives of \(h()\) would be written \(dh/dx\) and
\(dh/dy\) and \(dh/dz\). However, mathematical notation did not go in
this direction. Instead, for functions with multiple inputs, the three
derivatives are most usually written \(\partial h/\partial x\) and
\(\partial h/partial y\), and \(\partial h/\partial z\). In the
expression, \(\partial h/\partial y\), the symbol \(\partial\) is
pronounced ``partial,'' The three different derivatives
\(\partial h/\partial x\), \(\partial y /\partial y\), and
\(\partial h/\partial z\) are called ``partial derivatives'' and are the
subject of Chapter Section~\ref{sec-partial-change}.

This book uses \(\partial_x h\), \(\partial_y h\), and \(\partial_z h\)
to denote partial derivatives. This adequately identifies the
with-respect-to input and has a close analog in computer notation. For
instance, if \texttt{f(x,y,z)} has been defined already, the following
statements are entirely valid:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dz\_f }\OtherTok{\textless{}{-}} \FunctionTok{D}\NormalTok{(}\FunctionTok{f}\NormalTok{(x,y,z) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ z)}
\NormalTok{dy\_f }\OtherTok{\textless{}{-}} \FunctionTok{D}\NormalTok{(}\FunctionTok{f}\NormalTok{(x,y,z) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ y)}
\end{Highlighting}
\end{Shaded}

It has been more than 300 years since Leibniz's death. At this point
calculus is so we will established that we don't need the notation
\(df/dx\) to remind us that a derivative is ``a little bit of \(f\)
divided by a little bit of \(x\).''

There are several traditional notations for differentiation of a
single-input function named \(f()\). Here's a list of some of them,
along with the name associated with each:

\begin{itemize}
\item
  Leibnitz: \(\frac{df}{dx}\)
\item
  Partial: \(\frac{\partial f}{\partial x}\)
\item
  Newton (or ``dot''): \(\dot{f}\)
\item
  Lagrange (or ``prime''): \(f'\)
\item
  One-line (used in this book): \(\partial_x f\)
\end{itemize}

To read calculus fluently, you will have to recognize each of these
notations. For functions with one input, they all mean the same thing.
But when functions have multiple inputs, the choice is between the
styles \(\partial f / \partial x\) and \(\partial_x f\). We use the
later because it can easily be incorporated into computer commands.

\hypertarget{exercises-2}{%
\section{Exercises}\label{exercises-2}}

\hypertarget{sec-computing-derivs}{%
\chapter{Constructing derivatives}\label{sec-computing-derivs}}

This chapter showshow to use the computer to construct the derivative of
any function. This is easy because the task of constructing derivatives
is well suited to the computer. {\marginnote{\begin{footnotesize}There
are functions where the derivative cannot be meaningfully defined.
Examples are the absolute-value function or the Heaviside function which
we introduced when discussing piecewise functions in Chapter
\textbf{?@sec-piecewise-intro}. In Chapter
Section~\ref{sec-cont-and-smooth} we will consider the pathological
cases and show you how to spot them at a glance.\end{footnotesize}}}

We will demonstrate two methods:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{\emph{Symbolic differentiation}}, which transforms an
  algebraic formula for a function into a corresponding algebraic
  formula for the derivative.
\item
  \textbf{\emph{Finite-difference}} methods that use a ``small''---but
  not evanescent---\(h\).
\end{enumerate}

Chapter Section~\ref{sec-prod-comp-rules} covers the algorithms used by
the computer to construct symbolic derivatives. One reason for teaching
you to do with paper and pencil the simpler sorts of problems that the
computer does perfectly is analogous to why you learned basic arithmetic
by hand even though a calculator can perform the task more reliably.
Another reason to learn to carry out symbolic differentiation on your
own is to enable you to follow textbook or classroom demonstrations of
formulas which often come from solve a differentiation problem.

\hypertarget{why-differentiate}{%
\section{Why differentiate?}\label{why-differentiate}}

Before showing the easy computer-based methods for constructing the
derivative of a function, it is good to provide some motivation: Why is
differentiation so frequently in so many fields of study and
application?

A primary reason lies in the laws of physics. Newton's Second Law of
Motion reads:

\begin{quote}
\emph{``The change of motion of an object is proportional to the force
impressed; and is made in the direction of the straight line in which
the force is impressed.''}
\end{quote}

Newton defined used position \(x(t)\) as the basis for velocity
\(v(t) = \partial_t x(t)\). ``Change in motion,'' which we call
``acceleration,'' is in turn the derivative \(\partial v(t)\).
Derivatives are also central to the expression of more modern forms of
physics such as quantum theory and general relativity.

Many relationships encountered in the everyday or technical worlds are
more understandable if framed in terms of derivatives. For instance,

\begin{itemize}
\tightlist
\item
  Electrical power is the rate of change with respect to time of
  electrical energy.
\item
  Birth rate is one component of the rate of change with respect to time
  of population. (The others are the death rate and the rates
  immigration and emigration.)
\item
  Interest, as in bank interest or credit card interest, is the rate of
  change with respect to time of assets.
\item
  Inflation is the rate of change with respect to time of prices.
\item
  Disease incidence is one component of the rate of change with respect
  to time of disease prevalence. (The other components are death or
  recovery from disease.)
\item
  Force is the rate of change with respect to position of energy.
\item
  Deficit (as in spending deficits) is the change with respect to time
  of debt.
\end{itemize}

Often, we know one member in such function-and-derivative pairs, but to
need to calculate the other. Many modeling situations call for putting
together different components of change to reveal how some other
quantity of interest will change. For example, modeling the financial
viability of retirement programs such as the US Social Security involves
looking at the changing age structure of the population, the returns on
investment, the changing cost of living, and so on. In Block
\textbf{?@sec-dynamics-part}, we will use derivatives explicitly to
construct models of systems, such as an outbreak of disease, with many
changing parts.

Derivatives also play an important role in design. They play an
important role in the construction and representation of smooth curves,
such as a robot's track or the body of a car. (See Chapter
\textbf{?@sec-splines}.) Control systems that work to stabilize a
airplane's flight or regulate the speed and spacing of cars are based on
derivatives. The notion of ``stability'' itself is defined in terms of
derivatives. (See Chapter \textbf{?@sec-equilibria}.) Algorithms for
optimizing design choices also often make use of derivatives. (See
Chapter \textbf{?@sec-optimization-and-constraint}.)

\begin{intheworld}
Economics as a field makes considerable use of concepts of
calculus---particularly first and second derivatives, the subjects of
this Block---although the names used are peculiar to economics, for
instance, ``elasticity'', ``marginal returns'' and ``diminishing
marginal returns.''

The origins of modern economics, especially the theory of the free
market, are attributed to a book published in 1776, \emph{The Wealth of
Nations}. The author, Adam Smith (1723-1790), lays out dozens of
relationships between different quantities --- wages, labor, stock,
interest, prices, profits, and coinage among others. Yet despite the
invention of calculus a century before \emph{Wealth of Nations}, the
book uses no calculus.

Consider this characteristic statement in \emph{Wealth of Nations}:

\begin{quote}
\emph{The market price of every particular commodity is regulated by the
proportion between the quantity which is brought to market, and the
demand of those who are willing to pay the natural price of the
commodity.}
\end{quote}

Without calculus and the ideas of functions and their derivatives, Smith
was not able to think about prices in a modern way where price is shaped
by demand and supply. Instead, for Smith, each item has a ``natural
price'': a fixed quantity that depends on the amount of labor used to
produce the item. Nowadays, we understand that productivity changes as
new methods of production and new inventions are introduced. But Smith
lived near the end of a centuries-long period of \textbf{\emph{static}}
economies. Transportation, agriculture, manufacture, and population size
were all much as they had been for the past 500 years or longer. James
Watt's steam engine was introduced only in 1776 and it would be decades
before being adapted to the myriad uses of steam power characteristic of
the 19th century. The cotton gin (1793), labor-saving agricultural
machines such as the McCormick reaper (1831), the assembly line (1901),
and the many other innovations of industry all lay in the future when
Smith was writing \emph{Wealth of Nations}.

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/www/cournot-demand-curve.png}

}

\caption{Demand as a \emph{function} of price, as first published by
Antoine-Augustin Cournot in 1836.}

\end{marginfigure}

It took the industrial revolution and nearly a century of intellectual
development before economics had to and could embrace the rapid changes
in the production process. In this dynamical view, supply and demand are
not mere quantities, but \textbf{\emph{functions}} of which price is the
primary input. The tradition in economics is to use the word ``curve''
instead of ``function,'' giving us the phrases ``supply curve'' and
``demand curve.'' Making the transition from quantity to function, that
is, between a single amount and a relationship between amounts, is a
core challenge to those learning economics.

\end{intheworld}

\hypertarget{symbolic-differentiation}{%
\section{Symbolic differentiation}\label{symbolic-differentiation}}

The R/mosaic function \texttt{D()} takes a formula for a function and
produces the derivative. It uses the same sort of tilde expression used
by \texttt{makeFun()} or \texttt{contour\_plot()} or the other R/mosaic
tools. For instance,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{D}\NormalTok{(t }\SpecialCharTok{*} \FunctionTok{sin}\NormalTok{(t) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ t)}
\DocumentationTok{\#\# function (t) }
\DocumentationTok{\#\# sin(t) + t * cos(t)}
\end{Highlighting}
\end{Shaded}

If you prefer, you can use \texttt{makeFun()} to define a function, then
hand that function to \texttt{D()} for differentiation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myf }\OtherTok{\textless{}{-}} \FunctionTok{makeFun}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(y }\SpecialCharTok{*} \FunctionTok{pnorm}\NormalTok{(}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ x}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\AttributeTok{mean=}\DecValTok{2}\NormalTok{, }\AttributeTok{sd=}\DecValTok{3}\NormalTok{)) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{\&}\NormalTok{ y)}
\NormalTok{dx\_myf }\OtherTok{\textless{}{-}} \FunctionTok{D}\NormalTok{(}\FunctionTok{myf}\NormalTok{(x, y) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{y=}\DecValTok{3}\NormalTok{)}
\NormalTok{dx\_myf}
\DocumentationTok{\#\# function (x, y = 3) }
\DocumentationTok{\#\# \{}
\DocumentationTok{\#\#     .e1 \textless{}{-} 1 + x\^{}2}
\DocumentationTok{\#\#     x * y * dnorm(.e1, 2, 3)/sqrt(y * pnorm(.e1, mean = 2, sd = 3))}
\DocumentationTok{\#\# \}}
\end{Highlighting}
\end{Shaded}

In the right side of the tilde expression handed off to \texttt{D()}
names the with-respect-to input. This is similar to the tilde
expressions used in plotting, which name the inputs that form the
graphics domain. But it contrasts with the tilde expressions in
\texttt{makeFun()}, where the right-hand side specifies the order in
which you want the inputs to appear.

\begin{example}

Needless to say, \texttt{D()} knows the rules for the derivatives of the
pattern-book functions introduced in Section
Section~\ref{sec-d-pattern-book}. For instance,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{D}\NormalTok{(}\FunctionTok{sin}\NormalTok{(t) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ t)}
\DocumentationTok{\#\# function (t) }
\DocumentationTok{\#\# cos(t)}
\FunctionTok{D}\NormalTok{(}\FunctionTok{log}\NormalTok{(x) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\DocumentationTok{\#\# function (x) }
\DocumentationTok{\#\# 1/x}
\FunctionTok{D}\NormalTok{(}\FunctionTok{exp}\NormalTok{(x) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\DocumentationTok{\#\# function (x) }
\DocumentationTok{\#\# exp(x)}
\FunctionTok{D}\NormalTok{(x}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\DocumentationTok{\#\# function (x) }
\DocumentationTok{\#\# 2 * x}
\end{Highlighting}
\end{Shaded}

\end{example}

\hypertarget{finite-difference-derivatives}{%
\section{Finite-difference
derivatives}\label{finite-difference-derivatives}}

Whenever you have a formula amenable to the construction of a symbolic
derivative, that is what you should use. Finite-difference derivatives
are useful in those situation where you don't have such a formula. The
calculation is simple but has a weakness that points out the advantages
of the evanescent-\(h\) approach.

For a function \(f(x)\) and a ``small,'' non-zero number \(h\), the
finite-difference approximates the derivative with this formula:

\[\partial_x f(x) \approx \frac{f(x+h) - f(x-h)}{2h}\ .\] To
demonstrate, let's construct the finite-difference approximation to
\(\partial_x \sin(x)\). Since we already know the symbolic
derivative---it is \(\partial_x \sin(x) = \cos(x)\)---there is no
genuinely practical purpose for this demonstration. Still, it can serve
to confirm the symbolic rule.

We will call the finite-difference approximation \texttt{fq\_sin()} and
use \texttt{makeFun()} to construct it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fq\_sin }\OtherTok{\textless{}{-}} \FunctionTok{makeFun}\NormalTok{((}\FunctionTok{sin}\NormalTok{(x}\SpecialCharTok{+}\NormalTok{h)}\SpecialCharTok{{-}} \FunctionTok{sin}\NormalTok{(x}\SpecialCharTok{{-}}\NormalTok{h))}\SpecialCharTok{/}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{h) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{h=}\FloatTok{0.01}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Notice that \texttt{fq\_sin()} has a parameter, \texttt{h} whose default
value is being set to \texttt{0.01}. Whether 0.01 is ``small'' or not
depends on the context. Operationally, we define ``small'' to be a value
that gives practically the same result even if it is made smaller by a
factor of 2 or 10.

As a demonstration that \texttt{fq\_sin()} with \(h=0.01\) approximates
the genuine \(\partial_x \sin(x)\), we exploit our knowledge that
\(\partial_x \sin(x) = \cos(x)\). Figure~\ref{fig-confirm-fq-sin} plots
out the difference between the the \(h=0.01\) approximation and the
genuine derivative.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{slice\_plot}\NormalTok{(}\FunctionTok{fq\_sin}\NormalTok{(x, }\AttributeTok{h=}\FloatTok{0.01}\NormalTok{) }\SpecialCharTok{{-}} \FunctionTok{cos}\NormalTok{(x) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\FunctionTok{domain}\NormalTok{(}\AttributeTok{x=}\SpecialCharTok{{-}}\DecValTok{10}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_plot}\NormalTok{(}\FunctionTok{fq\_sin}\NormalTok{(x, }\AttributeTok{h=}\FloatTok{0.001}\NormalTok{) }\SpecialCharTok{{-}} \FunctionTok{cos}\NormalTok{(x) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{color=}\StringTok{"magenta"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{gf\_labs}\NormalTok{(}\AttributeTok{y=}\StringTok{"Error from true value."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/19-computing_files/figure-pdf/fig-confirm-fq-sin-1.pdf}

}

\caption{\label{fig-confirm-fq-sin}Comparing \texttt{fq\_sin()} to
\(\partial_x \sin(x)\) for two values of \(h\).}

\end{figure}

You will need to look carefully at the vertical axis scale in
Figure~\ref{fig-confirm-fq-sin} to see what's happening. For \(h=0.01\),
\texttt{fq\_sin()} is not exactly the same as \texttt{cos()}, but it is
close, always being within \$\pm\$0.00017. For many purposes, this would
be accurate enough. But not for all purposes. We can make the
approximation better by using a smaller \(h\). For instance, the
\(h=0.001\) version of \texttt{fq\_sin()} is accurate to within
\$\pm\$0.0000017.

In practical use, one employs the finite-difference method in those
cases where one does \emph{not already know} the exact derivative
function. This would be the case, for example, if the function is a
sound wave recorded in the form of an MP3 audio file.

In such situations, a practical way to determine what is a small \(h\)
is to pick one based on your understanding of the situation. For
example, much of what we perceive of sound involves mixtures of
sinusoids with periods longer than one-two-thousandth of a second, so
you might start with \(h\) of 0.002 seconds. Use this guess about \(h\)
to construct a candidate finite-difference approximation. Then,
construct another candidate using a smaller \texttt{h}, say, 0.0002
seconds. If the two candidates are a close match to one another, then
you have confirmed that your choice of \(h\) is adequate.

It is tempting to think that the approximation gets better and better as
\texttt{h} is made even smaller. But that is not necessarily true for
computer calculations. The reason is that quantities on the computer
have only a limited precision: about 15 digits. To illustrate, let's
calculate a simple quantity, \((\sqrt{3})^2 - 3\). Mathematically, this
quantity is exactly zero. On the computer, however it is not quite zero:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(}\DecValTok{3}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}} \DecValTok{3}
\DocumentationTok{\#\# [1] {-}4.440892e{-}16}
\end{Highlighting}
\end{Shaded}

We can see this loss of precision at work if we make \texttt{h} very
small in the finite-difference approximation to \(\partial_x \sin(x)\).
In Figure~\ref{fig-too-small-h} we are using
\texttt{h\ =\ 0.000000000001}. The result is unsatisfactory.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{slice\_plot}\NormalTok{(  }\FunctionTok{fq\_sin}\NormalTok{(x, }\AttributeTok{h=}\FloatTok{0.000000000001}\NormalTok{) }\SpecialCharTok{{-}} \FunctionTok{cos}\NormalTok{(x) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }
           \FunctionTok{domain}\NormalTok{(}\AttributeTok{x=}\SpecialCharTok{{-}}\DecValTok{10}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_plot}\NormalTok{(}\FunctionTok{fq\_sin}\NormalTok{(x, }\AttributeTok{h=}\FloatTok{0.0000000000001}\NormalTok{) }\SpecialCharTok{{-}} \FunctionTok{cos}\NormalTok{(x) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x,}
             \AttributeTok{color=}\StringTok{"magenta"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{gf\_labs}\NormalTok{(}\AttributeTok{y=}\StringTok{"Error from true value."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\sidecaption{\label{fig-too-small-h}In computer calculations, using too
small an \texttt{h} leads to a loss of accuracy in the finite-difference
approximation.}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/19-computing_files/figure-pdf/fig-too-small-h-1.pdf}

}

\end{figure}

\hypertarget{second-and-higher-order-derivatives}{%
\section{Second and higher-order
derivatives}\label{second-and-higher-order-derivatives}}

Many applications call for differentiating a derivative or even
differentiating the derivative of a derivative. In English, such phrases
are hard to read. They are much simpler using mathematical notation.

\begin{itemize}
\tightlist
\item
  \(f(x)\) a function
\item
  \(\partial_x f(x)\) the derivative of \(f(x)\)
\item
  \(\partial_x \partial_x f(x)\), the \textbf{\emph{second derivative}}
  of \(f(x)\), usually written even more concisely as
  \(\partial_{xx}f f(x)\).
\end{itemize}

There are third-order derivatives, fourth-order, and on up, although
they are not often used.

To compute a second-order derivative \(\partial_{xx} f(x)\), first
differentiate \(f(x)\) to produce \(\partial_x f(x)\). Then, still using
the techniques described earlier in this chapter, differentiate
\(\partial_x f(x)\).

There is a shortcut for constructing high-order derivatives using
\texttt{D()} in a single step. On the right-hand side of the tilde
expression, list the with-respect-to name repeatedly. For instance:

\begin{itemize}
\tightlist
\item
  The second derivative \(\partial_{xx} \sin(x)\):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{D}\NormalTok{(}\FunctionTok{sin}\NormalTok{(x) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{\&}\NormalTok{ x)}
\DocumentationTok{\#\# function (x) }
\DocumentationTok{\#\# {-}sin(x)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The third derivative \(\partial_{xxx} \ln(x)\):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{D}\NormalTok{(}\FunctionTok{log}\NormalTok{(x) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{\&}\NormalTok{ x }\SpecialCharTok{\&}\NormalTok{ x)}
\DocumentationTok{\#\# function (x) }
\DocumentationTok{\#\# 2/x\^{}3}
\end{Highlighting}
\end{Shaded}

\begin{example}
Physics students learn a formula for the position of an object in free
fall dropped from a height \(x_0\) and at an initial velocity \(v_0\):
\[ x(t) \equiv -\frac{1}{2} g t^2 + v_0 t + x_0\ .\] The acceleration of
the object is the second derivative \(\partial_{tt} x(t)\). Use
\texttt{D()} to find the object's acceleration.

The second derivative of \(x(t)\) with respect to \(t\) is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{D}\NormalTok{(}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{g}\SpecialCharTok{*}\NormalTok{t}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ v0}\SpecialCharTok{*}\NormalTok{t }\SpecialCharTok{+}\NormalTok{ x0 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ t }\SpecialCharTok{\&}\NormalTok{ t)}
\DocumentationTok{\#\# function (t, g, v0, x0) }
\DocumentationTok{\#\# g}
\end{Highlighting}
\end{Shaded}

The acceleration does not depend on \(t\); it is the constant \(g\). No
wonder \(g\) is called ``gravitational acceleration.''

\end{example}

\hypertarget{exercises-3}{%
\section{Exercises}\label{exercises-3}}

`\{=html\}

\hypertarget{sec-concavity-curvature}{%
\chapter{Concavity and curvature}\label{sec-concavity-curvature}}

Looking at the graph of a function, our eyes immediately register the
slope at any point we focus on. A glance shows whether the slope at that
point is positive or negative. Comparing the slopes at two locales is
also an automatic visual task: most people have little difficulty saying
which slope is steeper.

One consequence of this visual ability is that it is easy to recognize
whether a line that touches the graph at a point is tangent to the
graph.

There are other aspects of functions, introduced in Section
@ref(word-descriptions), that are also readily discerned from a glance
at the function graph.

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Concavity}}: We can tell within each locale whether the
  function is concave down, concave up, or not concave.
\item
  \textbf{\emph{Curvature}}: Generalizing the tangent line capability a
  bit, we can do a pretty good job of eyeballing the tangent circle
  recognizing whether a circle has much too large or much too small a
  radius..
\item
  \textbf{\emph{Smoothness}}: We can often distinguish smooth functions
  from non-smooth ones. But, as you will see, there are some kinds of
  smoothness that the trained eye can discern and others that are not
  apparent to the eye.
\end{itemize}

The following exercises are simply meant to test your \textbf{visual}
acuity in spotting concavity, tangency, and smoothness. Then we will
move on to the calculations involved.

\hypertarget{quantifing-concavity-and-curvature}{%
\section{Quantifing concavity and
curvature}\label{quantifing-concavity-and-curvature}}

It often happens in building models that the modeler (you!) knows
something about the concavity and/or curvature of a function. For
example, \textbf{\emph{concavity}} is important in classical economics;
the curve for supply as a function of price is concave down while the
curve for demand as a function of price is concave up. For a train, car,
or plane, there are forces that depend on the curvature of the track,
road, or trajectory. If you are designing a road, you will need to
calculate the curvature to know if the road is safe at the indicated
speed.

It turns out that quantifying these properties of functions or shapes is
naturally done by calculating derivatives.

\begin{intheworld}
Imagine designing a highway. Due to the terrain, part of the road is
oriented east-west and another part north-south. For vehicles to use the
road, those two parts need to be connected together! (In math-speak, we
might say that the road has to be \textbf{\emph{continuous}}, but this
is just common sense.)

From your experience with highways, you know the connection will be a
\textbf{\emph{smooth}} curve. If the curve is part of a circle, then the
design needs to specify the \textbf{\emph{radius of curvature}} of the
circle. Too tight a radius and the traffic won't be able to handle the
centrifugal force and will drift or skid off the road. A big radius is
needed for safety, but making the radius bigger than required adds
additional cost in road construction.

Real-world highway on- and off-ramps are usually not exactly sections of
a circle, so specifying the shape of the ramp is not as simple setting
the radius of the curve. The radius needs to \textbf{change} at the
entry and exit of the curve. Why? Here's an explanation from the
American Association of State Highway and Transportation Officials
*Policy on Geometric Design of Highways and Streets (1994):

\emph{Any motor vehicle follows a transition path as it enters or leaves
a circular horizontal curve. The steering change and the consequent gain
or loss of centrifugal force cannot be effected instantly. For most
curves the average driver can effect a suitable transition path within
the limits of normal lane width. However, with combinations of high
speed and sharp curvature the resultant longer transition can result in
crowding and sometimes actual occupation of adjoining lanes. In such
instances transition curves would be appropriate because they make it
easier for a driver to confine the vehicle to his or her own lane. The
employment of transition curves between tangents and sharp circular
curves and between circular curves of substantially different radii
warrants consideration.}

Later in this chapter, you will see the calculus concepts that relate to
designing a road with a gently changing curvature. (Hint, but don't get
scared: it is the \emph{third} derivative.)

\end{intheworld}

We will frame the calculations in terms of a function \(f(x)\).
Depending on the setting, \(x\) might be the price of a product and
\(f(x)\) the demand for that product. Or the graph of \(f(x)\) might be
the path of a road drawn in \((x,y)\) coordinates or the reach of a
robot arm as a function of time. Remember that \(f()\) is just a
\emph{pronoun} that I'm using instead of a proper descriptive name. I
use such pronouns (also, \(g()\), \(h()\), the ``she'' and ``he'' of
mathematical language) when writing about the general properties of
functions.

\hypertarget{sec-concavity-deriv}{%
\section{Concavity}\label{sec-concavity-deriv}}

Recall that to find the slope of a function \(f(x)\) at any input \(x\),
you compute the \textbf{\emph{derivative}} of that function, which we've
been writing \(\partial_x\,f(x)\). Plug in some value for the input
\(x\) and the output of \(\partial_x\, f(x)\) will be the slope of
\(f(x)\) at that input. (Section~\ref{sec-computing-derivs} introduced
some techniques for computing the derivative of any given function.)

Now we want to show how differentiation can be used to quantify the
concavity of a function. It will help if we augment our nomenclature a
bit. When we speak of the ``derivative'' of a function, we mean
something that might be more completely expressed as the
\textbf{\emph{first derivative}} of the function. Just that name
naturally suggests that there will be a \textbf{\emph{second
derivative}}, a \textbf{\emph{third derivative}}, and so on.

Figure~\ref{fig-changing-slope} shows a simple function that is concave
down.

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/20-concavity_files/figure-pdf/fig-changing-slope-1.pdf}

}

\caption{\label{fig-changing-slope}A function that is concave down.}

\end{marginfigure}

Notice that the concavity is not about the slope. The curve in
Figure~\ref{fig-changing-slope} is concave down everywhere in the domain
\(0 \leq x \leq 4\), but the slope is positive for \(0 \leq x \leq 1\)
and negative for larger \(x\). Slope and concavity are two different
aspects of a function.

As introduced in Chapter \textbf{?@sec-fun-describing}, the concavity of
a function describes not the slope, but the \textbf{\emph{change in the
slope}}. Figure~\ref{fig-changing-slope2} adds some annotations on top
of the graph in Figure~\ref{fig-changing-slope}. In the subdomain marked
A, the function slope is positive while in the subdomain B, the function
slope is negative. It is this \textbf{\emph{transition}} from the slope
in A to the slope in B that corresponds to the concavity of the function
between A and B.

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/20-concavity_files/figure-pdf/fig-changing-slope2-1.pdf}

}

\caption{\label{fig-changing-slope2}Concavity is about how the slope
\emph{changes} from one place in the domain to another.}

\end{marginfigure}

Similarly, the concavity of the function between B and C, reflects the
\textbf{\emph{transition}} in the slope from B to C. Even though the
slope is negative in both B and C, the change in slope tells us about
the concavity.

Let's look at this using symbolic notation. Keep in mind that the
function graphed is \(f(x)\) while the slope is the function
\(\partial_x\,f(x)\). We've seen that the concavity is indicated by the
change in slope of \(f()\), that is, the change in
\(\partial_x\, f(x)\). We will go back to our standard way of describing
the rate of change near an input \(x\):

\[\text{concavity.of.f}(x) \equiv\ \text{rate of change in}\ \partial_x\, f(x) = \partial_x [\partial_x f(x)] \\
\\
= \lim_{h\rightarrow 0}\frac{\partial_x f(x+h) - \partial_x f(x)}{h}\]
We are defining the concavity of a function \(f()\) at any input \(x\)
to be \(\partial_x [\partial_x f(x)]\). We create the
concavity\_of\_f(x) function by applying differentiation \textbf{twice}
to the function \(f()\).

Such a double differentiation of a function \(f(x)\) is called the
\textbf{\emph{second derivative}} of \(f(x)\). The second derivative is
so important in applications that it has its own compact notation:
\[\text{second derivative of}\ f()\ \text{is written}\ \partial_{xx} f(x)\]
Look carefully to see the difference between the first derivative
\(\partial_x f(x)\) and the second derivative \(\partial_{xx} f(x)\): it
is all in the double subscript \(_{xx}\).

Computing the second derivative is merely a matter of computing the
first derivative \(\partial_x f(x)\) and then computing the (first)
derivative of \(\partial_x f(x)\). In R this process looks like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dx\_f  }\OtherTok{\textless{}{-}} \FunctionTok{D}\NormalTok{(   }\FunctionTok{f}\NormalTok{(x) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)   }\CommentTok{\# First deriv. of f()}
\NormalTok{dxx\_f }\OtherTok{\textless{}{-}} \FunctionTok{D}\NormalTok{(}\FunctionTok{dx\_f}\NormalTok{(x) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)   }\CommentTok{\# Second deriv. of f()}
\end{Highlighting}
\end{Shaded}

\marginnote{\begin{footnotesize}

As a shortcut for the two-step process above, for the second derivative
you can use a notation which doubles up on the \texttt{x} on the
right-hand side of the tilde:
\texttt{dxx\_f\ \textless{}-\ D(f(x)\ \textasciitilde{}\ x\ \&\ x)}

\end{footnotesize}}

\hypertarget{sec-curvature-definition}{%
\section{Curvature}\label{sec-curvature-definition}}

As you see from Section Section~\ref{sec-concavity-deriv}, it is easy to
quantify the concavity of a function \(f(x)\): just evaluate the second
derivative \(\partial_{xx} f(x)\). But it turns out that people are very
poor at estimating the quantitative value of concavity by eye.

To illustrate, consider the square function, \(f(x) \equiv x^2\). (See
Figure~\ref{fig-square34}.)

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/20-concavity_files/figure-pdf/fig-square34-1.pdf}

}

\caption{\label{fig-square34}Does the concavity of the square function
vary with \(x\)?}

\end{marginfigure}

Clearly, the square function is concave up. Now a test: Looking at the
graph of the square function, where is the concavity the largest?
\textbf{Don't read on until you've pointed where you think the concavity
is largest.}

With your answer to the test question in mind, let's calculate the
concavity of the square function using derivatives.

\[f(x) \equiv x^2\ \text{      so     }\ 
\partial_x f(x) = 2 x\ \text{     and therefore     }\ \partial_{xx} f(x) = 2\]

The second derivative of \(f(x)\) is positive, as you would expect for a
function that is concave up. What you might not expect, however, is that
the second derivative is \textbf{\emph{constant}}.

The concavity-related property that the human eye reads from the graph
of a function is not the concavity itself, but the
\textbf{\emph{curvature}} of the function. The curvature of \(f(x)\) at
a point \(x_0\) is defined to be the radius of the circle that is
tangent to the function at \(x_0\).

Figure~\ref{fig-inscribed-circles} illustrates the changing curvature of
\(f(x) \equiv x^2\) by inscribing tangent circles at several points on
the function graph, marked with dots. You can see the tangency of the
circle to the function graph; the function's thin black line goes right
down the middle of the broader lines used to draw the circles.

\begin{figure}

\sidecaption{\label{fig-inscribed-circles}At any point on the graph of a
smooth function, a circle tangent to the graph can be drawn. The radius
of this circle is \(1/{\cal K}\).}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/20-concavity_files/figure-pdf/fig-inscribed-circles-1.pdf}

}

\end{figure}

Black dots have been put along the graph at the points where the graph
of the function is tangent to the inscribed circle. The visual sign of
tangency is that the graph of the function goes right down the center of
the circle.

The inscribed circle at \(x=0\) is tightest. The circle at \(x=1\) has a
somewhat larger radius. The radius of the circle at \(x=-1.5\) is the
largest of all. Whereas the concavity is the same at all points on the
graph, the visual impression that the function is most highly curved
near \(x=0\) is better captured by the radius of the inscribed circle.
The radius of the inscribed circle at any point is the reciprocal of a
quantity \({\cal K}\) called the \textbf{\emph{curvature}}.

The curvature \({\cal K}\) of a function \(f(x)\) depends on both the
first and second derivative. The formula for curvature \(K\) is somewhat
off-putting; \textbf{you are not expected to memorize it}. But you can
see where \(\partial x f()\) and \(\partial_{xx}f()\) come into play.

\[{\cal K}_f  \equiv \frac{\left|\partial_{xx} f(x)\right|}{\ \ \ \ \left|1 + \left[\strut\partial_x f(x)\right]^2\right|^{3/2}}\]

Mathematically, the curvature \(\cal K\) corresponds to the reciprocal
of the radius of the tangent circle. When the tangent circle is tight,
\(\cal K\) is large. When the tangent circle has a very large radius,
that is, the function is very close to approximating a straight line,
\(\cal K\) is very small.

\begin{intheworld}
Returning to the highway design example earlier in the chapter \ldots{}
The \emph{Policy on geometric design of highways and streets} called for
the curvature of a road to change gently, giving the driver time to
adjust the steering and accomodate the centrifugal force of the car
going around the curve.

Changing curvature implies that \(\partial_x {\cal K}\) is non-zero.
Since \({\cal K}\) depends on the first and second derivatives of
\(f(x)\), the \emph{Policy} on gradual change means that the
\textbf{third derivative} of \(f(x)\) is non-zero.

\end{intheworld}

\hypertarget{exercises-4}{%
\section{Exercises}\label{exercises-4}}

\hypertarget{sec-cont-and-smooth}{%
\chapter{Continuity and smoothness}\label{sec-cont-and-smooth}}

You've seen how various properties of a function---whether it is
monotonic, how it slopes, whether it is concave up or down (or not at
all), curvature, etc.---can be related to the first and second
derivatives of the function.

In this chapter, we will elaborate on \textbf{\emph{continuity}}, one of
the ideas introduced in Section \textbf{?@sec-word-descriptions}, and
use the concept of continuity to characterize functions in a new way:
their \textbf{\emph{smoothness}}.

\hypertarget{continuity}{%
\section{Continuity}\label{continuity}}

The intuition behind continuity is simple: If you can draw the graph of
a function without lifting the pencil from the paper, the function is
continuous.

Continuity can be an important attribute of a modeling function. Often,
we are modeling phenomena where a \textbf{\emph{small change in input}}
is expected to produce a \textbf{\emph{small change in output}}. For
instance, if your income changes by one penny, you would expect your
lifestyle not to change by much. If the temperature of an oven changes
by 1 degree, you don't expect the quality of the cake you are baking to
change in any noticeable way.

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/21-cont-and-smooth_files/figure-pdf/fig-heaviside2-1.pdf}

}

\caption{\label{fig-heaviside2}The Heaviside function is piecewise
constant with a discontiuity at \(x=0\).}

\end{marginfigure}

All of our basic modeling functions are continuous over their entire
input domain.\sidenote{\footnotesize The domain of the function \(1/x\) is the whole
  number line, except 0, where the positive and negative branches fail
  to meet up.} To illustrate \textbf{\emph{discontinuity}} we will
consider piecewise functions, as introduced in Section
\textbf{?@sec-fun-piecewise}. The \textbf{\emph{Heaviside function}},
graphed in Figure~\ref{fig-heaviside2} is discontinuous.

Drawing the graph of the Heaviside function \(H(x)\) involves lifting
the pencil at \(x=0\).

In contrast, the piecewise \textbf{\emph{ramp function}}
(Figure~\ref{fig-ramp2}) is continuous; you don't need to lift the
pencil from the paper to draw the ramp function.

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/21-cont-and-smooth_files/figure-pdf/fig-ramp2-1.pdf}

}

\caption{\label{fig-ramp2}The ramp function is a continuous piecewise
function.}

\end{marginfigure}

Imagine that you were constructing a model of plant growth as a function
of the amount of water (in cc) provided each day. The plant needs about
20 cc of water to thrive. Let's say that you use the Heaviside function
for the model, say \(H(W-20)\), where an output of 1 means the plant
thrives and a output 0 means the plant does not. The model implies that
if you provide 20.001 cc of water, the plant will thrive. But if you are
stingy, and provide only 19.999 cc of water, the plant will die. In
other words, a very small change in the input can lead to a large change
in the output.

Common sense suggests that a change of 0.002 cc in the amount of
water---that is a small fraction of a drop, 2 cubic millimeters of
volume, is not going to lead to a qualitative change in output. So you
might prefer to use a sigmoidal function as your model rather than a
Heaviside function.

On the other hand, sometimes a very small change in input does lead to a
large change in output. For instance, a sensible model of the hardness
of water as a function of temperature would include a discontinuity at
\(32^\circ\)F, the temperature at which water turns to ice.

One of author Charles Dickens's famous characters described the
relationship between income, expenditure, and happiness this way:

\begin{quote}
``\emph{Annual income 20 pounds, annual expenditure 19 {[}pounds{]} 19
{[}shillings{]} and six {[}pence{]}, result happiness. Annual income 20
pounds, annual expenditure 20 pounds ought and six, result misery.''}
--- the character
\href{https://en.wikipedia.org/wiki/Wilkins_Micawber}{Wilkins Micawber}
in \emph{David Copperfield}
\end{quote}

Macawber was referring to the common situation in pre-20th century
England of putting debtors in prison, regardless of the size of their
debt. Macawber's statement suggests he would model happiness as a
Heaviside function \(H(\text{income}- \text{expenditure})\).

Whenever the output of a function is a binary (yes-or-no) value, you can
anticipate that a model will involve a discontinuous function.

\hypertarget{discontinuity}{%
\section{Discontinuity}\label{discontinuity}}

Recall the logical path that led us to the idea of the derivative of a
function. We started with the \textbf{\emph{differencing operator}},
which takes as input a function and a ``small'' value of \(h\):
\[{\cal D}_x f(x) \equiv \frac{f(x+h) - f(x)}{h}\] Then, through
algebraic manipulation and numerical experiments we found that, once
\(h\) is small enough, the graph of the slope function
\({\cal D}_x f(x)\) does not depend on \(h\). And so we defined a
function \(\partial_x f(x)\) where \(h\) does not play a role, writing
\(\lim_{h\rightarrow 0}\) to remember our care to never divide by zero.
\[\partial_x f(x) \equiv \lim_{h\rightarrow 0} \frac{f(x+h) - f(x)}{h}\ .\]
Conveniently, we found that the derivatives of the pattern-book
functions can be written in terms of the pattern-book functions
\emph{without making any reference to \(h\)}. For instance:

\begin{itemize}
\tightlist
\item
  \(\partial_x \ln(x) = 1/x\) No \(h\) appears.
\item
  \(\partial_x e^x = e^x\) No \(h\) appears
\item
  \(\partial_x x^p = p\, x^{p-1}\) No \(h\) appears.
\item
  and so on.
\end{itemize}

With discontinuous functions, we have no such luck.
\textbf{?@fig-d-heaviside} shows what happens if we compute
\({\cal D}_x H(x)\), the derivative of the Heaviside function, for
smaller and smaller \(h\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{H }\OtherTok{\textless{}{-}} \FunctionTok{makeFun}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(x }\SpecialCharTok{\textgreater{}=}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\NormalTok{DH01   }\OtherTok{\textless{}{-}} \FunctionTok{makeFun}\NormalTok{((}\FunctionTok{H}\NormalTok{(x }\SpecialCharTok{+} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{{-}} \FunctionTok{H}\NormalTok{(x))}\SpecialCharTok{/}\FloatTok{0.1} \SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\NormalTok{DH001  }\OtherTok{\textless{}{-}} \FunctionTok{makeFun}\NormalTok{((}\FunctionTok{H}\NormalTok{(x }\SpecialCharTok{+} \FloatTok{0.01}\NormalTok{) }\SpecialCharTok{{-}} \FunctionTok{H}\NormalTok{(x))}\SpecialCharTok{/}\FloatTok{0.01} \SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\NormalTok{DH0001 }\OtherTok{\textless{}{-}} \FunctionTok{makeFun}\NormalTok{((}\FunctionTok{H}\NormalTok{(x }\SpecialCharTok{+} \FloatTok{0.001}\NormalTok{) }\SpecialCharTok{{-}} \FunctionTok{H}\NormalTok{(x))}\SpecialCharTok{/}\FloatTok{0.001} \SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\FunctionTok{slice\_plot}\NormalTok{(}\FunctionTok{DH01}\NormalTok{(x) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\FunctionTok{domain}\NormalTok{(}\AttributeTok{x=}\SpecialCharTok{{-}}\FloatTok{0.02}\SpecialCharTok{:}\FloatTok{0.02}\NormalTok{),}
           \AttributeTok{npts=}\DecValTok{500}\NormalTok{, }\AttributeTok{color=}\StringTok{"red"}\NormalTok{, }\AttributeTok{size=}\DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_plot}\NormalTok{(}\FunctionTok{DH001}\NormalTok{(x) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x,}
           \AttributeTok{color=}\StringTok{"darkgreen"}\NormalTok{, }\AttributeTok{npts=}\DecValTok{500}\NormalTok{, }\AttributeTok{size=}\DecValTok{3}\NormalTok{, }\AttributeTok{alpha=}\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_plot}\NormalTok{(}\FunctionTok{DH0001}\NormalTok{(x) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x,}
           \AttributeTok{color=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{npts=}\DecValTok{500}\NormalTok{, }\AttributeTok{alpha=}\FloatTok{0.5}\NormalTok{, }\AttributeTok{size=}\DecValTok{2}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/21-cont-and-smooth_files/figure-pdf/fig-D-heaviside-1.pdf}

}

\caption{\label{fig-D-heaviside}\({\cal D}_x H(x)\), the slope function
of the discontinuous Heaviside, function, depends on the value of \(h\)
used for the slope function. (Red: \(h=0.1\); Green: \(h=0.01\); Blue
\(h=0.001\))}

\end{figure}

Differencing the Heaviside function produces very different functions
depending on the value of \(h\). The bump near \(x=0\) gets taller and
taller as \(h\) gets smaller. Mathematicians would describe this
situation as
\[\lim_{h\rightarrow0}{\cal D}_x H(x=0) \equiv \lim_{h\rightarrow 0} \frac{H(0+h) - H(0)}{h}\ \ \ \text{does not exist}.\]
Of course, for any given value of \(h\), e.g.~\(h=0.000001\), the
function \({\cal D}_x H(x)\) has a definite shape. But that shape keeps
changing as \(h \rightarrow 0\), so we cannot point to any specific
shape as the ``limit as \(h \rightarrow 0\).''

Since there is no convergence in the shape of \({\cal D}_x H(0)\) as
\(h\) gets smaller, it is fair to say that the Heaviside function does
not have a derivative at \(x=0\). But away from \(x=0\), the Heaviside
function has a perfectly sensible derivative: \(\partial_x H(x) = 0\)
for \(x\neq 0\). But there is no derivative at \(x=0\).

\hypertarget{smoothness}{%
\section{Smoothness}\label{smoothness}}

\textbf{\emph{Smoothness}} is a different concept than continuity,
although the two are related. Most simply, any discontinuous function is
not smooth at any input where a discontinuity occurs. But even the
continuous ramp function is not smooth at the start of the ramp.
Intuitively, imagine you were sliding your hand along the ramp function.
You would feel the crease at \(x=0\).

A function is not smooth if the derivative of that function is
discontinuous. For instance, the derivative of the ramp function is the
Heaviside function, so the ramp is not smooth at \(x=0\).

All of our basic modeling functions are smooth everywhere in their
domain. In particular, the derivatives of the basic modeling functions
are continuous, as are the second derivative, third derivative, and so
on down the line. Such functions are called \textbf{\emph{C-infinity}},
written \(C^\infty\). The superscript \(\infty\) means that every order
of derivative is continuous.

You cannot tell from the plot that the second derivative is
discontinuous. But if you were in a plane flying along that trajectory,
you would feel a jerk as you crossed \(x=0\).

Mathematicians quantify the ``smoothness'' of a function by looking at
the continuity of the function and its derivatives. The mathematical
definition of smoothness is straightforward and phrased in terms of
derivatives. Suppose you are examining the smoothness of a function
\(f(x)\). The smoothness is assessed on a scale
\(C^0, C^1, C^2, \ldots, C^\infty\).

\begin{itemize}
\tightlist
\item
  \(C^0\): the function \(f()\) is continuous. Intuitively, this means
  that a graph of the function can be drawn without lifting the pencil
  from the paper.
\item
  \(C^1\): the function \(f()\) has a derivative over its entire domain
  \emph{and} that derivative \(\partial_x f(x)\) is continuous. (See
  \textbf{?@fig-c1-function} for an example.)
\item
  \(C^2\): the function \(\partial_x f(x)\) has a derivative over its
  entire domain \emph{and} that derivative is continuous. In other
  words, \(\partial_{xx} f(x)\) exists and is continuous.
\item
  \(C^n\): Like \(C^2\), but we are talking about the
  \(n\)\textsuperscript{th}-derivative of \(f(x)\) existing and being
  continuous.
\item
  \(C^\infty\): Usually when we denote a sequence with an infinite
  number of terms, we write down something like
  \(C^0, C^1, C^2, \ldots\). It would be entirely valid to do this in
  talking about the \(C^n\) sequence. But many of the mathematical
  functions we work with are \emph{infinitely differentiable}, that is
  \(C^\infty\).
\end{itemize}

\textbf{Examples of \(C^\infty\) functions}:

\begin{itemize}
\item
  \(\sin(x)\): the derivatives are \(\partial_x \sin(x) = \cos(x)\),
  \(\partial_{xx} \sin(x) = -\sin(x)\),
  \(\partial_{xxx} \sin(x) =-\cos(x)\),
  \(\partial_{xxxx} \sin(x) =\sin(x)\), \ldots{} You can keep going
  infinitely.
\item
  \(e^x\): the derivatives are \(\partial_x e^x = e^x\),
  \(\partial_{xx} e^x = e^x\), and so on.
\item
  \(x^2\): the derivatives are \(\partial_x x^2 = 2 x\),
  \(\partial_{xx} x^2 = 2\), \(\partial_{xxx} x^2 = 0\), \ldots{} Higher
  order derivatives are all simply 0. Boring, but still existing.
\end{itemize}

\textbf{Example of non-\(C^2\) functions:} We see these often when we
take two or more different \(C^\infty\) functions and split their
domain, using one function for one subdomain and the other(s) for other
subdomain(s).

\begin{itemize}
\tightlist
\item
  \(|x|\), the absolute value function. \(|x|\) is a pasting together of
  two \(C^\infty\) functions:
  \[|x| \equiv \left\{\begin{array}{rcl}+x & \text{for} & 0 \leq x\\-x&\text{for}& \text{otherwise}\end{array} \right.\ .\]
  The domain is split at \(x=0\).
\end{itemize}

For engineering and design problems, smoothness means something
substantially different than described by the mathematical concepts
above. Later in the course we will introduce \textbf{\emph{cubic
splines}} which are continuous functions defined by a finite set of
coordinate pairs, as in a data frame. Each line of the data frame
corresponds to a dot in a scatter plot, but in a cubic spline it is
called a ``knot point.'' The spline consists of cubic polynomials drawn
between consecutive knot points. The domain is split at each of the knot
points. Between any two adjacent knot points, the function is an
ordinary cubic polynomial. At a knot point, the cubics on either side
have been arranged to have their first and second derivatives match.
Thus, the first two derivatives are continuous. The function is at least
\(C^2\). The second derivative of a cubic is a straight-line function,
so the second derivative of a cubic spline is a series of straight-line
functions connected at the knot points. The second derivative does not
itself have a derivative at the knot points. So, a cubic spline cannot
satisfy the requirements for being \(C^3\); it is \(C^2\).

\hypertarget{exercises-5}{%
\section{Exercises}\label{exercises-5}}

\hypertarget{sec-prod-comp-rules}{%
\chapter{Derivatives of assembled functions}\label{sec-prod-comp-rules}}

In Section \textbf{?@sec-symbolic-differentiation} we used the rules
associated with evanescent \(h\), that is, \(\lim_{h\rightarrow 0}\), to
confirm our claims about the derivatives of many of the pattern-book
functions. We will call these rules \textbf{\emph{h-theory}} for short.
In this chapter, we are going to use h-theory to find algebraic
\textbf{\emph{rules}} to calculate the derivatives of linear
combinations of functions, products of functions, and composition of
functions. Remarkably, we can figure out these rules without having to
say specifically which functions are being combined. So the rules can be
written in terms of abstractions: \(f()\), \(g()\), and \(h()\). Later,
we will apply those rules to specific functions, to show how the rules
are used in practical work.

\hypertarget{sec-using-the-rules}{%
\section{Using the rules}\label{sec-using-the-rules}}

When you encounter a function that you want to differentiate, you first
have to examine the function to decide which rule you want to apply. In
the following, we will to use the names \(f()\) and \(g()\), but in
practice the functions will often be basic modeling functions, for
instance \(e^{kx}\) or \(\sin\left(\frac{2\pi}{P}t\right)\), etc.

\textbf{\emph{Step 1}}: Identify f() and g()

We will write the rules in terms of two function names, \(f()\) and
\(g()\), which can stand for any functions whatsoever. It is rare to see
the product or the composition written explicitly as \(f(x)g(x)\) of
\(f(g(x))\). Instead, you are given something like \(e^x \ln(x)\). The
first step in differentiating the product or composition is to identify
what are \(f()\) and \(g()\) individually.

In general, \(f()\) and \(g()\) might be complicated functions,
themselves involving linear combinations, products, and composition. But
to get started, we will practice with cases where they are simple,
pattern-book functions.

\textbf{\emph{Step 2}}: Find f'() and g'()

For differentiating either products or compositions, you will need to
identify both \(f()\) and \(g()\) (the first step) and then compute the
derivatives \(\partial_x f()\) and \(\partial_x g()\). That is, you will
write down four functions.

\textbf{\emph{Step 3}}: Apply the relevant rule

Recall from Chapter \textbf{?@sec-fun-assembling} that will will be
working with three important forms for creating new functions out of
existing functions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Linear combinations, e.g.~\(a f(x) + bg(x)\)
\item
  Products of functions, e.g.~\(f(x) g(x)\)
\item
  Compositions of functions, e.g.~\(f\left(g(x)\right)\)
\end{enumerate}

\hypertarget{differentiating-linear-combinations}{%
\section{Differentiating linear
combinations}\label{differentiating-linear-combinations}}

\textbf{\emph{Linear combination}} is one of the ways in which we make
new functions from existing functions. As you recall, linear combination
involves \textbf{\emph{scaling}} functions and then
\textbf{\emph{adding}} the scaled functions as in \(a f(x) + b g(x)\),
alinear combination of \(f(x)\) and \(g(x)\). We can easily use \(h\) to
show what is the result of differentiating a linear combination of
functions. First, let's figure out what is \(\partial_x\, a f(x)\),
Going back to writing \(\partial_x\) in terms of a slope function:
\[\partial_x\, a\,f(x) = \frac{a\, f(x + h) - a\,f(x)}{h}\\
\ \\
= a \frac{f(x+h) - f(x)}{h} = a\, \partial_x f(x)\] In other words, if
we know the derivative \(\partial_x\, f(x)\), we can easily find the
derivative of \(a\, f()\). Notice that even though \(h\) was used in the
derivation, it appears nowhere in the result
\(\partial_x\, b\,f(x) = b\, \partial_x\, f(x)\). The \(h\) is solvent
to get the paint on the wall and evaporates once its job is done.

Now consider the derivative of the sum of two functions, \(f(x)\) and
\(g(x)\): \begin{eqnarray}
\partial_x\, \left[f(x) + g(x)\right] & =\frac{\left[f(x + h) + g(x + h)\right] - \left[f(x) + g(x)\right]}{h} \\
\ \\
&= \frac{\left[f(x+h) -f(x)\right] + \left[g(x+h) - g(x)\right]}{h}\\
\ \\
&= \frac{\left[f(x+h) -f(x)\right]}{h} + \frac{\left[g(x+h) - g(x)\right]}{h}\\
\ \\
&= \partial_x\, f(x) + \partial_x\, g(x)
\end{eqnarray}

Because of the way that \(\partial_x\) can be ``passed through'' a
linear combination, mathematicians say that differentiation is a
\textbf{\emph{linear operator}}. Consider this new fact about
differentiation as a down payment on what will eventually become a
complete theory telling us how to differentiate a \textbf{\emph{product
of two functions}} or the \textbf{\emph{composition of two functions}}.
We will lay out the \(h\)-theory based algebra of this in the next two
sections.

We can summarize the h-theory result for linear combinations this way:

\begin{quote}
\emph{The derivative of a linear combination is the linear combination
of the derivatives.}
\end{quote}

That is:

\[\partial_x \left[\strut \color{magenta}{a} \color{brown}{f(x)} + \color{magenta}{b} \color{brown}{g(x)}\right] = \color{magenta}{a} {\large\color{brown}{f'(x)}} + \color{magenta}{b} {\large\color{brown}{g'(x)}}\]
as well as
\[\partial_x \left[\strut \color{magenta}{a}\, \color{brown}{f(x)} + \color{magenta}{b}\, \color{brown}{g(x)}  + \color{magenta}{c}\, \color{brown}{h(x)} + \cdots\right] = \color{magenta}{a}\, {\large\color{brown}{f'(x)}} + \color{magenta}{b}\, {\large\color{brown}{g'(x)}} + \color{magenta}{c}\, {\large\color{brown}{h'(x)}} + \cdots\]

\begin{example}
The derivative of a polynomial is a polynomial of a lower order.

Consider the polynomial
\[h(x) = \color{magenta}{a}\color{brown}{x^0}  + \color{magenta}{b} \color{brown}{x^1} + \color{magenta}{c} \color{brown}{x^2}\]
The derivative is
\[\partial_x h(x) = \color{brown}{0}\, \color{magenta}{a}  + \color{brown}{1}\, \color{magenta}{b}  + \color{magenta}{c}\, \color{brown}{2 x} = \color{magenta}{b} +  \color{magenta}{2 c}\  x\]

\end{example}

\hypertarget{product-rule-for-multiplied-functions}{%
\section{Product rule for multiplied
functions}\label{product-rule-for-multiplied-functions}}

The question at hand is how to compute the derivative
\(\partial_x f(x) g(x)\). Of course, you can always use numerical
differentiation. But let's look at the problem from the point of view of
symbolic differentiation. And since \(f(x)\) and \(g(x)\) are just
pronoun functions, we will assume you are starting out already knowing
the derivatives \(\partial_x f(x)\) and \(\partial_x g(x)\).

This situation arises particularly when \(f(x)\) and \(g(x)\) are
pattern-book functions for which you already have memorized
\(\partial_x f(x)\) and \(\partial_x g(x)\) or are basic modeling
functions whose derivatives you will memorize in Section
@ref(basic-derivs).

The purpose of this section is to \textbf{derive} the formula for
\(\partial_x f(x) g(x)\) in terms of \(f(x)\), \(g(x)\),
\(\partial_x f(x)\) and \(\partial_x g(x)\). This formula is called the
\textbf{\emph{product rule}}. The point of showing a derivation of the
product rule is to let you see how the logic of evanescent \(h\) plays a
role. In practice, everyone simply memorizes the rule, which has a
beautiful, symmetric form:

\[\text{Product rule:}\ \ \ \ \partial_x \left[\strut f(x)g(x)\right] = \left[\strut \partial_x f(x)\right]\, g(x) + f(x)\, \left[\strut\partial_x g(x)\right]\]
and is even prettier in Lagrange notation (where \(\partial_x f(x)\) is
written \(f'\)): \[ \left[\strut f g\right]' = f' g + g' f\]

As with all derivatives, the product rule is based on the instantaneous
rate of change
\[F'(x) \equiv \lim_{h\rightarrow 0} \frac{F(x+h) - F(x)}{h}\]
introduced in Section \textbf{?@sec-instantaneous-rate-of-change}.

We also need two other statements about \(h\) and functions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The derivative \(F'(x)\) is the slope of of \(F()\) at input \(x\).
  Taking a step of size \(h\) from \(x\) will induce a change of output
  of \(h F'(x)\), so \[F(x+h) = f(x) + h F'(x)\ .\]
\item
  Any result of the form \(h F(x)\), where \(F(x)\) is finite, gives 0.
  More precisely, \(\lim_{h\rightarrow 0} h F(x) = 0\)
\end{enumerate}

As before, we will put the standard \(\lim_{h\rightarrow 0}\) disclaimer
against dividing by \(h\) until there are no such divisions at all, at
which point we can safely use the equality \(h = 0\).

Suppose the function \(F(x) \equiv f(x) g(x)\), a product of the two
functions \(f(x)\) and \(g(x)\).

\[F'(x) = \partial_x \left[\strut f(x) g(x) \right] \equiv \lim_{h\rightarrow 0}\frac{f(x+h) g(x+h) - f(x) g(x)}{h}\]
We will replace \(g(x_h)\) with its equivalent \(g(x) + h g'(x)\) giving

\[= \lim_{h\rightarrow 0} \frac{f(x+h) \left[\strut g(x) + h g'(x) \right] - f(x) g(x)}{h} \]
\(g(x)\) appears in both terms in the numerator, once multiplied by
\(f(x+h)\) and once by \(f(x)\). Collecting those terms give:

\[=\lim_{h\rightarrow 0}\frac{\left[\strut f(x+ h) - f(x)\right]  g(x) + \left[\strut f(x+h) h\, g'(x)\right]}{h}\]
This has two bracketed terms added together over a common denominator.
Let's split them into separate terms:

\[=\lim_{h\rightarrow 0}\underbrace{\left[\strut \frac{f(x+h) - f(x)}{h}\right]}_{f'(x)} g(x) + \lim_{h\rightarrow 0}\frac{\left[\strut f(x) + h f'(x)\right]h\,g'(x)}{h}\]

The first term is \(g(x)\) multiplied by the familiar form for the
derivative of \(f(x)\)
\[= f'(x) g(x) + \lim_{h\rightarrow 0}\frac{f(x) h g'(x)}{h} + \lim_{h\rightarrow 0}\frac{h f'(x) h g'(x)}{h}\]
In each of the last two terms there is an \(h/h\) involved. This is
safely set to 1, since the \(\lim_{h\rightarrow 0}\) implies that \(h\)
will not be exactly zero. There remain no divisions by \(h\) so we can
drop the \(\lim_{h\rightarrow 0}\) in favor of \(h=0\):
\[= f'(x) g(x) + f(x) g'(x) + \cancel{h f'(x) g'(x)}\]

\[=f'(x) g(x) + g'(x) f(x)\]

The last step relies on statement (2) above.

Some people find it easier to read the rule in Lagrange shorthand, where
\(f\) and \(g\) stand for \(f(x)\) and \(g(x)\) respectivly, and \(f'\)
(``f-prime'') and \(g'\) (``g-prime'') stand for \(\partial f()\) and
\(\partial g()\).

\[\large\text{Lagrange shorthand:}\ \   \partial[\color{magenta}f \times \color{brown}g] = [\color{magenta}f \times \color{brown}g]' = \color{magenta}{f'}\color{brown}g + \color{brown}{g'}\color{magenta}f\]

\begin{example}
The expression \(\partial_x x^3\) is the same as
\(\partial_x \left[\strut x\  x^2\right]\). Since we already know
\(\partial_x x\) (it is 1) and \(\partial_x x^2\) (it is \(2x\)) let's
apply the product rule to find \(\partial_x x^3\):
\[\large\partial [\color{magenta}x \times \color{brown}{x^2}] = \color{magenta}{[\partial x]} \times \color{brown}{x^2} \ +\  \color{brown}{[\partial x^2]} \times \color{magenta}x =\color{magenta}1\times \color{brown}{x^2} + \color{brown}{2x} \times \color{magenta}x = 3 x^2\]

\end{example}

\begin{takenote}
Occasionally, mathematics gives us a situation where being more general
produces simplicity.

In the case of function products, the generalization is from products of
two functions \(f(x)\cdot g(x)\) to products of more than two functions,
e.g.~\(u(x) \cdot v(x) \cdot w(x)\).

The chain rule here takes a form that makes the overall structure much
clearer:

\begin{eqnarray}
\partial_x \left[\strut u(x) \cdot v(x) \cdot w(x)\right] = \ \ \ \ \ \ \ \ \ \ \ \ \\
\color{blue}{\partial_x u(x)} \cdot v(x) \cdot w(x)\ + \\ 
u(x) \cdot \color{blue}{\partial_x v(x)} \cdot w(x)\ + \\
u(x) \cdot v(x) \cdot \color{blue}{\partial_x w(x)}\ \  \ \ 
\end{eqnarray}\textbackslash end\{eqnarray\}

In the Lagrange shorthand, the pattern is even more evident:
\[\left[ u\cdot v\cdot w\right]' = \color{blue}{u'}\cdot v\cdot w\ +\ u\cdot \color{blue}{v'}\cdot w\ +\ u\cdot v\cdot \color{blue}{w}'\]

\end{takenote}

\hypertarget{chain-rule-for-function-composition}{%
\section{Chain rule for function
composition}\label{chain-rule-for-function-composition}}

A function composition, as described in Section
\textbf{?@sec-function-composition}, involves inserting the output of
one function (the ``interior function'') as the input of the other
function (the ``exterior function''). As we so often do, we will be
using pronouns a lot. A list might help keep things straight:

\begin{itemize}
\tightlist
\item
  There are two functions involved in a composition. We will call them
  \(f(y)\) and \(g(x)\). In the composition \(f(g(x))\), the
  \textbf{\emph{exterior function}} is \(f()\) and the
  \textbf{\emph{interior function}} is \(g()\).
\item
  Each of the two functions \(f()\) and \(g()\) has an input. In our
  examples, we will use \(y\) to stand for the input to the exterior
  function and \(x\) as the pronoun for the input to the interior
  function.
\item
  As with all rules for differentiation, we will need to compute the
  derivatives of the functions involved, each with respect to its own
  input. So these will be \(\partial_y f(y)\) and \(\partial_x g(x)\).
\end{itemize}

A reason to use different pronouns for the inputs to \(f()\) and \(g()\)
is to remind us that the output \(g(x)\) is in general not the same kind
of quantity as the input \(x\). In a function composition, the \(f()\)
function will take the output \(g(x)\) as input. But since \(g(x)\) is
not necessarily the same kind of thing as \(x\), why would we want to
use the same name for the input to \(f()\) as we use for the input to
\(g()\).

With this distinction between the names of the inputs, we can be even
more explicit about the composition, writing \(f(y=g(x))\) instead of
\(f(g(x))\). Had we used the pronound \(x\) for the input to \(f()\) but
our explicit statement, although technically correct, would be
confusing: \(f(x = g(x))\)!

With all these pronouns in mind, here is the \textbf{\emph{chain rule}}
for the derivative \(\partial_x f(g(x))\):

\[\large\partial_x \left[\strut \color{magenta}{f\left(\strut\right.}\strut \color{brown}{g(x)}\color{magenta}{\left.\right)}\right] = [\color{magenta}{\partial_y f}](\color{brown}{g(x)}) \times [\color{brown}{\partial_xg(x)}]\]
Or, using the Lagrange prime notation, where \('\) stands for the
derivative of a function with respect to its input, we have
\[\large\text{Lagrange shorthand:}\ \   [\color{magenta}f(\color{brown}g)]' = \color{magenta}{f'} (\color{brown}g) \times \color{brown}{g}'\]

\begin{intheworld}
In news and policy discussions, you will often hear about ``inflation
rate'' or ``birth rate'' or ``interest rate'' or ``investment rate of
return.'' In each case, there is a function of time combined with a
derivative of that function: with the general form
\[\frac{\partial_t f(t)}{ f(t)}\ .\]

\begin{itemize}
\tightlist
\item
  Inflation rate: The function is cost\_of\_living(\(t\)). The
  derivative is the rate of change with respect to time in the cost of
  living: \(\partial_t\,\)cost\_of\_living(\(t\)).
\item
  Birth rate: The function is population(\(t\)). The derivative is
  \(\partial_t\,\)population(\(t\)), or at least that component of the
  overall \(\partial_t\,\)population(\(t\)) that is related to births.
  (Other components are deaths and the balance of in-migration and
  out-migration.)
\item
  Interest rate: The function is account\_balance(\(t\)) and the
  derivative is \(\partial_t\,\)account\_balance(\(t\)).
\item
  Investment returns: The function is net\_worth(\(t\)) and the
  derivative is \(\partial_t\,\)net\_worth(\(t\)).
\end{itemize}

In all these cases, The ``rate'' is not merely ``per time'' as would be
the case for \(\partial_t f(t)\). Instead the rate is ``per unit of the
whole per time.'' Thus the birth rate is ``births \textbf{\emph{per
capita}} per year.''\sidenote{\footnotesize ``Per capita'' is Latin. Literally, it
  translates to ``by head.'' but its modern sense is ``per unit of
  population.'' The ``unit of population'' is, of course, a person.}
Interest and return rates are ``percent per year'' where the ``percent''
understood to be the ``change-in-value divided by the current value.''

Thanks to the chain rule, there is a shortcut way of writing these sorts
of ``rates per time.'' Exactly equivalent to the ratio
\(\frac{\partial_t f(t)}{ f(t)}\) is \[\partial_t \ln(f(t))\ .\]

Such changes in logarithms are encountered in fields such as economics
or finance, where it is common to consider the logarithm of the economic
quantity to render changes as percent of the whole.

It is also something to keep in mind when interpreting graphs of an
amount versus time, as in Figure~\ref{fig-italy-us-covid}.
\href{https://www.nytimes.com/2020/03/20/health/coronavirus-data-logarithm-chart.html}{Source}

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/www/NYT-italy-us-corona.png}

}

\caption{\label{fig-italy-us-covid}Growth in the number of Coronavirus
cases in Italy and the US early in the pandemic.}

\end{figure}

Look closely at the two graphs in Figure~\ref{fig-italy-us-covid}. They
show the same data about growing numbers of coronavirus cases, the left
graph on linear axes, the right on the now-familiar semi-log axes.

Most people are excellent at comparing slopes, even if they find it
difficult or tedious to quantify a slope with a number and units. For
instance, a glance suffices to show that in the left graph, well through
mid-March the red curve (Italy) is steeper on any given date than the
blue curve (US). This means that the number of people with coronavirus
was growing faster (per day) in Italy.

The right graph tells a different story: up until about March 1, the
Italian cases were increasing faster than the US cases. Afterwards, the
US sees a larger growth rate than Italy until, around March 19, the US
growth rate is substantially larger than the Italy growth rate.

The previous two paragraphs and their corresponding graphs may seem to
contradict one another. But they are both accurate, truthful depictions
of the same events. What's different between the two graphs is that the
left shows one kind of rate and the right shows another kind of rate. In
the left, the slope is new-cases-per-day, the output of the derivative
function On the right, the slope is the \textbf{\emph{proportional
increase}} in cases per day, that is, From the chain rule, we know that
\[\partial_t \left[\strut\ln(f(t))\right] = \frac{\partial_t f(t)}{f(t)}\]
Since the right graph is on semi-log axes, the slope we perceive
visually is \(\partial_t \left[\strut\ln(f(t))\right]\). That is an
obscure-looking bunch of notation until the chain rule reveals it to be
the rate of change at time \(t\) divided by the value at time \(t\).

\end{intheworld}

The derivation of the chain rule relies on two closely related
statements which are expressions of the idea that near any value \(x\) a
function can be expressed as a linear approximation with the slope equal
to the derivative of the function :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(g(x + h) = g(x) + h g'(x)\)
\item
  \(f(y + \epsilon) = f(y) + \epsilon f'(y)\), which is the same thing
  as (1) but uses \(y\) as the argument name and \(\epsilon\) to stand
  for the small quantity we usually write with an \(h\).
\end{enumerate}

We will now look at \(\partial_x f\left({\large\strut} g(x)\right)\) by
writing down the fundamental definition of the derivative. This, of
course, involves the disclaimer \(\lim_{h\rightarrow 0}\) until we are
sure that there is no division by \(h\) involved.

\[\partial_x \left[{\large\strut} f\left(\strut g(x)\right)\right]  \equiv \lim_{h\rightarrow 0}\frac{\color{magenta}{f(g(x+h))} - f(g(x))}{h}\]

Let's examine closely the expression
\(\color{magenta}{f\left(\strut g(x+h)\right)}\). Applying rule (1)
above turns it into
\[\lim_{h\rightarrow 0} f\left(\strut g(x) + \color{blue}{h g'(x)}\right)\]
Now apply rule (2) but substituting in \(g(x)\) for \(y\) and
\(\color{blue}{h g'(x)}\) for \(\epsilon\), giving\\
\[\lim_{h\rightarrow 0} \color{magenta}{f\left(\strut g(x+h)\right)} = \lim_{h\rightarrow 0} \color{brown}{\left[{\large\strut} f\left(g(x)\right) + \color{blue}{h g'(x)}f'\left(g(x)\right)\right]}\]
We will substitute the \(\color{blue}{blue}\) and
\(\color{brown}{brown}\) expression for the \(\color{magenta}{magenta}\)
expression in
\[\partial_x f\left(\strut g(x)\right)  \equiv \lim_{h\rightarrow 0}\frac{\color{magenta}{f(g(x+h))} - f(g(x))}{h}\]
giving
\[\partial_x f\left(\strut g(x)\right)  \equiv \lim_{h\rightarrow 0}\frac{\color{brown}{f\left(g(x)\right) + \color{blue}{h g'(x)}f'\left(g(x)\right)} - f\left(g(x)\right)}{h}\]
In the denominator, \(f\left(g(x)\right)\) appears twice and cancels
itself out. That leaves a single term with an \(h\) in the numerator and
an \(h\) in the denominator. Those \(h\)'s cancel out, at the same time
obviating the need for \(\lim_{h\rightarrow 0}\) and leaving us with the
chain rule:
\[\partial_x f\left(\strut g(x)\right)  \equiv \lim_{h\rightarrow 0}\frac{\color{brown}{ \color{blue}{h g'(x)} f'\left(g(x)\right)}}{h} = f'\left(g(x)\right)\ g'(x)\]

\begin{example}
Use the chain rule to find the derivative \(\partial_x e^{2x}\).

Recognize that \(g(x) \equiv 2x\) is the interior function in \(e^{2x}\)
and \(f(x) \equiv \exp(x)\) is the exterior function. Thus
\[\partial_x e^{2x} = f'(g(x)) g'(x) = \exp(g(x)) 2 = 2 e^{2x}\ .\]
Happily, this is the same result as we got from using the product rule
to find \(\partial_x e^{2x}\).

Recognizing \(e^{2x}\) as \(e^x \times e^x\), we can apply the
\textbf{\emph{product rule}}.

\end{example}

\begin{example}
The chain rule can be used in a clever way to find a formula for
\(\partial_x \ln(x)\).

We've already seen that the logarithm is the inverse function to the
exponential, and \emph{vice versa}. That is:
\[e^{\ln(y)} = y \ \ \ \text{and}\ \ \ \ln(e^y) = x\] Since \(\ln(e^y)\)
is the same function as \(y\), the derivative
\(\partial_y \ln(e^y) = \partial_y y = 1\).

Let's differentiate the second form using the chain rule:
\[\partial_y \ln(e^y) = \left[\partial_y \ln\right](e^y)\, e^x = 1\]
giving
\[\left[\partial_y \ln\right](e^y) = \frac{1}{e^y} = \recip(e^y)\]
Whatever the function \(\partial_x \ln()\) might be, it takes its input
and produces as output the reciprocal of that input. In other words:
\[\partial_x \ln(x) = \frac{1}{x}\ .\]

\end{example}

\begin{example}
Knowing that \(\partial_x \ln(x) = 1/x\) and the chain rule, we are in a
position to demonstrate the \textbf{\emph{power-law rule}}
\(\partial_x x^p = p\, x^{p-1}\). The key is to use the identity
\(e^{\ln(x)} = x\).

\[\partial_x x^p = \partial_x \left[e^{\ln(x)}\right]^p\] The rules of
exponents allow us to recognize
\[\left[e^{\ln(x)}\right]^p = e^{p \ln(x)}\] Thus, \(x^p\) can be seen
as a composition of the exponential function onto the logarithm
function.

Applying the chain rule to this composition gives
\[\partial_x e^{p \ln(x)} = e^{p\ln(x)}\partial_x [p \ln(x)] =
e^{p\ln(x)} \frac{p}{x}\ .\] Of course, we already know that
\(e^{p \ln(x)} = x^p\), so we have
\[\partial_x x^p = x^p \frac{p}{x} = p x^{p-1}\ .\]

\end{example}

\begin{itemize}
\tightlist
\item
  \(\large\partial_x [\color{brown}\sin(\color{magenta}{a x + b})] = [\partial_x \color{brown}{\sin}](\color{magenta}{a x + b}) \times \partial_x [\color{magenta}{ax + b}] = \color{brown}{\cos}(\color{magenta}{ax + b}) \times \color{magenta}a\).
\end{itemize}

\begin{intheworld}
In 1734, famous philosopher
\href{https://en.wikipedia.org/wiki/George_Berkeley}{George Berkeley}
(1685-1753) published a long-titled book: \emph{The Analyst: A Discourse
Addressed to an Infidel Mathematician: Wherein It Is Examined Whether
the Object, Principles, and Inferences of the Modern Analysis Are More
Distinctly Conceived, or More Evidently Deduced, Than Religious
Mysteries and Points of Faith}. In \emph{The Analyst}, Berkeley took
issue with the arguments of that time that it is legitimate to divide by
\(h\) when, ultimately, \(h\) will be replaced by zero. Calling \(h\) an
``evanescent increment,'' he asked,

\begin{quote}
\emph{``And what are these same evanescent Increments? They are neither
finite Quantities nor Quantities infinitely small, nor yet nothing. May
we not call them the ghosts of departed quantities?''}
\end{quote}

Interesting, Berkeley believed that the ghost of \(h\) yielded correct
results. His objection was that the framers of calculus had made two,
canceling errors.

\begin{quote}
\emph{``{[}B{]}y virtue of a two fold mistake you arrive, though not at
science, yet truth.''}
\end{quote}

Berkeley was saying that calculus had not yet been put on a solid
logical foundation. It wasn't until more than a century after Berkeley's
death that this work was accomplished. Once accomplished, the results
that had been claimed true all along were confirmed.

\end{intheworld}

\hypertarget{sec-basic-derivs}{%
\section{Derivatives of the basic modeling
functions}\label{sec-basic-derivs}}

The basic modeling functions are the same as the pattern-book functions,
but with bare \(x\) replaced by \(\line(x)\). In other words, each of
the basic modeling functions is a \textbf{\emph{composition}} of the
corresponding pattern-book function with \(\line(x)\). As such, the
derivatives of the basic modeling functions can be found using the chain
rule.

Suppose \(f()\) is one of our pattern-book functions. Then
\[\large\partial_x f(\color{magenta}{ax + b}) = \color{brown}{a} f'(\color{magenta}{ax + b})\]
where \(\color{brown}{a}\) is the derivative with respect to \(x\) of
\(\color{magenta}{ax + b}\).

Here are the steps for differentiating a basic modeling function
\(\color{brown}{f}(\color{magenta}{a x + b})\) where \(f()\) is one of
the pattern-book functions:

\begin{itemize}
\tightlist
\item
  Step 1: Identify the particular pattern-book function
  \(\color{brown}{f}()\) and write down its derivative
  \(\color{brown}{f'}\). For example, if \(f()\) is \(\sin()\), then
  \(f'()\) is \(\cos()\).
\item
  Step 2: Find the derivative of the linear interior function. If the
  function is \(\color{magenta}{ax + b}\), then the derivative is
  \(\color{magenta}{a}\). If the interior function is
  \(\frac{2\pi}{P}(t-t_0)\), the derivative is \(\frac{2 \pi}{P}\).
\item
  Step 3: Write down the original function
  \(\large\color{brown}{f}(\color{magenta}{a x + b})\) but replace
  \(\large\color{brown}{f}\) with \(\large \color{brown}{f'}\) and
  pre-multiply by the derivative of the interior function. For instance,
  \[\partial_x f(\color{magenta}{ax + b}) = {\large \color{magenta}{a}}{\large f'}(\color{magenta}{ax + b})\]
  Another example:
  \[\partial_t \color{brown}{\sin}\left(\color{magenta}{\frac{2 \pi}{P}(t-t_0)} \right) = {\large \color{magenta}{\frac{2 \pi}{P}}}\color{brown}{\large\cos}\left(\color{magenta}{\frac{2 \pi}{P}(t-t_0) }\right) \]
\end{itemize}

By convention, there are different ways of writing \(\line(x)\) for the
different pattern-book functions, for instance:

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\centering\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\centering
Pattern-book function \(\longrightarrow\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Basic modeling
\end{minipage} \\
\midrule
\endhead
\(\sin(x)\ \ \ \longrightarrow\) &
\(\sin\left(\strut2 \pi \left[x-x_0\right]/P\right)\) \\
\(\exp(x)\ \ \ \longrightarrow\) & \(\exp(k x)\) \\
\(x^2 \ \ \ \longrightarrow\) & \(\left[mx + b\right]^2\) \\
\(1/x \ \ \ \longrightarrow\) & \(1/\left[mx + b\right]\) \\
\(\ln(x) \ \ \ \longrightarrow\) & \(\ln(a x + b)\) \\
\bottomrule
\end{longtable}

The rule for the derivative of any basic modeling function
\(f(\line(x))\) is
\[\partial_x f(\line(x)) = \partial_x \line(x) \times \partial_x f\left(\strut\line(x)\right)\]

To illustrate:

\begin{itemize}
\tightlist
\item
  \(\partial_x e^{\color{magenta}{kx}} = {\large\color{magenta}{k}}\, e^{\color{magenta}{kx}}\)
  where \(\line(x) = kx\).
\item
  \(\partial_x \sin(2\pi (x-x_0)/P) = \frac{2\pi}{P} \sin(2\pi (x-x_0)/P)\)
  where \(\line(x) = 2\pi (x-x_0)/P)\).
\item
  \(\partial_x (mx + b)^2 = m\, 2 (m x + b) = 2 m^2 x + m^2 b\) where
  \(\line(x) = mx + b\).
\item
  \(\partial_x \text{reciprocal}(mx + b) = \partial_x \frac{1}{mx + b} = - \frac{m}{(mx + b)^2}\)
  where \(\line(x) = mx + b\) and we use the fact that
  \(\partial_x \text{reciprocal}(x) = - 1/x^2\)
\item
  \(\partial_x \ln(a x + b) = a/(ax+b)\)
\item
  \(\partial_x \pnorm(x, \text{mean}, \text{sd}) = dnorm(x, \text{mean}, \text{sd})\).
\item
  \(\partial_x \dnorm(x, \text{mean}, \text{sd}) = - \frac{x-m}{\text{sd}^2} \dnorm(x, \text{mean}, \text{sd})\)
\end{itemize}

You will be using the derivatives of the basic modeling functions so
often, that you should practice and practice until you can write the
derivative at a glance.

\begin{takenote}
There are many possible implementations of the general concept of hump
functions and sigmoidal functions. The one we use in this book is
\(\dnorm()\) for the hump and \(\pnorm()\) for the sigmoid.

The names \(\dnorm\) and \(\pnorm\) are worth remarking on. As we've
said before, \(\dnorm()\) is called the \textbf{\emph{gaussian
function}} in many fields of science and engineering. It is also a
centrally important function in statistics, where it is usually called
the \textbf{\emph{normal function}}. (that is how important it is: it is
just ``normal.'') You may also have heard the normal function described
as a ``bell-shaped curve.''

In statistical nomenclature, \(\dnorm()\) is called the ``normal
probability density function (PDF)'' and \(\pnorm()\) is called the
``normal cumulative density function (CDF).'' that is way too wordy for
our purposes. So, for brevity, we have adopted the R name for those
functions: \texttt{dnorm()} and \texttt{pnorm()}.

Owing to the origin of the names \(\dnorm\) and \(\pnorm\), we are
writing the parameters of the functions---mean and sd---using the
computer language notation. The pattern-book functions are just
\(\dnorm(x)\) and \(\pnorm(x)\), without listing the parameters. But the
basic modeling functions, with parameters, are written
\(\dnorm(x, \text{mean}, \text{sd})\) and
\(\dnorm(x, \text{mean}, \text{sd})\). This violates the convention that
the basic modeling functions are the composition of the pattern-book
functions with \(\line(x)\). But \(\dnorm()\) does not work this way
because, by convention, the amplitude of the peak of \(\dnorm()\)
changes with the input parameter sd. That is not true for any other
basic modeling function.

\end{takenote}

Composition or product?

There is one family of functions for which function composition is the
same thing as multiplying functions: the power-law family.

Consider, for instance, the function \(h(x) \equiv \left[3x\right]^4\).
Let's let \(g(x) \equiv 3x\) and \(f(y) \equiv y^4\). With these
definitions, \(h(x) = f(g(x))\).

Recognizing that \(\partial_y f(y) = 4 y^3\) and
\(\partial_x g(x) = 3\), the \emph{chain rule} gives
\[\partial_x h(x) = 
\underbrace{4 g(x)^3}_{f'(g(x))} \times \underbrace{3}_{g'(x)} = \underbrace{4 (3 x)^3}_{f'(g(x))} \times 3 = 4\cdot 3^4 \times x^3 = 324\ x^3\]
Another way to look at the same function is \(g(x)\) multiplied by
itself 3 times: \[h(x) = g(x)\cdot g(x) \cdot g(x) \cdot g(x)\] This is
a product of 4 terms. Applying the product rule gives \begin{eqnarray}
\partial_x h(x) &=& \color{blue}{g'(x)}\cdot g(x)\cdot g(x) \cdot g(x) +\\
&\ & g(x)\cdot \color{blue}{g(x)}'\cdot g(x) \cdot g(x) +\\
&\ & g(x)\cdot g(x)\cdot \color{blue}{g(x)'} \cdot g(x) +\\
&\ & g(x)\cdot g(x)\cdot g(x) \cdot \color{blue}{g'(x)}
\end{eqnarray} Since multiplication is commutative, all of these four
terms are the same, each being \(3^4 x^3\). The sum of all four is
therefore \(4 \times 3^4 x^3 = 324 x^3\).

These are two long-winded ways of getting to the result. For most
people, differentiating power-law functions algebraically is simplified
by using the \textbf{\emph{rules of exponentiation}} rather than the
product or chain rule. Here,
\[h(x) \equiv \left[3x\right]^4 = 3^4 x^4\]so \(\partial_x h(x)\) is
easily handled as a scalar (\(3^4\)) times a function \(x^4\).
Consequently, applying the rule for differentiating power laws,
\[\partial_x h(x) = 3^4 \times \partial_x x^4 = 3^4 \times 4 x^3 = 324 x^3\]
As another example, take \(h(x) \equiv \sqrt[4]{\strut x^3}\). This is,
of course, the composition \(f(g(x))\) where \(f(y) \equiv y^{1/4}\) and
\(g(x) \equiv x^3\). Applying the chain rule to find \(\partial_x h(x)\)
will work (of course!), but is more work than applying the rules of
exponentiation followed by a simple power-law differentiation.
\[h(x) = \sqrt[4]{\strut x^3} = x^{3/4}\ \ \text{so}\ \  \partial_x h(x) = \frac{3}{4} x^{(3/4 - 1)} = \frac{3}{4} x^{-1/4}\]

\hypertarget{exponentials-and-logarithms-optional}{%
\section{Exponentials and logarithms
(optional)}\label{exponentials-and-logarithms-optional}}

The \textbf{\emph{natural logarithm}} function, \(\ln(x)\), is one of
our basic modeling functions. As you know, there are other logarithmic
functions. The one most often used is the logarithm-base-10, written
\(\log_{10}(x)\) or \texttt{log10(x)}. Ten is an integer, and a nice
number to use in arithmetic. So in practice, it is sensible to use
\(\log_{10}()\). (Indeed, \(\log_{10}()\) is the digit() function,
introduced in Chapter \textbf{?@sec-magnitudes}).

The ``natural'' in the ``natural logarithm'' means something different.

The base of the natural logarithm is the number called
\textbf{\emph{Euler's constant}} and written \(e\). As a celebrity
number, \(e\) is right up there with \(\pi\) and \(i\). Just as \(\pi\)
has a decimal expansion that is infinitely long, the familiar
\(\pi = 3.14159265358979...\), Euler's constant has an infinitely long
decimal representation: \(e = 2.71828182845905...\)

It is not obvious at first glance why \(e = 2.71828182845905...\) should
be called ``natural'' by mathematicians. The reason is not the number
itself, but

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\ln(x)\) is the inverse of \(e^x\), which is special for being
  invariant under differentiation: \(\partial_x e^x = e^x\).
\item
  The derivative \(\partial_x \ln(x)\) which has a particularly simple
  form, namely, \(1/x\).
\end{enumerate}

Let's look at the log-base-10 and it is computer-savvy cousin
log-base-2. The very definition of logarithms means that both 10 and 2
can be written \[10 = e^{\ln(10)}\ \ \ \text{and}\ \ \ 2 = e^{\ln(2)}\]
This implies that the base-10 and base-2 exponential functions can be
written

\[10^x = \left[\strut e^{\strut\ln(10)}\right]^x = e^{\ln(10)x} \ \ \ \text{and}\ \ \ 2^x = \left[\strut e^{\strut\ln(2)}\right]^x = e^{\ln(2) x}\]
Calculating \(\partial_x 10^x\) or \(\partial_x 2^x\) is a matter of
applying the chain rule:

\[\partial_x [10^x] = \partial_x [e^{\ln(10)x}] = e^{\ln(10)x} \times \ln(10) \ =\  10^x \times 2.3026\]
and
\[\partial_x [2^x] = \partial_x [e^{\ln(2)x}] = e^{\ln(2)x} \times \ln(2) \ = \ 2^x \times 0.6931\]
Like \(e^x\), the derivatives of \(10^x\) and \(2^x\) are proportional
to themselves. For \(e^x\) the constant of proportionality is 1, a very
natural number indeed.

\hypertarget{exercises-6}{%
\section{Exercises}\label{exercises-6}}

`\{=html\}

\hypertarget{sec-optim-and-shape}{%
\chapter{Optimization}\label{sec-optim-and-shape}}

To ``optimize'' means to make something as good as possible with the
available resources. Optimization problems are common in science,
logistics, industry, and any other area where one seeks the best
solution to a problem. Some everyday examples:

\begin{itemize}
\tightlist
\item
  When to harvest trees being grown for lumber. Harvest too soon and you
  might be losing out on the prime growing years. Wait too long and
  trees will have settled in to slow growth, if any.
\item
  Walking up too steep a slope is tiring and slows you down; that is why
  hiking trails have switchbacks. When the switchbacks are too shallow,
  it takes a long time to cover the distance. What's the most efficient
  angle to enable hikers to get up the hill in the shortest time.
\item
  How much salt to add to a stew. Stews can be too salty, or they can be
  not salty enough. Somewhere in the middle is the optimum.
\end{itemize}

\hypertarget{structure-of-the-problem}{%
\section{Structure of the problem}\label{structure-of-the-problem}}

In an optimization problem, there is one or more input quantities whose
value you have to choose. The amount of salt; the years to wait from
planting to harvesting a tree; the angle of the trail with respect to
the slope. We will call this the \textbf{\emph{decision quantity}}.

Similarly, there is one or more output quantity that you value and want
to make as good as possible. The taste of the stew; the amount of usable
wood harvested; the time it takes to walk up the hill. The output
quantities are called the \textbf{\emph{objectives}}.

In this chapter, we will deal with optimization problems that involve
only a single objective. Problems with multiple objectives are among the
most interesting and important in real-world decision making.
Single-objective optimization techniques are a component of the more
complex decision making, but they are a good place to get started.

The model that relates inputs to the objective output is called the
\textbf{\emph{objective function}}. Solving an optimization
problem---once the modeling phase is complete---amounts to finding a
value for the decision quantity (the input to the objective function)
that produces the best output from the objective function.

Sometimes the objective is something that you want to
\textbf{\emph{minimize}}, make as small as possible. In the hiking trail
problem, we seek to minimize the amount of time it takes to walk up the
trail. Sometimes you want to \textbf{\emph{maximize}} the objective, as
in the wood-harvest problem where the objective is to harvest the most
wood per year.

\marginnote{\begin{footnotesize}

Mathematically, maximization and minimization are the same thing. Every
minimization problem can be turned into a maximization problem by
putting a negative sign in front of the objective function. To simplify
the discussion, in talking about finding the solution to an optimization
problem we will imagine that the goal is to maximize. But keep in mind
that many circumstances in the real world, ``best'' can mean
minimization.

\end{footnotesize}}

Recall from Section \textbf{?@sec-local-extremes} that there are two
components to the task of maximization or minimization. The
\textbf{\emph{argmax}} is the \textbf{input} to the objective function
which produces the largest output. The \textbf{\emph{maximum}} is the
value of that output.\sidenote{\footnotesize Another word for an ``input'' is
  ``argument.'' Argmax is the contraction of \textbf{arg}ument producing
  the \textbf{max}imum output.} \textbf{\emph{Argmin}} and
\textbf{\emph{minimum}} are the words used in a situation where you seek
the smallest value of the objective function.

Once you have found the argmax you can plug that value into the
objective function to find the value of the output. That value is the
\textbf{\emph{maximum}}.

\begin{takenote}

People often talk about ``finding the maximum.'' This is misleading. The
setup for an optimization problem is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Construct (that is, model) the objective function.
\item
  Now that you know the objective function, find the input to that
  function---that is, the \textbf{argmax}---that produces the maximum
  output.
\end{enumerate}

\end{takenote}

To illustrate the setup of an optimization problem, imagine yourself in
the situation of a contest to see who can shoot a tennis ball the
farthest into a field with a slingshot. During the contest, you will
adjust the vertical angle of launch, place the ball into the slingshot's
cradle, pull back as far as possible, and let go. To win the contest,
you need to optimize how you launch the ball.

The objective is to maximize the distance traveled by the ball. The
\textbf{\emph{objective function}} models the distance travelled as a
function of the quantities you can control, for instance the vertical
angle of launch or the amount by which you pull back the slingshot. For
simplicity, we will imagine that the slingshot is pulled back by a
standard amount, producing a velocity of the ball at release of \(v_0\).
Since \(v_0\) is fixed, you will win or lose based on the angle of
launch you choose.

Before you head out into the field to experiment, let's do a bit of
preparation for constructing the objective function. Using some
principles of physics and mathematics (which you may not yet
understand), we will model how far the ball will travel (horizontally)
as a function of the angle of launch \(\theta\) and the initial velocity
\(v_0\).

The mathematics of such problems involves an area called
\textbf{\emph{differential equations}}, an important part of calculus
which we will come to later in the course. Since you don't have the
tools yet, we will just state a simple model of how long the ball stays
in the air. \[\text{duration}(v_0, \theta) = 2 v_0 \sin(\theta)/g\]
\(g\) is the acceleration due to gravity, which is about
\(9.8 \text{m}\text{s}^{-2}\), assuming that the contest is being held
on Earth.

The horizontal distance travelled by the tennis ball will be
\[\text{hdist}(v_0, \theta) = \cos(\theta) v_0\, \text{duration}(v_0, \theta) = 2 v_0^2 \cos(\theta)\sin(\theta) / g\]
Our objective function is hdist(), and we seek to find the argmax. The
input \(v_0\) is (we have assumed) fixed, so the only decision quantity
is the angle \(\theta\).

The best choice of \(\theta\) will make the quantity
\(\cos(\theta)\sin(\theta)\) as large as possible. So in finding the
argmax, we don't need to be concerned with \(v_0\) or \(g\).

Finding the argmax can be accomplished simply by plotting the function
\(\cos(\theta)\sin(\theta)\). We will implement the function so that the
input is in units of \emph{degrees}.

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/23-optim_files/figure-pdf/fig-ball-theta-1.pdf}

}

\caption{\label{fig-ball-theta}In the simple model of a tennis ball
launched at an angle \(\theta\) from the horizontal, the distance
travelled is \(2 v_0^2 / g\) times \(\cos(\theta)\sin(\theta)\).}

\end{marginfigure}

You can see that the maximum value is about 0.5 and that this occurs at
an argmax \(\theta\) that is a little bit less than 50\(^\circ\).

Zooming in on the \(\theta\) axis let's you find the argmax with more
precision:

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/23-optim_files/figure-pdf/fig-ball-theta-zoom-1.pdf}

}

\caption{\label{fig-ball-theta-zoom}Zooming in on the argmax of the
objective function. It is important to look at the scale of the vertical
axis. Any value of \(\theta\) between about 40 and 50 gives a very close
approximation to the maximum.}

\end{marginfigure}

From the graph, especially the zoomed-in version, you can read off the
argmax as \(\theta = 45^\circ\).

Finding the argmax solves the problem. You may also want to present your
solution by saying what the value of the output of hdist() is when the
argmax is given as input. You can read off the graph that the maximum of
\(\cos(\theta)\sin(\theta)\) is 0.5 at \(\theta = 45^\circ\), so overall
the distance will be \(v_0^2 / g\)

\hypertarget{interpreting-the-argmax}{%
\section{Interpreting the argmax}\label{interpreting-the-argmax}}

The graphical solution given to the slingshot problem is entirely
satisfactory. Whether that solution will win the contest depends of
course on whether the model we built for the objective function is
correct. There are potentially important things we have left out, such
as air resistence.

Solving the optimization problem has prepared us to go out in the field
and test the result. Perhaps we will find that the real-world optimum
angle is somewhat steeper or shallower than \(\theta = 45^\circ\).

Besides the argmax, another important quantity to read from the graph in
Figure~\ref{fig-ball-theta} is the \textbf{\emph{precision}} of the
argmax. In strict mathematical terms, the argmax for the tennis-ball
problem is exactly 45 degrees at which point
\(\cos(\theta)\sin(\theta) = 0.5\). Suppose, however, that the ball were
launched at only 40 degrees. Five degrees difference is apparent to the
eye, but the result will be essentially the same as for 45 degrees:
\(\cos(\theta)\sin(\theta) = 0.492\). The same is true for a launch
angle of 50 degrees. For both ``sub-optimal'' launch angles, the output
is within 2 percent of the 45-degree result. It is easy to imagine that
a factor outside the scope of the simple model---the wind, for
instance---could change the result by as much or more than 2 percent, so
a practical report of the argmax should reasonable be ``40 to 50
degrees'' rather than ``exactly 45 degrees.''

Contests are won or lost by margins of less than 1\%, so you should not
casually deviate from the argmax. On the other hand, \(45^\circ\) is the
argmax of the \emph{model}. Reality may deviate from the model. For
instance, suppose that air resistance or wind might have an effect of
about 1\% on the distance. Since the real-world function might deviate
by as much as 1\% of the model value, we shouldn't expect the real-world
argmax to be any closer to 45\(^\circ\) than \(\pm 5^\circ\), since
anywhere in that input domain generates an output that is within 1\% of
the maximum output for the model.

\hypertarget{derivatives-and-optimization}{%
\section{Derivatives and
optimization}\label{derivatives-and-optimization}}

We are now going to reframe the search for the argmax and its
interpretation in terms of derivatives of the objective function with
respect to the decision quantity (\(\theta\) in the slingshot problem).
For a function with one input, this will not be an improvement from the
look-at-the-graph technique to find the argmax. A genuine reason to use
derivatives is to set us up in the future to solve problems with more
than one input, where it is hard to draw or interpret a graph. Also,
describing functions in the language of derivatives can help us think
more clearly about aspects of the problem, such as the precision of the
argmax.

With a graph such as Figure~\ref{fig-ball-theta}, it is easy to find the
argmax; common sense carries the day. So it won't be obvious at first
why we are going to take the following approach:

Let's denote an argmax of the objective function \(f(x)\) by
\(x^\star\). Let's look at the derivative \(\partial_x f(x)\) in the
neighborhood of \(x^\star\). Referring to Figure~\ref{fig-ball-theta},
where \(x^\star = 45^\circ\), you may be able to see that
\(\partial_x f(x^\star)\) is zero; the line tangent to the function's
graph at \(x^\star\) is horizontal.

Seen another way, the slope of \(f(x)\) to the left of \(x^\star\) is
positive. Move a tiny bit to the right (that is, increase \(x\) by a
very small amount) leads to an increase in the output \(f(x)\). Just to
the right of \(x^\star\), the slope of \(f(x)\) is negative; as you
reach the top of a hill and continue on, you will be going downhill. So
the derivative function is positive on one side of \(x^\star\) and
negative on the other, suggesting that it crosses zero at the argmax.

Common sense is correct: Walk uphill to get to the peak, walk downhill
to move away from the peak. When you come to the top of a smooth hill,
the terrain is level. (Since our modeling functions are smooth, so must
be the hills that we visualize the functions with.)

Inputs \(x^\star\) such that \(\partial_x f(x^\star) = 0\) are called
\textbf{\emph{critical points}}. Why not call them simply argmaxes?
Because a the slope will also be zero at an argmin. And it is even
possible to have the slope be zero at a point that is neither an argmin
or an argmax.

At this point, we know that values \(x^\star\) that give
\(\partial_x f(x^\star) = 0\) are ``critical points,'' but we haven't
said how to figure out whether a given critical point is an argmax, an
argmin, or neither. This is where the behavior of \(\partial_x f(x)\)
\emph{near} \(x=x^\star\) is important. If \(x^\star\) is an argmax,
then \(\partial_x f(x)\) will be positive to the left of \(x^\star\) and
negative to the right of \(x^\star\); walk up the hill to get to
\(x^\star\), at the top the hill is flat, and just past the top the hill
has a negative slope.

For an argmin, changing \(x\) from less than \(x^\star\) to greater than
\(x^\star\); you will be walking down into the valley, then level at the
very bottom \(x=x^\star\), then back up the other side of the valley
after you pass \(x=x^\star\). Figure~\ref{fig-d2-hill} shows the
situation.

\begin{figure*}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/23-optim_files/figure-pdf/fig-d2-hill-1.pdf}

}

\caption{\label{fig-d2-hill}Top row: An objective function near an
argmax (left) and an argmin (right). Bottom row: The derivative of the
objective function. A horizontal line (orange) has been added to mark
zero on the vertical axis.}

\end{figure*}

The bottom row of graphs in Figure~\ref{fig-d2-hill} shows the
derivative of the objective function \(f(x)\), that is,
\(\partial_x f(x)\). You can see that for the argmax of \(f(x)\), the
derivative \(\partial_x f(x)\) is positive to the left and negative to
the right. Similarly, near the argmin of \(f(x)\), the derivative
\(\partial_x f(x)\) is negative to the left and positive to the right.

Stated another way, the derivative \(\partial_x f(x)\) has a negative
slope just to the left of an argmin and a positive slope to the left of
an argmax.

The second derivative of the objective function \(f(x)\) at a critical
point \(x^\star\) is what tells us whether the critical point is an
argmax, an argmin, or neither.

\marginnote{\begin{footnotesize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3651}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3016}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Critical point \(x^\star\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\partial_x f(x^\star)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\partial_{xx} f(x^\star)\)
\end{minipage} \\
\midrule
\endhead
argmax & 0 & negative \\
argmin & 0 & positive \\
neither & 0 & 0 \\
\bottomrule
\end{longtable}

\end{footnotesize}}

\begin{takenote}

Throughout Block 2, we have translated features of functions that are
evident on a graph into the language of derivatives:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  The \textbf{slope} of a function \(f(x)\) at any input \(x\) is the
  \textbf{value} of the derivative function \(\partial_x f(x)\) at that
  same \(x\).
\item
  The \textbf{concavity} of a function \(f(x)\) at any input is the
  \textbf{slope} of the derivative function, that is,
  \(\partial_{xx} f(x)\).
\item
  Putting (i) and (ii) together, we get that the \textbf{concavity} of a
  function \(f(x)\) at any input \(x\) is the \textbf{value of the
  second derivative} function, that is, \(\partial_{xx} f(x)\).
\item
  At an argmax \(x^\star\) of \(f(x)\), the value of the derivative
  function \(\partial_x f(x^\star)\) is zero and the value of the second
  derivative function \(\partial_{xx} f(x^\star)\) is \textbf{negative}.
  The situation at an argmin is along the same lines, the derivative of
  the objective function is zero and the second derivative is
  \textbf{positive}.
\end{enumerate}

\end{takenote}

\begin{practice}
What's the critical point?

You're familiar with the quadratic polynomial:
\[g(x) = a_0 + a_1 x + a_2 x^2\] The graph of a quadratic polynomial is
a \textbf{\emph{parabola}}, which might be concave up or concave down.
As you know, a parabola has only one critical point, which might be an
argmin or an argmax.

Let's find the critical point. We know that the critical point is
\(x^\star\) such that \(\partial_x g(x^\star) = 0\). Since we know how
to differentiate a power law, we can see that
\[\partial_x g(x) = a_1 + 2 a_2 x\] and, more specifically, at the
critical point \(x^\star\) the derivative will be
\[a_1 + 2 a_2 x^\star = 0\] The above is an equation, not a definition.
It says that whatever \(x^\star\) happens to be, the quantity
\(a_1 + 2 a_2 x^\star\) must be zero. Using plain old algebra, we can
find the location of the critical point \[x^\star = -\frac{a_1}{2 a_2}\]

\end{practice}

\begin{intheworld}

In economics, a monopoly or similar arrangement can set the price for a
good or commodity. Monopolists can set the price at a level that
generates the most income for themselves.

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{Differentiation/www/cournot-demand-curve.png}

}

\caption{\label{fig-cournot-demand2}Demand as a \emph{function} of
price, as first published by Antoine-Augustin Cournot in 1836.
\href{https://www.richmondfed.org/~/media/richmondfedorg/publications/research/economic_review/1992/pdf/er780201.pdf}{Source})}

\end{figure}

In 1836, early economist Antoine-Augustin Cournot published a theory of
revenue versus demand based on his conception that demand will be a
monotonically decreasing function of price. (That is, higher price means
lower demand.) we will write as \(\text{Demand}(p)\) demand as a
function of price.

The \textbf{\emph{revenue}} generated at price \(p\) is
\(R(p) \equiv p \text{Demand}(p)\): price times demand.

To find the revenue-maximizing demand, differentiate \(R(p)\) with
respect to \(p\) and find the argmax \(p^\star\) at with
\(\partial_p R(p^\star) = 0).\) This can be done with the product rule.

\[\partial_p R(p) = p \ \partial_p \text{Demand}(p) + \text{Demand}(p)\]
At the argmax \(p^\star\) we have:
\[p^\star \partial_p \text{Demand}(p^\star) + \text{Demand}(p^\star) = 0 \ \ \stackrel{\text{solving for}\ p^\star}{\Longrightarrow} \ \ p^\star = - \frac{\text{Demand}(p^\star)}{\partial_p \text{Demand}(p^\star)}\]

If the monopolist knows the demand function \(D(p)\), finding the
revenue maximizing price is a simple matter. But in general, the
monopolist does not know the demand function in advance. Instead, an
informed guess is made to set the initial price \(p_0\). Measuring sales
\(D(p_0)\) gives one point on the demand curve. Then, try another price
\(p_1\). This gives another point on the demand curve as well as an
estimate \[\partial_p D(p_0) = \frac{D(p_1) - D(p_0)}{p_1 - p_0}\] Now
the monopolist is set to model the demand curve as a straight-line
function and easily to find \(p^\star\) for the model. For instance, if
the demand function is modeled as \(D_1 (p) = a + b p\), the optimal
price will be \(p^\star_1 = - \frac{a + b p^\star}{b}\) which can be
solved as \(p^\star_1 = - a/2b\).

\(p^\star_1\) is just an estimate of the optimum price. Still, the
monopolist can try out that price, giving a third data point for the
demand function from which a better model of the demand function can be
constructed. With the better estimate, find a new a argmax
\(p^\star_2\). This sort of iterative process for finding an argmax of a
real-world function is very common in practice.

\end{intheworld}

\hypertarget{sec-flat-on-top}{%
\section{Be practical!}\label{sec-flat-on-top}}

Decision making is about choosing among alternatives. In some
engineering or policy contexts, this can mean finding a value for an
input that will produce the ``best'' outcome. For those who have studied
calculus, it is natural to believe that calculus-based techniques for
optimization are the route to making the decision.

We emphasize that the optimization techniques covered in this chapter
are \textbf{only part} of a broader set of techniques for real-world
decision-making problems. In particular, most policy contexts involve
\textbf{\emph{multiple objectives}}. For example, in designing a car one
goal is to make it cheap to manufacture, another to make it attractive,
and still another to make it safe. These different objectives are often
at odds with one another. In Block 4 of this text, we will discuss some
calculus techniques that help policy-makers in multi-objective settings.

For now, sticking with the idealized (and often unrealistic) setting of
maximizing a single objective, with one or more inputs. Recall the
setting for calculus-type maximization. You have a function with one or
more inputs, say, \(f(x)\) or \(g(x,y)\) or, often,
\(h(x, y, z, \ldots)\) where \(\ldots\) might be standing for tens or
hundreds or thousands of inputs or more.

If you can graph the function (feasible for one- or two-input
functions), you can often easily scan the graph by eye to find the peak.
The calculus-based techniques were developed for situations where such
graphing is not possible and, instead, you have a formula for the
function. (Such occasions are of great theoretical interest but not all
that common in practice.) The basis of the calculus techniques is the
observation that, at the argmax of a smooth function, the derivative of
the function is 0.

As an example, consider
\href{https://www.dummies.com/education/math/calculus/calculate-the-optimum-volume-of-a-soup-can-practice-question/}{a
style problem that often appears in calculus textbooks}. You have been
tasked to design a container for a large volume V of liquid. It is
desired to make the weight of the container as little as possible. (This
is a \emph{minimization} problem, then.) In classical textbook fashion,
you are told that the container is to be a cylinder made out of a
particular metal of a particular thickness.

This is a lovely geometry/calculus problem. Whether it is relevant to
any genuine, real-world problem is another question.

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/www/cylinder.png}

}

\end{marginfigure}

Using the notation in the diagram, the volume and surface area of the
cylinder is
\[V(r, h) \equiv \pi r^2 h \ \ \ \text{and}\ \ \ A(r, h) \equiv 2 \pi r^2 + 2 \pi r h\]

Minimizing the weight of the cylinder is our objective (according to the
problem statement) and the weight is proportional to the surface area.
Since the volume \(V\) is given (according to the problem statement), we
want to re-write the area function to use volume:

\[h(r, V) \equiv V / \pi r^2 \ \ \ \implies\ \ \ A(r, V) = 2 \pi r^2 + 2 \pi r V/\pi r^2 = 2 \pi r^2 + 2 V / r\]
Suppose \(V\) were specified as 1000 liters. A good first step is to
choose appropriate units for \(r\) to make sure the formula for
\(A(r, V)\) is dimensionally consistent. Suppose we choose \(r\) in cm.
Then we want \(V\) in cubic centimeters (cc). 1000 liters is 1,000,000
cc. Now we can plot a slice of the area function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A }\OtherTok{\textless{}{-}} \FunctionTok{makeFun}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{*}\NormalTok{r}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{V}\SpecialCharTok{/}\NormalTok{r }\SpecialCharTok{\textasciitilde{}}\NormalTok{ r, }\AttributeTok{V=}\DecValTok{1000000}\NormalTok{)}
\FunctionTok{slice\_plot}\NormalTok{(}\FunctionTok{A}\NormalTok{(r) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ r, }\FunctionTok{domain}\NormalTok{(}\AttributeTok{r=}\FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{gf\_labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"radius (cm)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Surface area of container (square cm)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/23-optim_files/figure-pdf/unnamed-chunk-7-1.pdf}

}

\end{marginfigure}

As always, the function's derivative is zero at the optimal \(r\). In
the graph, the argmin is near \(r=50\) cm at which point the minimum is
about 50,000 cm\(^2\). Since \(h(r,V) = V/\pi r^2\), the required height
of cylinder will be near \(10^6 / \pi 50^2 = 127\)cm.

In calculus courses, the goal is often to find a \textbf{formula} for
the optimal radius as a function of \(V\). So we differentiate the
objective function---that is, the area function for any \(V\) and \(r\)
with respect to \(r\), \[\partial_r A(r, V) = 4 \pi r - 2 V / r^2\]
Setting this to zero (which will be true at the optimal \(r^\star\)) we
can solve for \(r^\star\) in terms of \(V\):
\[4 \pi r^\star - 2 \frac{V}{\left[r^\star\right]^2} = 0 \ \ \ \Longrightarrow\ \ \ 4\pi r^\star = 2\frac{V}{\left[r^\star\right]^2} \Longrightarrow\ \ \ \left[r^\star\right]^3 = \frac{1}{2\pi} V \ \ \ \Longrightarrow\ \ \  r^\star = \sqrt[3]{V/2\pi}\]

For \(V = 1,000,000\) cm\(^3\), this gives \(r^\star = 54.1926\) cm
which in turn implies that the corresponding height
\(h^\star = V/\pi (r^\star)^2 = 108.3852\) cm.

We've presented the optimum \(r^\star\) and \(h^\star\) to the nearest
\textbf{micron}. (Does that make sense to you? Think about it for a
moment before reading on.)

A good rule of thumb in modeling is this: ``If you don't know what a
sensible precision is for reporting your result, you don't have a
complete grasp of the problem.'' Here are two reasonable ways to sort
out a suitable precision.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Solve a closely related problem which for many practical purposes
  would have been equivalent.
\item
  Look at how big a change in the output of the objective function is
  produced by a change from the argmax.
\end{enumerate}

Approach (2) is always at hand, since you already know the objective
function. Let's graph the objective function near \(r = 54.1926\)
\ldots{}

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/23-optim_files/figure-pdf/unnamed-chunk-8-1.pdf}

}

\end{marginfigure}

Look carefully at the axes scales. Deviating from the mathematical
optimum by about 5cm (that is, 50,000 microns) produces a change in the
output of the objective function by about 400 units \textbf{out of
55,000}. In other words, about 0.7\%.

It is true that \(r^\star = 54.1926\) cm gives the ``best'' outcome. And
sometimes such precision is warranted. For example, improving the speed
of an elite marathon racer by even 0.1\% would give her a 7 second
advantage: often the difference between silver and gold!

What's different is that you know exactly what is the ultimate objective
of a marathon: finish faster. But you may not know the ultimate
objective of the system your ``optimal'' tank will be a part of. For
instance, your tank may be part of an external fuel pod on an aircraft.
Certainly the designers of the aircraft want the tank to be as light as
possible. But they also want to reduce drag as much as possible. A 54 cm
diameter tube has about 17\% more drag than a 50 cm tube. To save that
much drag, it is probably well worth increasing weight by 0.7\%.

In reporting the results from an optimization problem, you ought to give
the decision maker all relevant information. Here, that might be as
simple as including the above graph in your report.

We mentioned another technique for getting a handle on what precision is
meaningful: (1) solve a closely related problem. This often requires
some insight and creativity to frame the new problem. Here, we note that
large capacity tanks often are shaped like a lozenge: a cylinder with
hemi-spherical ends.

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/www/hemisphere.png}

}

\end{marginfigure}

Using \(h\) for the length of the cylindrical portion of the tank, and
\(r\) for the radius, the volume and surface area are:
\[V(r, h) = \pi r^2 h + \frac{4}{3} \pi r^3 \ \ \ \text{and}\ \ \ A(r,h) = 2 \pi r h + 4 \pi r^2\]
Again, \(V\) was specified as 1000 liters. As detailed in Exercise
23.18, the surface area of this 1000-liter tank is about 48,400
cm\(^2\). This is more than 10\% less than for the cylindrical tank.

\hypertarget{exercises-7}{%
\section{Exercises}\label{exercises-7}}

\hypertarget{sec-partial-change}{%
\chapter{Partial change and the gradient
vector}\label{sec-partial-change}}

We have two ways by which we represent functions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  As a \textbf{\emph{computational algorithm}} for generating the output
  from an input(s), typically involving arithmetic and such.
\item
  As a geometrical entity, specifically the graph of a function which
  can be a \textbf{\emph{curve}} or, for functions of two inputs, a
  \textbf{\emph{surface}}.
\end{enumerate}

These two modes are sometimes intertwined, as when we use the name
``line'' to refer to a computational object:
\(\line(x) \equiv a x + b\).

Unfortunately for functions of two inputs, a \textbf{\emph{surface}} is
hard to present in the formats that are most easily at hand: a piece of
paper, a printed page, a computer screen. That isbecause a curved
surface is naturally a 3-dimensional object, while paper and screens
provide two-dimensional images. Consequently, the graphics mode we
prefer for presenting functions of two inputs is the contour plot, which
is not a single geometrical object but a set of many objects: contours,
labels, colored tiles.

We've been doing calculus on functions with one input because it is so
easy to exploit both the computational mode and the graphical mode. And
it might fairly be taken as a basic organizing theme of calculus that

\begin{quote}
a line segment approximates a curve in a small region around a point.
\end{quote}

When figuring out the derivative function \(\partial_x f(x)\) from a
graph of \(f(x)\), we find the tangent to the graph at each of many
input values, record the slope of the line (and throw away the
intercept) and then write down the series of slopes as a function of the
input, typically by representing the slope by position along the
vertical axis and the corresponding input by position along the
horizontal axis. Figure~\ref{fig-segment-deriv} shows the process.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/24-partial_files/figure-pdf/unnamed-chunk-2-1.pdf}

}

\end{figure}

\begin{figure}

\sidecaption{\label{fig-segment-deriv}(A) The graph of a smooth function
annotated with small line segments that approximate the function
locally. The color of each labeled segment corresponds to the value of
\(x\) for that segment. The slope of each segment is written numerically
below the segment. (B) The labeled dots show the slope of each segment
from (A). The slope is encoded using vertical position (as usual) and
carries over the numerical label from (A). Connecting the dots sketches
out the derivative of the function in (A).}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/24-partial_files/figure-pdf/fig-segment-deriv-1.pdf}

}

\end{figure}

Panel (A) in Figure~\ref{fig-segment-deriv} shows a smooth function
\(f(x)\) (thin black curve). To find the function \(\partial_x f(x)\),
we take the slope of \(f(x)\) at many closely spaced inputs. In Panel
(A), we've highlighted short, tangent line segments at the
closely-spaced points labeled A through V. The slope of each tangent
line segment can be calculated by the usual rise-over-run method; the
numerical value of the slope is written underneath the segment. To plot
the derivative \(\partial_x f(x)\), I have taken the slope information
from (A) and plotted it as a function of \(x\).

To restate what you already know, in the neighborhood of any input value
\(x\), the slope of any local straight-line approximation to \(f(x)\) is
given by the value of of \(\partial_x f(x)\).

\hypertarget{calculus-on-two-inputs}{%
\section{Calculus on two inputs}\label{calculus-on-two-inputs}}

Although we use contour plots for good practical reasons, the
\textbf{\emph{graph}} of a function \(g(x,y)\) with two inputs is a
surface, as described in Section @ref(surface-plot). The derivative of
\(g(x,y)\) should encode the information needed to approximate the
surface at any input \((x,y)\). In particular, we want the derivative of
\(g(x,y)\) to tell us the orientation of the \textbf{\emph{tangent
plane}} to the surface.

A tangent plane is infinite in extent. Let's use the word
\textbf{\emph{facet}} to refer to a little patch of the tangent plane
centered at the point of contact. Each facet is flat. (it is part of a
plane!) Figure~\ref{fig-melon-facets} shows some facets tangent to a
familiar curved surface. No two of the facets are oriented the same way.

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/www/tangent-planes-on-melon.png}

}

\caption{\label{fig-melon-facets}A melon as a model of a curved surface
such as the graph of a function of two inputs. Each tangent facet has
its own orientation. (Disregard the slight curvature of the small pieces
of paper. Summer humidity has interfered with my attempt to model a flat
facet with a piece of Post-It paper!}

\end{marginfigure}

Better than a picture of a summer melon, pick up a hardcover book and
place it on a curved surface such as a basketball. The book cover is a
flat surface: a facet. The orientation of the cover will match the
orientation of the surface at the point of tangency. Change the
orientation of the cover and you will find that the point of tangency
will change correspondingly.

If melons and basketballs are not your style, you can play the same game
on an interactive graph of a function with two inputs. The snapshot
below is a link to an applet that shows the graph of a function as a
\textbf{blue} surface. You can specify a point on the surface by setting
the value of the (x, y) input using the sliders. Display the tangent
plane (which will be \textbf{green}) at that point by check-marking the
``Tangent plane'' input. (Acknowledgments to Alfredo Sánchez Alberca who
wrote the applet using the GeoGebra math visualization system.)

::: \{asis eval=knitr::is\_html\_output()\} :::

\begin{figure}

\sidecaption{\label{fig-tangent-app}An interactive app for visualizing
tangent planes. To open, go to
\url{https://www.geogebra.org/m/wTh7KKd3}.}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/www/geogebra-tangent-app.png}

}

\end{figure}

For the purposes of computation by eye, a contour graph of a surface can
be easier to deal with. Figure~\ref{fig-whole-plot} shows the contour
graph of a smoothly varying function. Three points have been labeled A,
B, and C.

\begin{figure}

\sidecaption{\label{fig-whole-plot}A function of 2 inputs with 3
specific inputs marked A, B, and C}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/24-partial_files/figure-pdf/fig-whole-plot-1.pdf}

}

\end{figure}

Zooming in on each of the marked points presents a simpler picture for
each of them, although one that is different for each point. Each
zoomed-in plot contains almost parallel, almost evenly spaced contours.
If the surface had been exactly planar over the entire zoomed-in domain,
the contours would be \emph{exactly} parallel and exactly evenly spaced.
We can approach such exact parallelness by zooming in more closely
around the labeled point.

\begin{figure*}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/24-partial_files/figure-pdf/fig-zoomed-plot-1.pdf}

}

\caption{\label{fig-zoomed-plot}Zooming in on the neighborhoods of A, B,
and C in Figure~\ref{fig-whole-plot} shows a simple, almost planar,
local landscape. The bottom row shows the contours of the tangent plane
near each of the neighborhoos in the top row.}

\end{figure*}

Just as the function \(\line(x) \equiv a x + b\) describes a straight
line, the function \(\text{plane}(x, y) \equiv a + b x + c y\) describes
a plane whose orientation is specified by the value of the parameters
\(b\) and \(c\). (Parameter \(a\) is about the vertical location of the
plane, not it is orientation.)

In the bottom row of Figure~\ref{fig-zoomed-plot}, the facets tangent to
the original surface at A, B, and C are displayed. Comparing the top and
bottom rows of Figure~\ref{fig-zoomed-plot}) you can see that each facet
has the same orientation as the surface; the contours face in the same
way.

Remember that the point of constructing such facets is to generalize the
idea of a derivative from a function of one input \(f(x)\) to functions
of two or more inputs such as \(g(x,y)\). Just as the derivative
\(\partial_x f(x_0)\) reflects the \textbf{\emph{slope}} of the
\emph{line} tangent to the graph of \(f(x)\) at \(x=x_0\), our plan for
the ``derivative'' of \(g(x_0,y_0)\) is to represent the
\textbf{\emph{orientation}} of the facet tangent to the graph of
\(g(x,y)\) at \((x=x_0, y=y_0)\). The question for us now is what
information is needed to specify an orientation.

One clue comes from the formula for a function whose graph is a plane
oriented in a particular direction:

\[\text{plane}(x,y) \equiv a + b x + cy\]

\begin{scaffolding}
To explore the roles of the parameters \(b\) and \(c\) in setting the
orientation of the line, open a
\href{https://maa-statprep.shinyapps.io/CalcZ-Sandbox/}{SANDBOX}. The
scaffolding code generates a particular instance of
\(\text{plane}(x,y)\) and plots it in two ways: a contour plot and a
surface plot. Change the numerical values of \(b\) and \(c\) and observe
how the orientation of the planar surface changes in the graphs. You can
also see that the value of \(a\) is irrelevant to the orientation of the
plane, just as the intercept of a straight-line graph is irrelevant to
the slope of that line.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plane }\OtherTok{\textless{}{-}} \FunctionTok{makeFun}\NormalTok{(a }\SpecialCharTok{+}\NormalTok{ b}\SpecialCharTok{*}\NormalTok{x }\SpecialCharTok{+}\NormalTok{ c}\SpecialCharTok{*}\NormalTok{y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ y, }\AttributeTok{a =} \DecValTok{1}\NormalTok{, }\AttributeTok{b =} \SpecialCharTok{{-}}\FloatTok{2.5}\NormalTok{, }\AttributeTok{c =} \FloatTok{1.6}\NormalTok{)}
\ControlFlowTok{if}\NormalTok{ (knitr}\SpecialCharTok{::}\FunctionTok{is\_html\_output}\NormalTok{()) \{}
  \FunctionTok{interactive\_plot}\NormalTok{(}\FunctionTok{plane}\NormalTok{(x, y) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ y, }\FunctionTok{domain}\NormalTok{(}\AttributeTok{x=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{y=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)))}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\StringTok{"www/plane{-}3d.png"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/www/plane-3d.png}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{contour\_plot}\NormalTok{(}\FunctionTok{plane}\NormalTok{(x, y) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ y, }\FunctionTok{domain}\NormalTok{(}\AttributeTok{x=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{y=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{gf\_refine}\NormalTok{(}\FunctionTok{coord\_fixed}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/24-partial_files/figure-pdf/unnamed-chunk-9-2.pdf}

}

\end{figure}

As always it can be difficult to extract quantitative information from a
surface plot. For the example here, you can see that the high-point on
the surface is when \(x\) is most negative and \(y\) is most positive.
Compare that to the contour plot to verify that two modes are displaying
the same surface.

(Note: The \texttt{gf\_refine(coord\_fixed())} part of the contour-plot
command makes numerical intervals on the horizontal and vertical axes
have the same length.)

\end{scaffolding}

An instructive experience is to pick up a rigid, flat object, for
instance a smartphone or hardcover book. Hold the object level with
pinched fingers at the mid-point of each of the short ends, as shown in
\textbf{?@fig-hold-book} (left).

\begin{figure*}

{\centering \includegraphics[width=0.3\textwidth,height=\textheight]{Differentiation/www/level-book.png}

}

\caption{\label{fig-hold-book-1}Combining two simple movements can tip a
plane to all sorts of different orientations.}

\end{figure*}

\begin{figure*}

{\centering \includegraphics[width=0.3\textwidth,height=\textheight]{Differentiation/www/rotated-book.png}

}

\caption{\label{fig-hold-book-2}Combining two simple movements can tip a
plane to all sorts of different orientations.}

\end{figure*}

\begin{figure*}

{\centering \includegraphics[width=0.3\textwidth,height=\textheight]{Differentiation/www/tipped-book.png}

}

\caption{\label{fig-hold-book-3}Combining two simple movements can tip a
plane to all sorts of different orientations.}

\end{figure*}

You can tip the object in one direction by raising or lowering one hand.
(middle picture) And you can tip the object in the other coordinate
direction by rotating the object around the line joining the points
grasped by the left and right hands. (right picture) By combining these
two motions, you can orient the surface of the object in a wide range of
directions.\sidenote{\footnotesize In describing the orientation of aircraft and
  ships, three parameters are used: pitch, roll, and yaw. For a
  geometrical plane (as opposed to an aircraft or ship, which have
  distinct front and back ends), yaw isn't applicable.}

The purpose of this lesson is to show that two-numbers are sufficient to
dictate the orientation of a plane. In terms of \textbf{?@fig-hold-book}
these are 1) the amount that one hand is raised relative to the other
and 2) the angle of rotation around the hand-to-hand axis.

Similarly, in the formula for a plane, the orientation is set by two
numbers, \(b\) and \(c\) in \(\text{plane}(x, y) \equiv a + b x + c y\).

How do we find the right \(b\) and \(c\) for the tangent facet to a
function \(g(x,y)\) at a specific input \((x_0, y_0)\)? Taking slices of
\(g(x,y)\) provides the answer. In particular, these two slices:
\[\text{slice}_1(x) \equiv g(x, y_0) = a + b\, x + c\, y_0 \\ \text{slice}_2(y) \equiv g(x_0, y) = a + b x_0 + c\, y\]

Look carefully at the formulas for the slices. In \(\text{slice}_1(x)\),
the value of \(y\) is being held constant at \(y=y_0\). Similarly, in
\(\text{slice}_2(y)\) the value of \(x\) is held constant at \(x=x_0\).

The parameters \(b\) and \(c\) can be read out from the derivatives of
the respective slices: \(b\) is equal to the derivative of the
slice\(_1\) function with respect to \(x\) evaluated at \(x=x_0\), while
\(c\) is the derivative of the slice\(_2\) function with respect to
\(y\) evaluated at \(y=y_0\). Or, in the more compact mathematical
notation:

\[b = \partial_x \text{slice}_1(x)\left.\strut\right|_{x=x_0} \ \ \text{and}\ \ c=\partial_y \text{slice}_2(y)\left.\strut\right|_{y=y_0}\]
These derivatives of slice functions are called \textbf{\emph{partial
derivatives}}. The word ``partial'' refers to examining just one input
at a time. In the above formulas, the \({\large |}_{x=x_0}\) means to
evaluate the derivative at \(x=x_0\) and \({\large |}_{y=y_0}\) means
something similar.

You don't need to create the slices explicitly to calculate the partial
derivatives. Simply differentiate \(g(x, y)\) with respect to \(x\) to
get parameter \(b\) and differentiate \(g(x, y)\) with respect to \(y\)
to get parameter \(c\). To demonstrate, we will make use of the
\textbf{\emph{sum}} rule:
\[\partial_x g(x, y) = \underbrace{\partial_x a}_{=0} + \underbrace{\partial_x b x}_{=b} + \underbrace{\partial_x cy}_{=0} = b\]
Similarly,
\[\partial_y g(x, y) = \underbrace{\partial_y a}_{=0} + \underbrace{\partial_y b x}_{=0} + \underbrace{\partial_y cy}_{=c} = c\]

\begin{takenote}
Get in the habit of noticing the subscript on the differentiation symbol
\(\partial\). When taking, for instance,
\(\partial_y f(x,y,z, \ldots)\), all inputs other than \(y\) are to be
\textbf{\emph{held constant}}. Some examples:

\[\partial_y 3 x^2 = 0\ \ \text{but}\ \ \ 
\partial_x 3 x^2 = 6x\\
\ \\
\partial_y 2 x^2 y = 2x^2\ \ \text{but}\ \ \
\partial_x 2 x^2 y = 4 x y
\]

\end{takenote}

\hypertarget{all-other-things-being-equal}{%
\section{All other things being equal
\ldots{}}\label{all-other-things-being-equal}}

Recall that the derivative of a function with one input, say,
\(\partial_x f(x)\) tells you, at each possible value of the input
\(x\), how much the output will change proportional to a small change in
the value of the input.

Now that we are in the domain of multiple inputs, writing \(h\) to stand
for ``a small change'' is not entirely adequate. Instead, we will write
\(dx\) for a small change in the \(x\) input and \(dy\) for a small
change in the \(y\) input.

With this notation, we write the first-order polynomial approximation to
a function of a single input \(x\) as
\[f(x+dx) = f(x) + \partial_x f(x) \times dx\] Applying this notation to
functions of two inputs, we have:
\[g(x + \color{magenta}{dx}, y) = g(x,y) + \color{magenta}{\partial_x} g(x,y) \times \color{magenta}{dx}\]
and
\[g(x, y+\color{brown}{dy}) = g(x,y) + \color{brown}{\partial_y} g(x,y) \times \color{brown}{dy}\]

Each of these statements is about \emph{changing one input} while
\emph{holding the other input(s) constant}. Or, as the more familiar
expression goes, ``The effect of changing one input \textbf{all other
things being equal} or \textbf{all other things held
constant}.\sidenote{\footnotesize The Latin phrase for this is \emph{ceteris paribus},
  often used in economics.}

Everything we've said about differentiation rules applies not just to
functions of one input, \(f(x)\), but to functions with two or more
inputs, \(g(x,y)\), \(h(x,y,z)\) and so on.

\hypertarget{gradient-vector}{%
\section{Gradient vector}\label{gradient-vector}}

For functions of two inputs, there are two partial derivatives. For
functions of three inputs, there are three partial derivatives. We can,
of course, collect the partial derivatives into Cartesian coordinate
form. This collection is called the \textbf{\emph{gradient vector}}.

Just as our notation for differences (\(\cal D\)) and derivatives
(\(\partial\)) involves unusual typography on the letter ``D,'' the
notation for the gradient involves such unusual typography although this
time on \(\Delta\), the Greek version of ``D.'' For the gradient symbol,
turn \(\Delta\) on its head: \(\nabla\). That is,
\[\nabla g(x,y) \equiv \left(\stackrel\strut\strut\partial_x g(x,y), \ \ \partial_y g(x,y)\right)\]

Note that \(\nabla g(x,y)\) is a function of both \(x\) and \(y\), so in
general the gradient vector differs from place to place in the
function's domain.

The graphics convention for drawing a gradient vector for a particular
input, that is, \(\nabla g(x_0, y_0)\), puts an arrow with its root at
\((x_0, y_0)\), pointing in direction \(\nabla g(x_0, y_0)\), as in
Figure~\ref{fig-one-grad-arrow}.

\begin{figure}

\sidecaption{\label{fig-one-grad-arrow}The gradient vector
\(\nabla g(x=1,y=2)\). The vector points in the steepest uphill
direction. Consequently, it is perpendicular to the contour passing
through its root.}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/24-partial_files/figure-pdf/fig-one-grad-arrow-1.pdf}

}

\end{figure}

A \textbf{\emph{gradient field}} (see \textbf{?@fig-gradient-field-a})
is the value of the gradient vector at each point in the function's
domain. Graphically, to prevent over-crowding, the vectors are drawn at
discrete points. The lengths of the drawn vectors are set proportional
to the numerical length of \(\nabla g(x, y)\), so a short vector means
the surface is relatively level, a long vector means the surface is
relatively steep.

\begin{figure}

\sidecaption{\label{fig-gradient-field-A}A plot of the gradient field
\(\nabla g(x,y)\).}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/24-partial_files/figure-pdf/fig-gradient-field-A-1.pdf}

}

\end{figure}

\hypertarget{total-derivative-optional}{%
\section{Total derivative (optional)}\label{total-derivative-optional}}

The name ``partial derivative'' suggests the existence of some kind of
derivative that is not just a part, but the whole thing. The
\textbf{\emph{total derivative}} is such a whole and gratifyingly made
up of its parts, that is, the partial derivatives.

Suppose you are modeling the temperature of some volume of the
atmosphere, given as \(T(t, x, y, z)\). This merely says that the
temperature depends on both time and location, something that is
familiar from everyday life.

The partial derivatives have an easy interpretation: \(\partial_t T()\)
tells how the temperature is changing over time at a given location,
perhaps because of the evaporation or condensation of water vapor.
\(\partial_x T()\) tells how the temperature changes in the \(x\)
direction, and so on.

The total derivative gives an overall picture of the changes in a parcel
of air, which you can thnk of as a tiny balloon-like structure but
without the balloon membrane. The temperature inside the ``balloon'' may
change with time (e.g.~condensation or evaporation of water), but as the
ballon drifts along with the motion of the air (that is, the wind), the
evolving location can change the temperature as well. Think of a balloon
caught in an updraft: the temperature goes down as the balloon ascends.

For an imaginary observer located in the balloon, the temperature is
changing with time. Part of this change is the instrinsic change
measured by \(\partial_t T\) but we need to add to that the changes
induces by the evolving location of the balloon. The partial change in
temperature due to a change in altitude is \(\partial_z T\), but it is
important to realize that the coordinates of the location are themselves
functions of time: \(x(t), y(t), z(t)\). Seeing the function \(T()\) for
the observer in the balloon as a function of \(t\), we have
\(T(t, x(t), y(t), z(t))\). This is a function composition: \(T()\)
composed with each of \(x()\), \(y()\), and \(z()\). Recall in the chain
rule \(\partial_v f(g(v)) = \partial_v f(g(v)) \partial_v g(v)\) that
the derivative of the composed quantity is the product of two
derivatives.

Likewise, the total derivative of temperature with respect to the
observer riding in the balloon will be add together the parts due to
changes in time (holding position constant), x-coordinate (holding time
and the other space coordinates constant), and the like. Signifying the
total differentiation with a capital \(D\), we have
\[D\, T(t) = \partial_t T() + \partial_x T() \cdot\partial_t x + \partial_y T()\cdot \partial_t y + \partial_z T() \cdot\partial_t z\]
Note that \(\partial_t x\) is the velocity of the balloon in the
x-direction, and similarly for the other coordinate directions. Writing
these velocities as \(v_x, v_y, v_z\), the total derivative for
temperature of a parcel of air embedded in a moving atmosphere is

\[D\ T(t) = \partial_t T + v_x\, \partial_x T + v_y\, \partial_y T + v_z\, \partial_z T\]
Formulations like this, which put the parts of change together into a
whole, are often seen in the mathematics of fluid flow as applied in
meteorology and oceanology.

\hypertarget{sec-differential-skier}{%
\section{Differentials}\label{sec-differential-skier}}

\begin{quote}
\emph{A little bit of this, a little bit of that.} --- Stevie Wonder,
``The Game of Love''
\end{quote}

We have framed calculus in terms of \textbf{\emph{functions}}:
transformations that take one (or more!) quantities as input and return
a quantity as output. This was not the original formulation. In this
section, we will use the original style to demonstrate how you can
sometimes skip the step of constructing a function before
differentiating to answer a question of the sort: ``If this quantity
changes by a little bit, how much will another, related quantity
change?''

As an example, consider the textbook-style problem of a water skier
being pulled along the water by a rope pulled in from the top of a tower
of height \(H\). The skier is distance \(x\) from the tower. As the rope
is winched in at a constant rate, does the skier go faster or slower as
she approaches the tower.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/www/water-skier.png}

}

\end{figure}

In the function style of approach, we can write the position function
\(x(t)\) with input the length of the rope \(L(t)\). Using the diagram,
you can see that \[x(t) = \sqrt{\strut L(t)^2 - H^2}\ .\]

Differentiate both sides with respect to \(t\) to get the velocity of
the skier: \(\partial_t x(t)\) through the chain rule:
\[\underbrace{\partial_t x(t)}_{\partial_t f(g(t))} = \underbrace{\frac{1}{2\sqrt{\strut L(t)^2 - H^2}}}_{\left[ \partial_t f \right](g(t)) } \times \underbrace{\left[2 \partial_t L(t)\right]}_{\partial_t g(t)} = \frac{\partial_t L(t)}{\strut\sqrt{L(t)^2 - H^2}}\]

Now to reformulate the problem without defining a function.

Newton referred to ``flowing quantities'' or ``\emph{fluents}'' and to
what today is universally called derivatives as ``\emph{fluxions}.''
Newton did not have a notion of \textbf{\emph{inputs}} and
\textbf{\emph{output}}.\sidenote{\footnotesize The meaning of ``output'' as ``to
  produce'' dates from more than 100 years after Newton's death.}

At about the same time as Newton's inventions, very similar ideas were
being given very different names by mathematicians on the European
continent. There, an infinitely small change in a quantity was called a
``\emph{differential}'' and the differential of \(x\) was denoted
\(dx\).

The first calculus textbook was subtitled, \emph{Of the Calculus of
Differentials}, in other words, how to calculate differentials. (See
Figure~\ref{fig-calcul-des-differences}.) Section I of this 1696 text is
entitled, ``Where we give the rules of this calculation,'' those rules
being recognizably the same as presented in Chapter
Section~\ref{sec-prod-comp-rules} of this book.

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/www/Hopital-part-1.png}

}

\caption{\label{fig-calcul-des-differences}From the start of the first
calculus textbook, by le marquis de l'Hôpital, 1696.}

\end{marginfigure}

Definition I of Section I states,

\begin{quote}
``\emph{We call quantities }variable* that grow or decrease
continuously; and to the contrary \emph{constant} quantities are those
that remain the same while the others change. \ldots{} The infinitely
small amount by which a continuous quantity increases or decreases is
called the \textbf{differential}.*''
\end{quote}

The differential is not a derivative. The differential is an
\textbf{\emph{infinitely small change}} in a quantity and a derivative
is a \textbf{\emph{rate of change}}. The differential of a quantity
\(x\) is written \(dx\) in the textbook.\sidenote{\footnotesize A ``warning'' is given
  in the textbook that the symbol \(d\) will always be used to mark the
  differential of a variable quantity and that \(d\) will never be used
  to indicate a parameter.}

The point of Section I of de l'Hôpital's textbook is to present the
rules by which the differentials of complex quantities can be
calculated. You will recognize the product rule in de l'Hôpital's
notation:

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/www/lhopitals-product-rule.png}

}

\caption{The differential of \(x\,y\) is \(y\,dx + x\,dy\)}

\end{marginfigure}

The Pythagorean theorem relates the various quantities this way:

\[L^2 = x^2 + H^2\]

The differential of each side of the equation refers to ``a little bit''
of increase in the quantity on that side of the equation:
\[d(L^2) = d(x^2)\ \ \ \implies\ \ \ 2 L\, dL = 2 x\, dx\] where we've
used one of the ``rules'' for calculating differentials. This gives us
\[dx = \frac{L}{x} dL\] Think of this as a recipe for calculating
\(dx\). If you tell me \(L\), \(x\), and \(dL\) then you can calculate
the value of \(dx\). For instance, suppose the tower is 52 feet tall and
that there is \(L=173\) feet of tow-rope extending to the skier. The
Pythagorean theorem tells us the skier is \(x=165\) feet from the base
of the tower. The rope is, let us suppose, being pulled in at the top of
the tower at \(dL = 10\) feet per second. How fast is \(x\) changing?
\[dx = \frac{173\ \text{ft}}{165\ \text{ft}} \times 10 \text{ft s}^{-2} = 10.05\ \text{ft s}^{-1}\]

We will return to ``a little bit of this'' when we explore how to add up
little bits to get the whole in Chapter \textbf{?@sec-accum-symbolic}.

\hypertarget{exercises-8}{%
\section{Exercises}\label{exercises-8}}

`\{=html\}

\hypertarget{sec-local-approximations}{%
\chapter{Local approximations
{[}DRAFT{]}}\label{sec-local-approximations}}

NOTE NOTE NOTE

The material in this first section has been moved to the new Block 1
chapter 5. CHANGE THIS CHAPTER TO FOCUS ON THE ANALYSIS OF LOW-ORDER
POLYNOMIALS BY differentiation.

The information that you have about the relationship often takes the
form of a data table. Each row records one trial in which the values of
the inputs have been measured and the corresponding output value
recorded. We will discuss the methods of constructing functions to match
such data in Block 5 of this course.

Another common form for the information about the relationship is about
derivatives. That is, you know something about the derivative of a
relationship even though you don't (yet) have a form for the function
describing the relationship. As an example, think about building a model
of the sustainable speed of a bicycle as a function of the gear selected
and the grade of the road---up or down.

Consider these three questions that any experienced bicyclist can likely
answer:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  On a given grade of road, is there an optimal gear for the highest
  sustained speed? (Have in mind a particular rider, perhaps yourself.)
\item
  Imagine that the grade of the road is described by a positive number
  for uphill and a negative number for downhill: that is, the slope of
  the road. For a positive (uphill) grade and at a fixed gear, will the
  bike's sustained speed be higher or lower as a function of the
  grade?\sidenote{\footnotesize it is much the same for downhill biking, but you have
    to keep in mind that a shallow downhill has a higher numerical slope
    than a steep downhill. That is, the derivative of the hill is near
    zero for a very shallow grade and far from zero (that is, more
    negative) for a steep downhill grade.}
\item
  Assuming you answered ``yes'' to question (1): Does the optimal gear
  choice depend on the grade of the road? (In concrete terms, would you
  choose different gears for an uphill climb than for a level road or a
  downhill stretch?)
\end{enumerate}

Using the methods in this chapter, the answers to those three questions
let you choose an appropriate form for the speed(gear, grade) function.
Then, using methods in Block 5 of this text, you can make a few
measurements for any given rider and construct a model customized to
that rider.

Note that the three questions all have to do with derivatives. An
``optimal gear'' is a gear at which
\(\partial_\text{gear} \text{speed}(\text{gear}, \text{grade}) = 0\).
That you ride slower the higher the numerical value of the slope means
that
\(\partial_\text{grade} \text{speed}(\text{gear}, \text{grade}) < 0\).
And we know that
\(\partial_\text{gear} \text{speed}(\text{gear}, \text{grade})\) depends
on the grade; that is why there is a different optimal gear at each
grade.

\begin{intheworld}
If you're like many people, you find it harder to walk uphill than down,
and find it takes more out of you to walk longer distances than shorter.
Let's build a model of this, using nothing more than your intuition and
the method of low-order polynomial approximations.

Let's call the map distance walked \(d\). (``Map distance'' is the
horizontal change in position, disregarding vertical changes.) The
steepness of the hill will be the ``grade'' \(g\), which is measured as
the horizontal distance covered divided by the vertical climb. If you're
going downhill, the grade is negative.

The key ingredient in the model: we will measure the ``difficulty'' or
``exertion'' to walking as the \textbf{\emph{energy consumed}} during
the walk: \(E(d, g)\).

Some assumptions about walking and energy consumed:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If you don't walk, you consume zero energy walking.
\item
  The energy consumed should be proportional to the length of the walk.
  This is an assumption, and is probably valid, only for walks of short
  to medium distances, as opposed to forced marches over tens of miles.
\end{enumerate}

We will start with the full 2nd-order polynomial in two inputs, and then
seek to eliminate terms that aren't needed.

\[E_{big}(d, g) \equiv a_0 + a_d\, d + a_g\, g + a_{dg}\, d\, g + a_{dd}\,d^2 + a_{gg}\,g^2\]
According to assumption (1), when \(E(d=0, g) = 0\). Of course, if you
are walking zero distance, it does not matter what the grade is; the
energy consumed is still zero.

Consequently, we know that all terms that don't include a \(d\) should
go away. This leaves us with

\[E_{medium}(d, g) \equiv  a_d\, d + a_{dg}\, d\, g + a_{dd}\,d^2 = d \left[\strut a_d + a_{dg}\, g + a_{dd}\,d\right]\]
Assumption (2) says that energy consumed is proportional to \(d\). The
multiplier on \(d\) in \(E_{medium}()\) is
\(\left[\strut a_d + a_{dg}\, g + a_{dd}\,d\right]\) which is itself a
function of \(d\). A proportional relationship implies a multiplier that
does not depend on the quantity itself. This means that \(a_{dd} = 0\).

This leaves us with a very simple model:
\[E(d, g) \equiv \left[\strut a_1 + a_2\, g\right]\, d\] where we have
simplified the labeling on the coefficients since there are only two in
the model.

Perhaps assumption (2) is misplaced and that the energy consumed per
unit distance in a walk increases with the length of the walk. If so, we
would need to return to the question of \(a_{dd}\). This is typical of
the modeling cycle. Trying to be economical with model terms highlights
the question of which terms are so small they can be ignored.

\end{intheworld}

\begin{example}
In selecting cadets for pilot training, two criteria are the cadet's
demonstrated flying aptitude and the leadership potential of the cadet.
Let's assume that the overall merit \(M\) of a candidate is a function
of flying aptitude \(F\) and leadership potential \(L\).

Currently, the merit score is a simple function of the \(F\) and \(L\)
scores: \[M_{current}(F, L) \equiv F + L\]

The general in charge of the training program is not satisfied with the
current merit function. ``I'm getting too many cadets who are great
leaders but poor pilots, and too many pilot hot-shots who are not good
leaders. I would rather have an good pilot who is a good leader than
have a great pilot who is a poor leader or a poor pilot who is a great
leader.'' (You might reasonably agree or disagree with this point of
view, but the general is in charge.)

The general has tasked you to revise the formula to better match her
views about the balance betwen flying ability and leadership potential.

How should you go about constructing \(M_{improved}(F, L)\)?

You recognize that \(F + L\) is a low-order polynomial: just the linear
terms are present without a constant or interaction term or quadratic
terms. Low-order polynomials are a good way to approximate any formula
locally, so you have decided to follow that route.

Quadratic terms are appropriate when a model needs to feature a locally
\textbf{\emph{optimal}} level of the of the inputs. But it will never be
the case that a lower flying score will be more favored than a higher
score, and the same thing for the leadership score. So your model does
not need quadratic terms.

That leaves the interaction term as the way forward. The low-order
polynomial model will be
\[M_{improved}(F, L) \equiv d_0 + F + L + d_{FL} FL\] Should \(d_{FL}\)
be positive or negative?

Imagine a cadet Drew with acceptable and equal F and L scores. Another
cadet, Blake, has scores that are \(F+\epsilon\) and \(L-\epsilon\),
where \(\epsilon\) might be positive or negative. Under the original
formula for merit, Drew and Blake have equal merit. Under the new
criteria, Drew should have a higher merit than Blake. In other words:
\[M_{improved}(F, L) - M_{improved}(F+\epsilon, L-\epsilon) > 0\]

Replace \(M_{improved}(F, L)\) with the low-order polynomial
approximation given earlier.
\[\underbrace{d_0 + F + L + d_{FL} FL}_{M_{improved}(F, L)} - \underbrace{\left[{\large\strut} d_0 + \left[ F + \epsilon\right] + \left[ L - \epsilon\right] + d_{FL} (FL -\epsilon L + \epsilon F -  \epsilon^2)\right]}_{M_{improved}(F+\epsilon, L-\epsilon)} > 0\]
Collecting and cancelling terms in the above gives
\[- d_{FL}(\epsilon(F-L) + \epsilon^2) > 0\] Since \(F\) and \(L\) were
assumed equal, this results in
\[M_{improved}(F, L) - M_{improved}(F+\epsilon, L-\epsilon) = d_{FL}\, \epsilon^2 > 0\]
Thus, \(d_{FL}\) will have to be positive.

\end{example}

END OF MATERIAL COPIED From modeling/05-low-order-polynomials.Rmd

We have focused in this book on a small set of basic modeling functions
and three operations for assembling new functions out of old ones:
\textbf{\emph{linear combination}}, \textbf{\emph{multiplication}}, and
\textbf{\emph{composition}}. All of these have a domain that is the
whole number line, or the positive half of the number line, or perhaps
the whole number line leaving out zero or some other isolated point.
Consider such domains to be \textbf{\emph{global}}.

We also discussed the components of \textbf{\emph{piecewise functions}}.
Each component is a function defined on a limited domain, an interval
\(a \leq x \leq b\). In contrast to the global domains, we will call the
limited domains \textbf{\emph{local}}.

In this chapter, we will explore a simple and surprisingly powerful
method to approximate any function \textbf{\emph{locally}}, that is,
over a small domain.

\begin{why}
Why would you want to approximate a function? Why not just use the
function itself?

It is often the case that we know about or hypothesize about
relationships only from data. We believe there is a definite functional
form for the relationship, but it is unknown and unknowable to us.
Still, we can approximate even an unknown function, matching the
approximation to the data that is the visible manifestation of the
unknown function. Local approximations provide a general-purpose method
for creating functions that can represent a wide range of relationship
patterns, even ones that are not otherwise known to us.

In fields such as physics or engineering, there are often theories that
dictate a particular form of function. For example, Newton's universal
law of gravitation posits an inverse square law for the force of gravity
as a function of distance. Mechanical engineers use power laws to
describe the shape of a beam under load, and communications engineers
(and others) make extensive use of sinusoids. Textbooks in those fields
rightfully emphasize those particular function forms.

The utility of the local approximation method is that you can move
forward even in the absence of a detailed theory. You need only apply
your insight to posit which quantities are related to each other and
then apply the approximation methods to produce a functional form. This
approach is ubiquitous in all fields.

Sometimes, the local approximation \emph{becomes} the theory. This is
seen, for instance, in
\href{https://en.wikipedia.org/wiki/Newton\%27s_law_of_cooling}{Newton's
law of cooling}, in
\href{https://en.wikipedia.org/wiki/Hooke\%27s_law}{Hooke's law}
relating force and extension, or the chemist's
\href{https://en.wikipedia.org/wiki/Law_of_mass_action}{law of mass
action}.

\end{why}

The information that you have about the relationship often takes the
form of a data table. Each row records one trial in which the values of
the inputs have been measured and the corresponding output value
recorded. We will discuss the methods of constructing functions to match
such data in Block 5 of this course.

Another common form for the information about the relationship is about
derivatives. That is, you know something about the derivative of a
relationship even though you don't (yet) have a form for the function
describing the relationship. As an example, think about building a model
of the sustainable speed of a bicycle as a function of the gear selected
and the grade of the road---up or down.

Consider these three questions that any experienced bicyclist can likely
answer:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  On a given grade of road, is there an optimal gear for the highest
  sustained speed? (Have in mind a particular rider, perhaps yourself.)
\item
  Imagine that the grade of the road is described by a positive number
  for uphill and a negative number for downhill: that is, the slope of
  the road. For a positive (uphill) grade and at a fixed gear, will the
  bike's sustained speed be higher or lower as a function of the
  grade?\sidenote{\footnotesize it is much the same for downhill biking, but you have
    to keep in mind that a shallow downhill has a higher numerical slope
    than a steep downhill. That is, the derivative of the hill is near
    zero for a very shallow grade and far from zero (that is, more
    negative) for a steep downhill grade.}
\item
  Assuming you answered ``yes'' to question (1): Does the optimal gear
  choice depend on the grade of the road? (In concrete terms, would you
  choose different gears for an uphill climb than for a level road or a
  downhill stretch?)
\end{enumerate}

Using the methods in this chapter, the answers to those three questions
let you choose an appropriate form for the speed(gear, grade) function.
Then, using methods in Block 5 of this text, you can make a few
measurements for any given rider and construct a model customized to
that rider.

Note that the three questions all have to do with derivatives. An
``optimal gear'' is a gear at which
\(\partial_\text{gear} \text{speed}(\text{gear}, \text{grade}) = 0\).
That you ride slower the higher the numerical value of the slope means
that
\(\partial_\text{grade} \text{speed}(\text{gear}, \text{grade}) < 0\).
And we know that
\(\partial_\text{gear} \text{speed}(\text{gear}, \text{grade})\) depends
on the grade; that is why there is a different optimal gear at each
grade.

\hypertarget{eight-simple-shapes}{%
\section{Eight simple shapes}\label{eight-simple-shapes}}

In many modeling situations with a single input, you can get very close
to a good modeling function \(f(x)\) by selecting one of
\textbf{\emph{eight simple shapes}}, shown in
Figure~\ref{fig-eight-simple-shapes2}.

\begin{figure}

\sidecaption{\label{fig-eight-simple-shapes2}The \textbf{\emph{eight
simple shapes}}, locally, of functions with one input. (See Chapter
Section~\ref{sec-local-approximations}.)}

{\centering \includegraphics[width=0.4\textwidth,height=\textheight]{Differentiation/25-approximation_files/figure-pdf/fig-eight-simple-shapes2-1.pdf}

}

\end{figure}

\begin{figure}

\sidecaption{\label{fig-eight-simple-shapes3}The \textbf{\emph{eight
simple shapes}}, locally, of functions with one input. (See Chapter
Section~\ref{sec-local-approximations}.)}

{\centering \includegraphics[width=0.4\textwidth,height=\textheight]{Differentiation/25-approximation_files/figure-pdf/fig-eight-simple-shapes3-1.pdf}

}

\end{figure}

To choose among these shapes, consider your modeling context:

\begin{itemize}
\tightlist
\item
  is the relationship positive (slopes up) or negative (slopes down)
\item
  is the relationship monotonic or not
\item
  is the relationship concave up, concave down, or neither
\end{itemize}

Some examples, scenarios where the modeler knows about the derivative
and concavity of the relationship being modeled. It is often the case
that your knowledge of the system comes in this form.

\begin{itemize}
\item
  The incidence of an out-of-control epidemic versus time is concave up,
  but shallow-then-steep. As the epidemic is brought under control, the
  decline is steep-then-shallow and concave up. Over the whole course of
  an epidemic, there is a maximum incidence. Experience shows that
  epidemics can have a phase where incidence reaches a local minimum: a
  decline as people practice social distancing followed by an increase
  as people become complacent.
\item
  How many minutes can you run as a function of speed? Concave down and
  shallow-then-steep; you wear out faster if you run at high speed. How
  far can you walk as a function of time? Steep-then-shallow and concave
  down; your pace slows as you get tired.
\item
  How does the stew taste as a function of saltiness. The taste improves
  as the amount of salt increases \ldots{} up to a point. Too much salt
  and the stew is unpalatable.
\item
  The temperature of cooling water or the emission of radioactivity as
  functions of time are concave up and steep-then-shallow.
\item
  How much fuel is consumed by an aircraft as a function of distance?
  For long flights the function is concave up and shallow-then-steep;
  fuel use increases with distance, but the amount of fuel you have to
  carry also increases with distance and heavy aircraft use more fuel
  per mile.
\item
  In micro-economic theory there are \textbf{\emph{production
  functions}} that describe how much of a good is produced at any given
  price, and \textbf{\emph{demand functions}} that describe how much of
  the good will be purchased as a function of price.

  \begin{itemize}
  \tightlist
  \item
    As a rule, production increases with price and demand decreases with
    price. In the short term, production functions tend to be concave
    down, since it is hard to squeeze increased production out of
    existing facilities.\\
  \item
    For demand in the short term, functions will be concave up when
    there is some group of consumers who have no other choice than to
    buy the product. An example is the consumption of gasoline versus
    price: it is hard in the short term to find another way to get to
    work. In the long term, consumption functions can be concave down as
    consumers find alternatives to the high-priced good. For example,
    high prices for gasoline may, in the long term, prompt a switch to
    more efficient cars, hybrids, or electric vehicles. This will push
    demand down steeply.
  \end{itemize}
\end{itemize}

\hypertarget{low-order-polynomials}{%
\section{Low-order polynomials}\label{low-order-polynomials}}

There is a simple, familiar functional form that, by selecting
parameters appropriately, can take on each of the eight simple shapes:
the \textbf{\emph{second-order polynomial}}.
\[g(x) \equiv a + b x + c x^2\] As you know, the graph of \(g(x)\) is a
parabola.

\begin{itemize}
\tightlist
\item
  The parabola opens upward if \(0 < c\). That is the shape of a
  \textbf{\emph{local minimum}}.
\item
  The parabola opens downward if \(c < 0\). That is the shape of a
  \textbf{\emph{local maximum}}
\end{itemize}

Consider what happens if \(c = 0\). The function becomes simply
\(a + bx\), the straight-line function.

\begin{itemize}
\tightlist
\item
  When \(0 < b\) the line slopes upward.
\item
  When \(b < 0\) the line slopes downward.
\end{itemize}

With the appropriate choice of parameters, the form \(a + bx + cx^2\) is
capable of representing four of the \textbf{\emph{eight simple shapes}}.
What about the remaining four? This is where the idea of
\textbf{\emph{local}} becomes important. Those remaining four shapes are
the sides of parabolas, as in Figure~\ref{fig-four-shapes}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/25-approximation_files/figure-pdf/fig-four-shapes-1.pdf}

}

\caption{\label{fig-four-shapes}Four of the \textbf{\emph{eight simple
shapes}} correspond to the sides of the parabola. The labels refer to
the graphs in Figure~\ref{fig-eight-simple-shapes3}.}

\end{figure}

\hypertarget{sec-low-order-two}{%
\section{The low-order polynomial with two
inputs}\label{sec-low-order-two}}

For functions with two inputs, the low-order polynomial approximation
looks like this:

\[g(x, y) \equiv a_0 + a_x x + a_y y + a_{xy} x y + a_{yy} y^2 + a_{xx} x^2\]
In reading this form, note the system being used to name the
polynomial's coefficients. First, we've used \(a\) as the root name of
all the coefficients. Sometimes we might want to compare two or more
low-order polynomials, so it is convenient to be able to use \(a\) for
one, \(b\) for another, and so on.

The subscripts on the coefficients describes exactly which term in the
polynomial involves each coefficient. For instance, the \(a_{yy}\)
coefficient applies to the \(y^2\) term, while \(a_x\) applies to the
\(x\) term.

Each of \(a_0, a_x,\) \(a_y,\) \(a_{xy}, a_{yy}\), and \(a_{xx}\) will,
in the final model, be a constant quantity. Don't be confused by the use
of \(x\) or \(y\) in the name of the coefficients. Each coefficient is a
constant and not a function of the inputs. Often, your prior knowledge
of the system being modeled will tell you something about one or more of
the coefficients, for example, whether it is positive or negative.
Finding a precise value is often based on quantitative data about the
system.

It helps to have different names for the various terms. It is not too
bad to say something like, ``the \(a_{xy}\) term.'' (Pronounciation: ``a
sub x y'' or ``a x y'') But the proper names are: \textbf{\emph{linear
terms}}, \textbf{\emph{quadratic terms}}, and \textbf{\emph{interaction
term}}. And a shout out to \(a_0\), the \textbf{\emph{constant term}}.

\[g(x, y) \equiv a_0 + \underbrace{a_x x + a_y y}_\text{linear terms} \ \ \ + 
\underbrace{a_{xy} x y}_\text{interaction term} +\ \ \  \underbrace{a_{yy} y^2 + a_{xx} x^2}_\text{quadratic terms}\]

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/25-approximation_files/figure-pdf/unnamed-chunk-5-1.pdf}

}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/www/show_poly2_104.png}

}

\caption{\label{fig-saddle2}A saddle}

\end{figure}

\begin{intheworld}
If you're like many people, you find it harder to walk uphill than down,
and find it takes more out of you to walk longer distances than shorter.
Let's build a model of this, using nothing more than your intuition and
the method of low-order polynomial approximations.

Let's call the map distance walked \(d\). (``Map distance'' is the
horizontal change in position, disregarding vertical changes.) The
steepness of the hill will be the ``grade'' \(g\), which is measured as
the horizontal distance covered divided by the vertical climb. If you're
going downhill, the grade is negative.

The key ingredient in the model: we will measure the ``difficulty'' or
``exertion'' to walking as the \textbf{\emph{energy consumed}} during
the walk: \(E(d, g)\).

Some assumptions about walking and energy consumed:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If you don't walk, you consume zero energy walking.
\item
  The energy consumed should be proportional to the length of the walk.
  This is an assumption, and is probably valid, only for walks of short
  to medium distances, as opposed to forced marches over tens of miles.
\end{enumerate}

We will start with the full 2nd-order polynomial in two inputs, and then
seek to eliminate terms that aren't needed.

\[E_{big}(d, g) \equiv a_0 + a_d\, d + a_g\, g + a_{dg}\, d\, g + a_{dd}\,d^2 + a_{gg}\,g^2\]
According to assumption (1), when \(E(d=0, g) = 0\). Of course, if you
are walking zero distance, it does not matter what the grade is; the
energy consumed is still zero.

Consequently, we know that all terms that don't include a \(d\) should
go away. This leaves us with

\[E_{medium}(d, g) \equiv  a_d\, d + a_{dg}\, d\, g + a_{dd}\,d^2 = d \left[\strut a_d + a_{dg}\, g + a_{dd}\,d\right]\]
Assumption (2) says that energy consumed is proportional to \(d\). The
multiplier on \(d\) in \(E_{medium}()\) is
\(\left[\strut a_d + a_{dg}\, g + a_{dd}\,d\right]\) which is itself a
function of \(d\). A proportional relationship implies a multiplier that
does not depend on the quantity itself. This means that \(a_{dd} = 0\).

This leaves us with a very simple model:
\[E(d, g) \equiv \left[\strut a_1 + a_2\, g\right]\, d\] where we have
simplified the labeling on the coefficients since there are only two in
the model.

Perhaps assumption (2) is mis-placed and that the energy consumed per
unit distance in a walk increases with the length of the walk. If so, we
would need to return to the question of \(a_{dd}\). This is typical of
the modeling cycle. Trying to be economical with model terms highlights
the question of which terms are so small they can be ignored.

\end{intheworld}

\begin{example}
In selecting cadets for pilot training, two criteria are the cadet's
demonstrated flying aptitude and the leadership potential of the cadet.
Let's assume that the overall merit \(M\) of a candidate is a function
of flying aptitude \(F\) and leadership potential \(L\).

Currently, the merit score is a simple function of the \(F\) and \(L\)
scores: \[M_{current}(F, L) \equiv F + L\]

The general in charge of the training program is not satisfied with the
current merit function. ``I'm getting too many cadets who are great
leaders but poor pilots, and too many pilot hot-shots who are not good
leaders. I would rather have an good pilot who is a good leader than
have a great pilot who is a poor leader or a poor pilot who is a great
leader.'' (You might reasonably agree or disagree with this point of
view, but the general is in charge.)

The general has tasked you to revise the formula to better match her
views about the balance betwen flying ability and leadership potential.

How should you go about constructing \(M_{improved}(F, L)\)?

You recognize that \(F + L\) is a low-order polynomial: just the linear
terms are present without a constant or interaction term or quadratic
terms. Low-order polynomials are a good way to approximate any formula
locally, so you have decided to follow that route.

Quadratic terms are appropriate when a model needs to feature a locally
\textbf{\emph{optimal}} level of the of the inputs. But it will never be
the case that a lower flying score will be more favored than a higher
score, and the same thing for the leadership score. So your model does
not need quadratic terms.

That leaves the interaction term as the way forward. The low-order
polynomial model will be
\[M_{improved}(F, L) \equiv d_0 + F + L + d_{FL} FL\] Should \(d_{FL}\)
be positive or negative?

Imagine a cadet Drew with acceptable and equal F and L scores. Another
cadet, Blake, has scores that are \(F+\epsilon\) and \(L-\epsilon\),
where \(\epsilon\) might be positive or negative. Under the original
formula for merit, Drew and Blake have equal merit. Under the new
criteria, Drew should have a higher merit than Blake. In other words:
\[M_{improved}(F, L) - M_{improved}(F+\epsilon, L-\epsilon) > 0\]

Replace \(M_{improved}(F, L)\) with the low-order polynomial
approximation given earlier.
\[\underbrace{d_0 + F + L + d_{FL} FL}_{M_{improved}(F, L)} - \underbrace{\left[{\large\strut} d_0 + \left[ F + \epsilon\right] + \left[ L - \epsilon\right] + d_{FL} (FL -\epsilon L + \epsilon F -  \epsilon^2)\right]}_{M_{improved}(F+\epsilon, L-\epsilon)} > 0\]
Collecting and cancelling terms in the above gives
\[- d_{FL}(\epsilon(F-L) + \epsilon^2) > 0\] Since \(F\) and \(L\) were
assumed equal, this results in
\[M_{improved}(F, L) - M_{improved}(F+\epsilon, L-\epsilon) = d_{FL}\, \epsilon^2 > 0\]
Thus, \(d_{FL}\) will have to be positive.

\end{example}

\hypertarget{sec-partial-thought}{%
\section{Thinking partially}\label{sec-partial-thought}}

The expression for a general low-order polynomial in two inputs can be
daunting to think about all at once:
\[g(x, y) \equiv a_0 + a_x x + a_y y + a_{xy} x y + a_{xx} x^2 + a_{yy} y^2\]
As with many complicated settings, a good approach can be to split
things up into simpler pieces. With a low-order polynomial, one such
splitting up involves partial derivatives. There are six potentially
non-zero partial derivatives for a low-order polynomial, of which two
are the same; so only five quantities to consider.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\partial_x g(x,y) = a_x + a_{xy}y + 2 a_{xx} x\)
\item
  \(\partial_y g(x,y) = a_y + a_{xy}x + 2 a_{yy} y\)
\item
  \(\partial_{xy} g(x,y) = \partial_{yx} g(x,y) = a_{xy}\). These are
  the so-called \textbf{\emph{mixed partial derivatives}}. It does not
  matter whether you differentiate by \(x\) first or by \(y\) first. The
  result will always be the same for any smooth function.
\item
  \(\partial_{xx} g(x,y) = 2 a_{xx}\)
\item
  \(\partial_{yy} g(x,y) = 2 a_{yy}\)
\end{enumerate}

The above list states neutral mathematical facts that apply generally to
any low-order polynomial whatsoever.\sidenote{\footnotesize Note that any other
  derivative you construct, for instance \(\partial_{xxy} g(x,y)\) must
  always be zero.} Those facts, however, shape a way of asking questions
of yourself that can help you shape the model of a given phenomenon
based on what you already know about how things work.

To illustrate, consider the situation of modeling the effect of study
\(S\) and of tutoring \(T\) (a.k.a. office hours, extended instruction)
on performance \(P(S,T)\) on an exam. In the spirit of partial
derivatives, we will assume that all other factors (student aptitude,
workload, etc.) are held constant.

To start, pick fiducial values for \(S\) and \(T\) to define the local
domain for the model. Since \(S=0\) and \(T=0\) are easy to envision, we
will use those for the fiducial values.

Next, ask five questions, in this order, about the system being modeled.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Does performance increase with study time? Don't over-think this.
  Remember that the approximation is around a fiducial point. Here, a
  reasonable answer is, ``yes.'' we will take\(\partial_S P(S, T) > 0\)
  to imply that \(a_S > 0\). This is appropriate because close to the
  fiducial point, the other contributors to \(\partial_S P(S, T)\),
  namely \(a_{ST}T + 2 a_{SS} S\) will be vanishingly small.
\item
  Does performance increase with time spent being tutored? Again, don't
  over-think this. Don't worry (yet) that your social life is collapsing
  because of the time spent studying and being tutored, and the
  consequent emotional depression will cause you to fail the exam. We
  are building a model here and the heuristic being used is to consider
  factors in isolation. Since (as we expect you will agree)
  \(\partial_T P(S, T) > 0\), we have that \(a_T > 0\).
\end{enumerate}

Now the questions get a little bit harder and will exercise your
calculus-intuition since you will have to think about \emph{changes} in
the rates of change.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  This question has to do with the mixed partial derivative, which we've
  written variously as \(\partial_{ST} P(S,T)\) or
  \(\partial_{TS} P(S,T)\) and which it might be better to think about
  as \(\partial_S \left[\partial_T P(S,T) \right]\) or
  \(\partial_T \left[\partial S P(S,T)\right]\). Although these are
  mathematically equal, often your intuition will favor one form or the
  other. Recall that we are working on the premise that
  \(\partial_S P(S,T) > 0\), or, in other words, study will help you do
  better on the exam. Now for
  \(\partial_T \left[\partial S P(S,T)\right]\). This is a the matter of
  whether some tutoring will make your study more effective. Let's say
  yes here, since tutoring can help you overcome a misconception that is
  a roadblock to effective study. So \(\partial_{TS} P(S,T) > 0\) which
  implies \(a_{ST} > 0\).
\end{enumerate}

The other way round, \(\partial_S \left[\partial_T P(S,T) \right]\) is a
matter of whether increasing study will enhance the positive effect of
tutoring. We will say yes here again, because a better knowledge of the
material from studying will help you follow what the tutor is saying and
doing. From pure mathematics, we already know that the two forms of
mixed partials are equivalent, but to the human mind they sometimes (and
incorrectly) appear to be different in some subtle, ineffable way.

In some modeling contexts, there might be no clear answer to the
question of \(\partial_{xy}\, g(x,y)\). That is also a useful result,
since it tells us that the \(a_{xy}\) term may not be important to
understanding that system.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  On to the question of \(\partial_{SS} P(S,T)\), that is, whether
  \(a_{SS}\) is positive, negative, or negligible. We know that
  \(a_{SS} S^2\) will be small whenever \(S\) is small, so this is our
  opportunity to think about bigger \(S\). So does the impact of a unit
  of additional study increase or decrease the more you study? One point
  of view is that there is some moment when ``it all comes together''
  and you understand the topic well. But after that epiphany, more study
  might not accomplish as much as before the epiphany. Another bit of
  experience is that ``cramming'' is not an effective study strategy.
  And then there is your social life \ldots{} So let's say,
  provisionally, that there is an argmax to study, beyond which point
  you're not helping yourself. This means that \(a_{SS} < 0\).
\item
  Finally, consider \(\partial_{TT} P(S, T)\). Reasonable people might
  disagree here, which is itself a reason to suspect that \(a_{TT}\) is
  negligible.
\end{enumerate}

Answering these questions does not provide a numerical value for the
coefficients on the low-order polynomial, and says nothing at all about
\(a_0\), since all the questions are about change.

Another step forward in extracting what you know about the system you
are modeling is to construct the polynomial informed by questions 1
through 5. Since you don't know the numerical values for the
coefficients, this might seem impossible. But there is a another
modeler's trick that might help.

Let's imagine that the domain of both \(S\) and \(T\) or the interval
zero to one. This is not to say that we think one hour of study is the
most possible but simply to defer the question of what are appropriate
units for \(S\) and \(T\). Very much in this spirit, for the
coefficients we will use \(+0.5\) when are previous answers indicated
that the coefficient should be greater than zero, \(-0.5\) when the
answers pointed to a negative coefficient, and zero if we don't know.
Using this technique, here is the model, which mainly serves as a basis
for checking whether our previous answers are in line with our broader
intuition before we move on quantitatively.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{P }\OtherTok{\textless{}{-}} \FunctionTok{makeFun}\NormalTok{(}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{S }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{T }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{S}\SpecialCharTok{*}\NormalTok{T }\SpecialCharTok{{-}} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{S}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{\textasciitilde{}}\NormalTok{ S }\SpecialCharTok{\&}\NormalTok{ T)}
\FunctionTok{contour\_plot}\NormalTok{(}\FunctionTok{P}\NormalTok{(S, T) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ S }\SpecialCharTok{\&}\NormalTok{ T, }\FunctionTok{domain}\NormalTok{(}\AttributeTok{S=}\DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{, }\AttributeTok{T=}\DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/25-approximation_files/figure-pdf/fig-zero-one-1.pdf}

}

\caption{\label{fig-zero-one}The result of our intuitive investigation
of the effects of study and tutoring on exam performance. The units are
not yet assigned.}

\end{figure}

Notice that for small values of \(T\), the horizontal spacing between
adjacent contours is large. That is, it takes a lot of study to improve
performance a little. At large values of \(T\) the horizontal spacing
between contours is smaller.

\hypertarget{finding-coefficients-from-data}{%
\section{Finding coefficients from
data}\label{finding-coefficients-from-data}}

Low-order polynomials are often used for constructing functions from
data. In this section, I'll demonstrate briefly how this can be done.
The full theory will be introduced in Block 5 of this text.

The data I'll use for the demonstration is a set of physical
measurements of height, weight, abdominal circumference, etc. on 252
human subjects. These are contained in the \texttt{Body\_fat} data
frame, shown below. ::: \{.cell layout-align=``center''
fig.showtext=`false'\} ::: \{.cell-output-display\}

\begin{tabular}{r|r|r|r|r|r|r|r|r|r|r|r|r|r}
\hline
bodyfat & age & weight & height & neck & chest & abdomen & hip & thigh & knee & ankle & biceps & forearm & wrist\\
\hline
12.3 & 23 & 154.25 & 67.75 & 36.2 & 93.1 & 85.2 & 94.5 & 59.0 & 37.3 & 21.9 & 32.0 & 27.4 & 17.1\\
\hline
6.1 & 22 & 173.25 & 72.25 & 38.5 & 93.6 & 83.0 & 98.7 & 58.7 & 37.3 & 23.4 & 30.5 & 28.9 & 18.2\\
\hline
25.3 & 22 & 154.00 & 66.25 & 34.0 & 95.8 & 87.9 & 99.2 & 59.6 & 38.9 & 24.0 & 28.8 & 25.2 & 16.6\\
\hline
10.4 & 26 & 184.75 & 72.25 & 37.4 & 101.8 & 86.4 & 101.2 & 60.1 & 37.3 & 22.8 & 32.4 & 29.4 & 18.2\\
\hline
28.7 & 24 & 184.25 & 71.25 & 34.4 & 97.3 & 100.0 & 101.9 & 63.2 & 42.2 & 24.0 & 32.2 & 27.7 & 17.7\\
\hline
20.9 & 24 & 210.25 & 74.75 & 39.0 & 104.5 & 94.4 & 107.8 & 66.0 & 42.0 & 25.6 & 35.7 & 30.6 & 18.8\\
\hline
19.2 & 26 & 181.00 & 69.75 & 36.4 & 105.1 & 90.7 & 100.3 & 58.4 & 38.3 & 22.9 & 31.9 & 27.8 & 17.7\\
\hline
12.4 & 25 & 176.00 & 72.50 & 37.8 & 99.6 & 88.5 & 97.1 & 60.0 & 39.4 & 23.2 & 30.5 & 29.0 & 18.8\\
\hline
4.1 & 25 & 191.00 & 74.00 & 38.1 & 100.9 & 82.5 & 99.9 & 62.9 & 38.3 & 23.8 & 35.9 & 31.1 & 18.2\\
\hline
11.7 & 23 & 198.25 & 73.50 & 42.1 & 99.6 & 88.6 & 104.1 & 63.1 & 41.7 & 25.0 & 35.6 & 30.0 & 19.2\\
\hline
\end{tabular}

::: :::

One of the variables records the body-fat percentage, that is, the
fraction of the body's mass that is fat. This is thought to be an
indicator of fitness and health, but it is extremely hard to measure and
involves weighing the person when they are fully submerged in water.
This difficulty motivates the development of a method to approximation
body-fat percentage from other, easier to make measurements such as
height, weight, and so on.

For the purpose of this demonstration, we will build a local polynomial
model of body-fat percentage as a function of height (in inches) and
weight (in pounds).

The polynomial we choose will omit the quadratic terms. It will contain
the constant, linear, and interaction terms only. That is
\[\text{body.fat}(h, w) \equiv c_0 + c_h h + c_w w + c_{hw} h w\] The
process of finding the best coefficients in the polynomial is called
\textbf{\emph{linear regression}}. Without going into the details, we
will use linear regression to build the body-fat model and then display
the model function as a contour plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bodyfat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ height }\SpecialCharTok{+}\NormalTok{ weight }\SpecialCharTok{+}\NormalTok{ height}\SpecialCharTok{*}\NormalTok{weight,}
          \AttributeTok{data =}\NormalTok{ Zcalc}\SpecialCharTok{::}\NormalTok{Body\_fat)}
\NormalTok{body\_fat\_fun }\OtherTok{\textless{}{-}} \FunctionTok{makeFun}\NormalTok{(mod)}
\FunctionTok{contour\_plot}\NormalTok{(}\FunctionTok{body\_fat\_fun}\NormalTok{(height, weight) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ height }\SpecialCharTok{+}\NormalTok{ weight,}
             \FunctionTok{domain}\NormalTok{(}\AttributeTok{weight=}\FunctionTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{250}\NormalTok{), }\AttributeTok{height =} \FunctionTok{c}\NormalTok{(}\DecValTok{60}\NormalTok{, }\DecValTok{80}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{gf\_labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Body fat percentage"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/25-approximation_files/figure-pdf/fig-bodyfat-mod-1.pdf}

}

\caption{\label{fig-bodyfat-mod}A low order polynomial model of body fat
percentage as a function of height (inches) and weight (lbs).}

\end{figure}

That we can build such a model does not mean that it is useful for
anything. In Block 5 of the text we will return to the question of how
well a model constructed from data represents the real-world
relationships that the model attempts to describe.

\hypertarget{exercises-9}{%
\section{Exercises}\label{exercises-9}}

\hypertarget{polynomials}{%
\chapter{Polynomials}\label{polynomials}}

A big part of the high-school algebra curriculum is about polynomials.
In some ways, this is appropriate since polynomials played an outsized
part in the historical development of mathematical theory. Indeed, the
so-called ``Fundamental theorem of algebra'' is about
polynomials.\sidenote{\footnotesize The fundamental theorem says that an order-n
  polynomial has n roots (including multiplicities).}

For modelers, polynomials are a mixed bag. They are very widely used in
modeling. Sometimes this is entirely appropriate, for instance the
low-order polynomials that are the subject of Chapter
Section~\ref{sec-local-approximations}. The problems come when
high-order polynomials are selected for modeling purposes. Building a
reliable model with high-order polynomials requires a deep knowledge of
mathematics, and introduces serious potential pitfalls. Modern
professional modelers learn the alternatives to high-order polynomials,
but newcomers often draw on their experience in high-school and give
unwarranted credence to polynomials. This chapter attempts to guide you
to the ways you are likely to see polynomials in your future work and to
help you avoid them when better alternatives are available.

\hypertarget{sec-polynomial-basics}{%
\section{Basics of polynomials with one
input}\label{sec-polynomial-basics}}

A polynomial is a \textbf{\emph{linear combination}} of a particular
class of functions: power-law functions with non-negative, integer
exponents: 1, 2, 3, \ldots. The individual functions are called
\textbf{\emph{monomials}}, a word that echoes the construction of
chemical polymers out of monomers; for instance, the material
\emph{polyester} is constructed by chaining together a basic chemical
unit called an
\href{https://en.wikipedia.org/wiki/Ester}{\emph{ester}}\emph{.}

In one input, say \(x\), the monomials are \(x^1, x^2, x^3\), and so on.
(There is also \(x^0\), but that is better thought of as the constant
function.) An \textbf{\emph{n-th order}} polynomial has monomials up to
exponent \(n\). For example, the form of a third-order polynomial is
\[a_0 + a_1 x^1 + a_2 x^2 + a_3 x^3\]

The \textbf{\emph{domain}} of polynomials, like the power-law functions
they are assembled from, is the \textbf{\emph{real numbers}}, that is,
the entire number line \(-\infty < x < \infty\). But for the purposes of
understanding the shape of high-order polynomials, it is helpful to
divide the domain into three parts: a \textbf{\emph{wriggly domain}} at
the center and two \textbf{\emph{tail domains}} to the right and left of
the center.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/26-taylor_files/figure-pdf/fig-wriggly-polynomial-1.pdf}

}

\caption{\label{fig-wriggly-polynomial}A \(n\)th-order polynomial can
have up to \(n-1\) critical points that it wriggles among. A 7-th order
polynomial is shown here in which there are six local maxima or minima
alternatingly.}

\end{figure}

Figure~\ref{fig-wriggly-polynomial} shows a 7th order polynomial---that
is, the highest-order term is \(x^7\). In one of the tail domains the
function value heads off to \(\infty\), in the other to \(-\infty\).
This is a necessary feature of all odd-order polynomials: 1, 3, 5, 7,
\ldots{}

In contrast, for even-order polynomials (2, 4, 6, \ldots) the function
value in the two tail domains go in the same direction, either both to
\(\infty\) (Hands up!) or both to \(-\infty\).

In the wriggly domain in Figure~\ref{fig-wriggly-polynomial}, there are
six argmins or argmaxes.

\textbf{An \(n\)th-order polynomial can have up to \(n-1\) extrema.}

Note that the local polynomial approximations in Chapter
Section~\ref{sec-local-approximations} are at most 2nd order and so
there is at most 1 wriggle: a unique argmax. If the approximation does
not include the quadratic terms (\(x^2\) or \(y^2\)) then there is no
argmax for the function.

\hypertarget{multiple-inputs}{%
\section{Multiple inputs?}\label{multiple-inputs}}

High-order polynomials are rarely used with multiple inputs. One reason
is the proliferation of coefficients. For instance, here is the
third-order polynomial in two inputs, \(x\), and \(y\).
\[\underbrace{b_0 + b_x x + b_y y}_\text{first-order terms} + \underbrace{b_{xy} x y + b_{xx} x^2 + b_{yy} y^2}_\text{second-order terms} + \underbrace{b_{xxy} x^2 y + b_{xyy} x y^2 + b_{xxx} x^3 + b_{yyy} y^3}_\text{third-order terms}\]

This has 10 coefficients. With so many coefficients it is hard to
ascribe meaning to any of them individually. And, insofar as some
feature of the function does carry meaning in terms of the modeling
situation, that meaning is spread out and hard to quantify.

\hypertarget{sec-high-order-approx}{%
\section{High-order approximations}\label{sec-high-order-approx}}

The potential attraction of high-order polynomials is that, with their
wriggly interior, they can take on a large number of appearances. This
chameleon-like behavior has historically made them the tool of choice
for understanding the behavior of approximations. That theory has
motivated the use of polynomials for modeling patterns in data, but,
paradoxically, has shown that high-order polynomials should \textbf{not}
be the tool of choice for modeling data.\sidenote{\footnotesize The mathematical
  background needed for those better tools won't be available to us
  until Block 5, when we explore linear algebra.}

Polynomial functions lend themselves well to calculations, since the
output from a polynomial function can be calculated using just the basic
arithmetic functions: addition, subtraction, multiplication, and
division. To illustrate, consider this polynomial:
\[g(x) \equiv x - \frac{1}{6} x^3\] Since the highest-order term is
\(x^3\) this is a third-order polynomial. (As you will see, we picked
these particular coefficients, 0, 1, 0, -1/6, for a reason.) With such
simple coefficients the polynomial is easy to handle by mental
arithmetic. For instance, for \(g(x=1)\) is \(5/6\). Similarly,
\(g(x=1/2) = 23/48\) and \(g(x=2) = 2/3\). A person of today's
generation would use an electronic calculator for more complicated
inputs, but the mathematicians of Newton's time were accomplished human
calculators. It would have been well within their capabilities to
calculate, using paper and pencil, \(g(\pi/4) = 0.7046527\).\sidenote{\footnotesize Unfortunately
  for these human calculators, pencils weren't invented until 1795.
  Prior to the introduction of this advanced, graphite-based computing
  technology, mathematicians had to use quill and ink.}

Our example polynomial, \(g(x) \equiv x - \frac{1}{6}x^3\), graphed in
color in Figure~\ref{fig-small-sine}, does not look exactly like the
sinusoid. If we increased the extent of the graphics domain, the
disagreement would be even more striking, since the sinusoid's output is
always in \(-1 \leq \sin(x) \leq 1\), while the polynomial's tails are
heading off to \(\infty\) and \(-\infty\). But, for a small interval
around \(x=0\), exactly aligns with the sinusoid.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/26-taylor_files/figure-pdf/fig-small-sine-1.pdf}

}

\caption{\label{fig-small-sine}The polynomial \(g(x) \equiv x -x^3 / 6\)
is remarkably similar to \(\sin(x)\) near \(x=0\).}

\end{figure}

It is clear from the graph that the approximation is excellent near
\(x=0\) and gets worse as \(x\) gets larger. The approximation is poor
for \(x \approx \pm 2\). We know enough about polynomials to say that
the approximation will not get better for larger \(x\); the sine
function has a range of \(-1\) to \(1\), while the left and right tails
of the polynomial are heading off to \(\infty\) and \(-\infty\)
respectively.

One way to measure the quality of the approximation is the
\textbf{\emph{error}} \({\cal E}(x)\) which gives, as a function of
\(x\), the difference between the actual sinusoid and the approximation:
\[{\cal E}(x) \equiv |\strut\sin(x) - g(x)|\] The absolute value used in
defining the error reflects our interest in how \textbf{\emph{far}} the
approximation is from the actual function and not so much in whether the
approximation is below or above the actual function.
Figure~\ref{fig-sin-error} shows \({\cal E}(x)\) as a function of \(x\).
Since the error is the same on both sides of \(x=0\), only the positive
\(x\) domain is shown.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/26-taylor_files/figure-pdf/fig-sin-error-1.pdf}

}

\caption{\label{fig-sin-error}The error \({\cal E}(x)\) of \(x - x^3/6\)
as an approximation to \(\sin(x)\). Top panel: linear scale. Bottom
panel: on a log-log scale.}

\end{figure}

Figure~\ref{fig-sin-error} shows that for \(x < 0.3\), the error in the
polynomial approximation to \(\sin(x)\) is in the 5th decimal place. For
instance, \(\sin(0.3) = 0.2955202\) while \(g(0.3) = 0.2955000\).

That the graph of \({\cal E}(x)\) is a straight-line on log-log scales
diagnoses \({\cal E}(x)\) as a power law. That is:
\({\cal E}(x) = A x^p\). As always for power-law functions, we can
estimate the exponent \(p\) from the slope of the graph. It is easy to
see that the slope is positive, so \(p\) must also be positive.

The inevitable consequence of \({\cal E}(x)\) being a power-law function
with positive \(p\) is that \(\lim_{x\rightarrow 0} {\cal E}(x) = 0\).
That is, the polynomial approximation \(x - \frac{1}{6}x^3\) is
\emph{exact} as \(x \rightarrow 0\).

Throughout this book, we've been using straight-line approximations to
functions around an input \(x_0\).
\[g(x) = f(x_0) + \partial_x f(x_0) [x-x_0]\] One way to look at
\(g(x)\) is as a straight-line function. Another way is as a first-order
polynomial. This raises the question of what a second-order polynomial
approximation should be. Rather than the polynomial matching just the
slope of \(f(x)\) at \(x_0\), we can arrange things so that the
second-order polynomial will also match the curvature of the \(f()\).
Since the curvature involves only the first and second derivatives of a
function, the polynomial constructed to match both the first and the
second derivative will necessarily match the slope and curvature of
\(f()\). This can be accomplished by setting the polynomial coefficients
appropriately.

Start with a general, second-order polynomial centered around \(x_0\):
\[g(x) \equiv a_0 + a_1 [x-x_0] + a_2 [x - x_0]^2\] The first- and
second-derivatives, evaluated at \(x=x_0\) are:
\[\partial_x g(x)\left.{\Large\strut}\right|_{x=x_0} = a_1 + 2 a_2 [x  - x_0] \left.{\Large\strut}\right|_{x=x_0} = a_1\]
\[\partial_{xx} g(x)\left.{\Large\strut}\right|_{x=x_0} =  2 a_2\]
Notice the 2 in the above expression. When we want to write the
coefficient \(a_2\) in terms of the second derivative of \(g()\), we
will end up with

\[a_2 = \frac{1}{2} \partial_{xx} g(x)\left.{\Large\strut}\right|_{x=x_0}\]

To make \(g(x)\) approximate \(f(x)\) at \(x=x_0\), we need merely set
\[a_1 = \partial_x f(x)\left.{\Large\strut}\right|_{x=x_0}\] and
\[a_2 = \frac{1}{2} \partial_{xx} f(x) \left.{\Large\strut}\right|_{x=x_0}\]
This logic can also be applied to higher-order polynomials. For
instance, to match the third derivative of \(f(x)\) at \(x_0\), set
\[a_3 = \frac{1}{6} \partial_{xxx} f(x)  \left.{\Large\strut}\right|_{x=x_0}\]
Remarkably, each coefficient in the approximating polynomial involves
only the corresponding order of derivative. \(a_1\) involves only
\(\partial_x f(x) \left.{\Large\strut}\right|_{x=x_0}\); the \(a_2\)
coefficient involves only
\(\partial_{xx} f(x) \left.{\Large\strut}\right|_{x=x_0}\); the \(a_3\)
coefficient involves only
\(\partial_{xx} f(x) \left.{\Large\strut}\right|_{x=x_0}\), and so on.

Now we can explain where the polynomial that started this section,
\(x - \frac{1}{6} x^3\) came from and why those coefficients make the
polynmomial approximate the sinusoid near \(x=0\).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5333}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Order
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\sin(x)\) derivative
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(x - \frac{1}{6}x^3\) derivative
\end{minipage} \\
\midrule
\endhead
0 & \(\sin(x) \left.{\Large\strut}\right|_{x=0} = 0\) &
\(\left( 1 - \frac{1}{6}x^3\right)\left.{\Large\strut}\right|_{x=0} = 0\) \\
1 & \(\cos(x) \left.{\Large\strut}\right|_{x=0} = 1\) &
\(\left(1 - \frac{3}{6} x^2\right) \left.{\Large\strut}\right|_{x=0}= 1\) \\
2 & \(-\sin(x) \left.{\Large\strut}\right|_{x=0} = 0\) &
\(\left(- \frac{6}{6} x\right) \left.{\Large\strut}\right|_{x=0} = 0\) \\
3 & \(-\cos(x) \left.{\Large\strut}\right|_{x=0} = -1\) &
\(- 1\left.{\Large\strut}\right|_{x=0} = -1\) \\
4 & \(\sin(x) \left.{\Large\strut}\right|_{x=0} = 0\) &
\(0\left.{\Large\strut}\right|_{x=0} = 0\) \\
\bottomrule
\end{longtable}

The first four derivatives of \(x - \frac{1}{6} x^3\) exactly match, at
\(x=0\), the first four derivatives of \(\sin(x)\).

The polynomial constructed by matching successive derivatives of a
function \(f(x)\) at some input \(x_0\) is called a \textbf{\emph{Taylor
polynomial}}.

\begin{practice}
Let's construct a 3rd-order Taylor polynomial approximation to
\(f(x) = e^x\) around \(x=0\).

We know it will be a 3rd order polynomial:
\[g_{\exp}(x) \equiv a_0 + a_1 x + a_2 x^2 + a_3 x^3\] The exponential
function is particularly nice for examples because the function value
and all its derivatives are identical: \(e^x\). So \[f(x= 0) = 1\]\\
\[ \partial_x f(x=0) = 1\] \[\partial_{xx} f(x=0) = 1\]
\[\partial_{xxx} f(x=0) = 1\] and so on.

The function value and derivatives of \(g_{\exp}(x)\) at \(x=0\) are:
\[g_{\exp}(x=0) = a_0\] \[\partial_{x}g_{\exp}(x=0) = a_1\]
\[\partial_{xx}g_{\exp}(x=0) = 2 a_2\]

\[\partial_{xxx}g_{\exp}(x=0) = 2\cdot3\cdot a_3 = 6\, a_3\] Matching
these to the exponential evaluated at \(x=0\), we get \[a_0 = 1\]
\[a_1 = 1\] \[a_2 = \frac{1}{2}\]
\[a_3 = \frac{1}{2 \cdot 3} = \frac{1}{6}\]

Result: the 3rd-order Taylor polynomial approximation to the exponential
at \(x=0\) is
\[g_{\exp}(x) = 1 + x + \frac{1}{2} x^2 +  \frac{1}{2\cdot 3} x^3 +\frac{1}{2\cdot 3\cdot 4} x^4\]

Figure~\ref{fig-taylor-exp-3} shows the exponential function \(e^x\) and
its 3th-order Taylor polynomial approximation near \(x=0\):

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/26-taylor_files/figure-pdf/fig-taylor-exp-3-1.pdf}

}

\caption{\label{fig-taylor-exp-3}The 3th-order Taylor polynomial
approximation to \(e^x\) arount \(x=0\)}

\end{figure}

The polynomial is exact at \(x=0\). The \textbf{\emph{error}}
\({\cal E}(x)\) grows with increasing distance from \(x=0\):

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{Differentiation/26-taylor_files/figure-pdf/fig-taylor-exp-5-1.pdf}

}

\caption{\label{fig-taylor-exp-5-1}The error from a 3rd-order Taylor
polynomial approximation to \(e^x\) around \(x=0\) is a power-law
function with exponent \(4\).}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{Differentiation/26-taylor_files/figure-pdf/fig-taylor-exp-5-2.pdf}

}

\caption{\label{fig-taylor-exp-5-2}The error from a 3rd-order Taylor
polynomial approximation to \(e^x\) around \(x=0\) is a power-law
function with exponent \(4\).}

\end{figure}

The plot of \(\log_{10} {\cal E}(x)\) versus \(\log_{10} | x |\) in
\textbf{?@fig-taylor-exp-5} shows that the error grows from zero at
\(x=0\) as a power-law function. Measuring the exponent of the power-law
from the slope of the graph on log-log axes give
\({\cal E}(x) = a |x-x_0|^5\). This is typical of Taylor polynomials:
for a polynomial of degree \(n\), the error will grow as a power-law
with exponent \(n+1\). This means that the higher is \(n\), the faster
\(\lim_{x\rightarrow x_0}{\cal E}(x) \rightarrow 0\). On the other hand,
since \({\cal E}_x\) is a power law function, as \(x\) gets further from
\(x_0\) the error grows as \(\left(x-x_0\right)^{n+1}\).

\end{practice}

\begin{intheworld}
\href{https://en.wikipedia.org/wiki/Brook_Taylor}{Brooke Taylor}
(1685-1731), a near contemporary of Newton, published his work on
approximating polynomials in 1715. Wikipedia reports: ``{[}T{]}he
importance of {[}this{]} remained unrecognized until 1772, when
Joseph-Louis Lagrange realized its usefulness and termed it `the main
{[}theoretical{]} foundation of differential
calculus'.''\href{https://en.wikipedia.org/wiki/Brook_Taylor\#/media/File:Taylor_Brook_Goupy_NPG.jpg}{Source}

\begin{figure}

{\centering \includegraphics[width=0.4\textwidth,height=\textheight]{Differentiation/www/Brook_Taylor.jpg}

}

\caption{\label{fig-brook-taylor}Brook Taylor}

\end{figure}

Due to the importance of Taylor polynomials in the development of
calculus, and their prominence in many calculus textbooks, many students
assume their use extends to constructing models from data. They also
assume that third- and higher-order monomials are a good basis for
modeling data. Both these assumptions are wrong. Least squares is the
proper foundation for working with data.

Taylor's work preceded by about a century the development of techniques
for working with data. One of the pioneers in these new techniques was
Carl Friedrich Gauss (1777-1855), after whom the gaussian function is
named. Gauss's techniques are the foundation of an incredibly important
statistical method that is ubiquitous today: \textbf{\emph{least
squares}}. Least squares provides an entirely different way to find the
coefficients on approximating polynomials (and an infinite variety of
other function forms). The R/mosaic \texttt{fitModel()} function for
polishing parameter estimates is based on least squares. In Block 5, we
will explore least squares and the mathematics underlying the
calculations of least-squares estimates of parameters.

\end{intheworld}

\hypertarget{indeterminate-forms}{%
\section{Indeterminate forms}\label{indeterminate-forms}}

Let's return to an issue that has bedeviled calculus students since
Newton's time. The example we will use is the function
\[\text{sinc}(x)  \equiv \frac{\sin(x)}{x}\]

The sinc() function (pronounced ``sink'') is still important today, in
part because of its role in converting discrete-time measurements (as in
an mp3 recording of sound) into continuous signals.

What is the value of \(\text{sinc}(0)\)? One answer, favored by
arithmetic teachers is that \(\text{sinc}(0)\) is meaningless, because
it involves division by zero.

On the other hand, \(\sin(0) = 0\) as well, so the sinc function
evaluated at zero involves 0/0. This quotient is called an
\textbf{\emph{indeterminant form}}. The logic is this: Suppose we assume
that \(0/0 = b\) for some number \(b\). then \(0 = 0 \times b = 0\). So
any value of \(b\) would do; the value of \(0/0\) is ``indeterminant.''

Still another answer is suggested by plotting out sinc(\(x\)) near
\(x=0\) and reading the value off the graph: sinc(0) = 1.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{slice\_plot}\NormalTok{(}\FunctionTok{sin}\NormalTok{(x) }\SpecialCharTok{/}\NormalTok{ x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\FunctionTok{domain}\NormalTok{(}\AttributeTok{x=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{)), }\AttributeTok{npts=}\DecValTok{500}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/26-taylor_files/figure-pdf/fig-sinc-near-zero-1.pdf}

}

\caption{\label{fig-sinc-near-zero}To judge from this plot,
\(\sin(0) = 1\).}

\end{figure}

The graph of sinc() looks smooth and the shape makes sense. Even if we
zoom in very close to \(x=0\), the graph continues to look smooth. We
call such functions \textbf{\emph{well behaved}}.

Compare the well-behaved sinc() to a very closely related function
(which does not seem to be so important in applied work):
\(\frac{\sin(x)}{x^3}\).

Both \(\sin(x)/x\) and \(\sin(x) / x^3\), evaluated at \(x=0\) involve a
divide by zero. Both are indeterminate forms 0/0 at \(x=0\). But the
graph of \(\sin(x) / x^3\) (see \textbf{?@fig-sinc2}) is not we will
behaved. \(\sin(x) / x^3\) does not have any particular value at
\(x=0\); instead, it has an asymptote.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{slice\_plot}\NormalTok{(}\FunctionTok{sin}\NormalTok{(x) }\SpecialCharTok{/}\NormalTok{ x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\FunctionTok{domain}\NormalTok{(}\AttributeTok{x=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)), }\AttributeTok{npts=}\DecValTok{500}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{gf\_refine}\NormalTok{(}\FunctionTok{scale\_y\_log10}\NormalTok{())}
\FunctionTok{slice\_plot}\NormalTok{(}\FunctionTok{sin}\NormalTok{(x) }\SpecialCharTok{/}\NormalTok{ x}\SpecialCharTok{\^{}}\DecValTok{3} \SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\FunctionTok{domain}\NormalTok{(}\AttributeTok{x=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)), }\AttributeTok{npts=}\DecValTok{500}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{gf\_refine}\NormalTok{(}\FunctionTok{scale\_y\_log10}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/26-taylor_files/figure-pdf/fig-sinc2-1.pdf}

}

\caption{\label{fig-sinc2-1}Zooming in around the division by zero.
Left: The graph of \(\sin(x)/x\) versus \(x\). Right: The graph of
\(\sin(x)/x^2\). The vertical scales on the two graphs are utterly
different.}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{Differentiation/26-taylor_files/figure-pdf/fig-sinc2-2.pdf}

}

\caption{\label{fig-sinc2-2}Zooming in around the division by zero.
Left: The graph of \(\sin(x)/x\) versus \(x\). Right: The graph of
\(\sin(x)/x^2\). The vertical scales on the two graphs are utterly
different.}

\end{figure}

Since both \(\sin(x)/x\left.{\Large\strut}\right|_{x=0}\) and
\(\sin(x)/x^3\left. {\Large\strut}\right|_{x=0}\) involve a
divide-by-zero, the answer to the utterly different behavior of the two
functions is not to be found at zero. Instead, it is to be found
\textbf{\emph{near}} zero. For any non-zero value of \(x\), the
arithmetic to evaluate the functions is straight-forward. Note that
\(\sin(x) / x^3\) starts its mis-behavior away from zero. The slope of
\(\sin(x) / x^3\) is very large near \(x=0\), while the slope of
\(\sin(x) / x\) smoothly approaches zero.

Since we are interested in behavior \textbf{near} \(x=0\), a useful
technique is to approximate the numerator and denominator of both
functions by polynomial approximations.

\begin{itemize}
\tightlist
\item
  \(\sin(x) \approx x - \frac{1}{6} x^3\) near \(x=0\)
\item
  \(x\) is already a polynomial.
\item
  \(x^3\) is already a polynomial.
\end{itemize}

Remember, these approximations are \textbf{exact} as \(x\) goes to zero.
So sufficiently close to zero,

\[\frac{\sin(x)}{x} = \frac{x - \frac{1}{6} x^3}{x} = 1 + \frac{1}{6} x^2\]
Even at \(x=0\), there is nothing indeterminant about \(1 + x^2/6\); it
is simply 1.

Compare this to the polynomial approximation to \(\sin(x) / x^3\):
\[\frac{\sin(x)}{x^3} = \frac{x - \frac{1}{6} x^3}{x^3} = \frac{1}{x^2} - \frac{1}{6}\]

Evaluating this at \(x=0\) involves division by zero. No wonder it is
badly behaved.

The procedure for checking whether a function involving division by zero
behaves well or poorly is described in the first-ever calculus textbook,
published in 1697. The title (in English) is: \emph{The analysis into
the infinitely small for the understanding of curved lines}. In honor of
the author, the Marquis de l'Hospital, the procedure is called
\textbf{\emph{l'Hopital's rule}}.\sidenote{\footnotesize In many French words, the
  sequence ``os'' has been replaced by a single, accented letter,
  \(\hat{\text{o}}\).}

Conventionally, the relationship is written
\[\lim_{x\rightarrow x_0} \frac{u(x)}{v(x)} = \lim_{x\rightarrow x_0} \frac{\partial_x u(x)}{\partial_x v(x)}\]

Let's try this out with our two example functions around \(x=0\):

\[\lim_{x\rightarrow 0} \frac{\sin(x)}{x} = \frac{\lim_{x\rightarrow 0} \cos(x)}{\lim_{x \rightarrow 0} 1} = \frac{1}{1} = 1\]

\[\lim_{x\rightarrow 0} \frac{\sin(x)}{x^3} = \frac{\lim_{x\rightarrow 0} \cos(x)}{\lim_{x \rightarrow 0} 3x^2} = \frac{1}{0} \ \ \text{indeterminate}!\]

\hypertarget{computing-with-indeterminate-forms}{%
\section{Computing with indeterminate
forms}\label{computing-with-indeterminate-forms}}

In the early days of electronic computers, division by zero would cause
a fault in the computer, often signaled by stopping the calculation and
printing an error message to some display. This was inconvenient, since
programmers did not always forsee division-by-zero situations and avoid
them.

As you've seen, modern computers have adopted a convention that
simplifies programming considerably. Instead of stopping the
calculation, the computer just carries on normally, but produces as a
result one of two indeterminant forms: \texttt{Inf} and \texttt{NaN}.

\texttt{Inf} is the output for the simple case of dividing zero into a
non-zero number, for instance:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{17}\SpecialCharTok{/}\DecValTok{0}
\DocumentationTok{\#\# [1] Inf}
\end{Highlighting}
\end{Shaded}

\texttt{NaN}, standing for ``not a number,'' is the output for more
challenging cases: dividing zero into zero, or multiplying \texttt{Inf}
by zero, or dividing \texttt{Inf} by \texttt{Inf}.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{0}\SpecialCharTok{/}\DecValTok{0}
\DocumentationTok{\#\# [1] NaN}
\DecValTok{0} \SpecialCharTok{*} \ConstantTok{Inf}
\DocumentationTok{\#\# [1] NaN}
\ConstantTok{Inf} \SpecialCharTok{/} \ConstantTok{Inf}
\DocumentationTok{\#\# [1] NaN}
\end{Highlighting}
\end{Shaded}

The brilliance of the idea is that any calculation that involves
\texttt{NaN} will return a value of \texttt{NaN}. This might seem to get
us nowhere. But most programs are built out of other programs, usually
written by other people interested in other applications. You can use
those programs (mostly) without worrying about the implications of a
divide by zero. If it is important to respond in some particularly way,
you can always check the result for being \texttt{NaN} in your own
programs. (Much the same is true for \texttt{Inf}, although dividing a
non-\texttt{Inf} number by \texttt{Inf} will return 0.)

Plotting software will often treat \texttt{NaN} values as ``don't plot
this.'' that is why it is possible to make a sensible plot of
\(\sin(x)/x\) even when the plotting domain includes zero.

\hypertarget{exercises-10}{%
\section{Exercises}\label{exercises-10}}

\textless!-- \#\# Fitting polynomials

Taylor polynomials provide a means to approximate continuous and smooth
functions around a center \(x_0\). So long as \(x\) is very close to
\(x_0\), the approximation will be excellent. But Taylor polynomials
aren't a solution to every problem. Consider the piecewise continuous
function, \(ramp(x-1)\), in \textbf{?@fig-ramp-taylor}.

\end{document}
