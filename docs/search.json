[
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html",
    "href": "Preliminaries/05-pattern-book-functions.html",
    "title": "5  Pattern-book functions",
    "section": "",
    "text": "In this Chapter, we introduce the pattern-book functions—a brief list of basic mathematical functions that provide a large majority of the tools for representing the real world as a mathematical object. Think of the items in the pattern-book list as different actors, each of whom is skilled in portraying an archetypal character: hero, outlaw, lover, fool, comic. A play brings together different characters, costumes them, and relates them to one another through dialog or other means.\nA mathematical model is a kind of story; a mathematical modeler is a kind of playwright. She combines mathematical character types to tell a story about relationships. We need only a handful of mathematical functions, the analog of the character actors in drama and comedy to sketch a model. In creating a mathematical model, you clothe the actors to suit the era and location and assemble them in harmony or discord.\nCostume designers use their imagination, often enhanced by referencing published collections of patterns and customizing them to the needs at hand. These references are sometimes called “pattern books.” (See Figure 5.1.)\nSimilarly, we will start with a pattern set of functions that have been collected from generations of experience. To remind us of their role in modeling, we’ll call these pattern-book functions. These pattern-book functions are useful in describing diverse aspects of the real world and have simple calculus-related properties that make them relatively easy to deal with. There are just a handful of pattern-book functions from which untold numbers of useful modeling functions can be constructed. Mastering calculus is in part a matter of becoming familiar with the mathematical connections among the pattern-book functions. (You’ll see these in due time.)\nHere is a list of our pattern-book functions written traditionally and in R:\nThe input name used in the table, \\(x\\), is entirely arbitrary. You can (and will) use the pattern-book functions with other quantities—\\(y\\), \\(z\\), \\(t\\), and so on, even zebra if you like.\nIn addition to the names of the pattern-book functions, you should be able to draw their shapes easily. ?tbl-pattern-book-shapes provides a handy guide.\nThese pattern-book functions are widely applicable. But nobody would confuse the pictures in a pattern book with costumes that are ready for wear. Each pattern must be tailored to fit the actor and customized to fit the theme, setting, and action of the story. We’ll study how this is done starting in Chapter Section 8."
  },
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html#pattern-and-shape",
    "href": "Preliminaries/05-pattern-book-functions.html#pattern-and-shape",
    "title": "5  Pattern-book functions",
    "section": "5.1 Pattern and shape",
    "text": "5.1 Pattern and shape\nThe pattern-book functions are closely related to one another but each has a distinctive shape. The variety of shapes makes the small set of pattern-book functions suited to a wide range of modeling tasks.\n\n\n\n\n\n\n\n\n\nFigure 5.2: The output of the constant function is always 1, regardless of the input.\n\n\n\nThe constant function is so simple that you may be inclined to deny that it is a function at all. The output of the constant function is always numerical 1, regardless of the input. Its graph is therefore a horizontal line.\nYou may wonder why to take the trouble to make a function whose output is always the same. After all, in a formula it shows up simply as the number 1, not looking like a function at all. But formulas are not the only way of representing functions. For instance, in Block ?sec-vectors-linear-combinations we will use a mathematical structure called a “vector” to represent functions where the constant function won’t be just the number 1.\n\n\n\n\n\n\nFigure 5.3: The output of the proportional function is whatever the input value is.\n\n\n\nThe proportional function is also simple. Whatever is the input will become the output. (Another appropriate name for the proportional function is the “identity function” because the output is identical to the input.) The graph of the proportional function is a straight line going through \\((0,0)\\) with slope 1.\nDespite the simplicity of the proportional function, it is heavily used in modeling. In a formula, the proportional function appears as \\(x\\) or \\(y\\) or whatever the input name is to be. This can make it hard to remember that it is indeed a function.\nThe remaining pattern-book functions all have curved shapes.\n\n\n\n\n\n\n\n\n\nFigure 5.4: The “bell-shaped” Gaussian function.\n\n\n\nThe *Gaussian function is shaped like a mountain or, in many descriptions, like the outline of a bell. As the input becomes larger—in either the positive or negative directions—the output gets closer and closer to zero but never touches zero exactly.\nThe Gaussian function shows up so often in modeling that it has another widely used name: the normal function. But “normal” has additional meanings in mathematics, so we will not use that name in this book.\n\n\n\n\n\n\nFigure 5.5: The sigmoid function gives a smooth transition from zero to one.\n\n\n\nThe sigmoid function models smooth transitions. For large negative inputs, the output is (very nearly) zero. For large positive inputs, the output is (very nearly) one. The sigmoid is closely related to the Gaussian. As you progress in calculus, the relationship will become evident.\n\n\n\n\n\n\nFigure 5.6: The output of the exponential function grows faster and faster as the input increases.\n\n\n\nThe exponential function has important applications throughout science, technology, and the economy. For large negative inputs, the value is very close to zero in much the same way as for the Gaussian or sigmoid functions. But the output increases faster and faster as the input gets bigger. Note that the output of the exponential function is never negative for any input.\n\n\n\n\n\n\nFigure 5.7: The logarithm is defined only for inputs greater than zero.\n\n\n\nThe logarithmic function is defined only for positive inputs. As the input increase from just above zero, the output t constantly grows but at a slower and slower rate. It never levels out.\nThe exponential and logarithmic functions are intimate companions. You can see the relationship by taking the graph of the logarithm, and rotating it 90 degrees, then flipping left for right as in Figure 5.8.\n\n\n\n\n\n\nFigure 5.8: The exponential function is the same as the logarithm but—and this is a big but—reversing the roles of the input and output. Visually, that reversal of roles amounts to flipping the graph.\n\n\n\nThe output of the sinusoid function oscillates as the input changes. It’s a periodic function, meaning that the pattern repeats itself identically from left to right in the graph.\nIf you studied trigonometry, you may be used to the sine of an angle in the context of a right triangle. That’s the historical origin of the idea. For our purposes, think of the sinusoid just as a function that takes an input and returns an output. Starting in the early 1800s, oscillatory functions found a huge area of application in representing phenomena such as the diffusion of heat through solid materials and, eventually, radio communications.\nIn trigonometry, the sine has the cosine as a partner. But the two functions \\(\\sin()\\) and \\(\\cos()\\) are so closely connected that we will not often need to make the distinction, calling both of them simply “sinusoids.”\n\n\n\n\n\nFigure 5.9: The output of the sinusoid function oscillates as the input increases."
  },
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html#sec-doubling-time",
    "href": "Preliminaries/05-pattern-book-functions.html#sec-doubling-time",
    "title": "5  Pattern-book functions",
    "section": "5.2 Exponentials & doubling time",
    "text": "5.2 Exponentials & doubling time\nThe exponential function has a characteristic property that makes it extremely important in models of many phenomena: The value doubles in constant time. That phrase, “doubles in constant time” can be obscure, so let’s look at it carefully with reference to a graph of the exponential function.\nFigure 5.10 is a graph of the exponential function, annotated with a set of evenly spaced vertical lines. The intersection of each vertical line with the function has been marked with a dot to make it easier to read off the output of the exponential when the input has any of the values marked by the vertical lines. For example, one of the vertical lines is at \\(t=0\\). From that you can confirm that \\(\\exp(t=0) = 1\\). Looking at the vertical line at \\(t = 1 \\times 0.693\\) you can confirm that \\(\\exp(t=0.693) = 2\\). Similarly \\(\\exp(t=2\\times 0.693) = 4\\) and \\(\\exp(t=3\\times 0.693) = 8\\).\n\n\n\n\n\nFigure 5.10: The exponential function doubles in constant time.\n\n\n\n\nIn other words, the output of the exponential function doubles whenever the input is increased by 0.693. Likewise, decreasing the input by 0.693 cuts the output by half. No other function than the exponential has this property that a constant change in the input (namely, 0.693) leads to a doubling of the output. So, 0.693 is “constant time” that leads to doubling."
  },
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html#sec-power-law-family",
    "href": "Preliminaries/05-pattern-book-functions.html#sec-power-law-family",
    "title": "5  Pattern-book functions",
    "section": "5.3 The power-law family",
    "text": "5.3 The power-law family\nFour of the pattern-book functions—\\(1\\), \\(1/x\\), \\(x\\), \\(x^2\\)— belong to an infinite family called the power-law functions. Some other examples of power-law functions are \\(x^3, x^4, \\ldots\\) as well as \\(x^{1/2}\\) (also written \\(\\sqrt{x}\\)), \\(x^{1.36}\\), and so on. Some of these also have special (albeit less frequently used) names, but all of the power-law functions can be written as \\(x^p\\), where \\(x\\) is the input and \\(p\\) is a number. .\nWithin the power-law family, it is helpful to know and be able to distinguish between several groups:\n\nThe monomials. These are power-law functions such as \\(m_0(x) \\equiv x^0\\), \\(m_1(x) \\equiv x^1\\), \\(m_2(x) \\equiv x^2\\), \\(\\ldots\\), \\(m_p(x) \\equiv x^p\\), \\(\\ldots\\), where \\(p\\) is a whole number (i.e., a non-negative integer). Of course, \\(m_0()\\) is the same as the constant function, since \\(x^0 = 1\\). Likewise, \\(m_1(x)\\) is the same as the identity function since \\(x^1 = x\\). As for the rest, they have just two general shapes: both arms up for even powers of \\(p\\) (like in \\(x^2\\), a parabola); one arm up and the other down for odd powers of \\(p\\) (like in \\(x^3\\), a cubic). Indeed, you can see in Figure 5.11 that \\(x^4\\) has a similar shape to \\(x^2\\) and that \\(x^5\\) is similar in shape to \\(x^3\\). For this reason, high-order monomials are rarely needed in practice.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.11: The monomials \\(x^0\\), \\(x^1\\), \\(x^2\\), \\(x^3\\), \\(x^4\\), \\(x^5\\). The dashed \\(\\color{magenta}{\\text{magenta}}\\) line marks zero output.\n\n\n\n\n\nThe negative powers. These are power-law functions where \\(p<0\\), such as \\(f(x) \\equiv x^{-1}\\), \\(g(x) \\equiv x^{-2}\\), \\(h(x) \\equiv x^{-1.5}\\). For negative powers, the size of the output is inversely proportional to the size of the input. In other words, when the input is large (not close to zero) the output is small, and when the input is small (close to zero), the output is very large. This behavior happens because a negative exponent like \\(x^{-2}\\) can be rewritten as \\(\\frac{1}{x^2}\\); the input is inverted and becomes the denominator, hence the term “inversely proportional”.\n\n\n\n\n\n\n\nFigure 5.12: ?(caption)\n\n\n\n\n\n\n\nFigure 5.13: ?(caption)\n\n\n\n\n\n\n\n\n\nFigure 5.14: ?(caption)\n\n\n\n\n\n\n\nFigure 5.15: ?(caption)\n\n\n\n\n\n\n\n\n\n\nFigure 5.16: Graphs of power-law functions with negative integer exponents. The arrows point to the output being very large when \\(x\\) is near zero.\n\n\n\n\n\nThe non-integer powers, e.g. \\(f(x) = \\sqrt{x}\\), \\(g(x) = x^\\pi\\), and so on. When \\(p\\) is either a fraction or an irrational number (like \\(\\pi\\)), the real-valued power-law function \\(x^p\\) can only take non-negative numbers as input. In other words, the domain of \\(x^p\\) is \\(0\\) to \\(\\infty\\) when \\(p\\) is not an integer. You have likely already encountered this domain restriction when using the power law with \\(p=\\frac{1}{2}\\) since \\(f(x)\\equiv x^{1/2}=\\sqrt{x}\\), and the square root of a negative number is not a real number. You may have heard about the imaginary numbers that allow you to take the square root of a negative number, but for the moment, you only need to understand that when working with real-valued power-law functions with non-integer exponents, the input must be non-negative. (The story is a bit more complicated since, algebraically, rational exponents like \\(1/3\\) or \\(1/5\\) with an odd-valued denominator can be applied to negative numbers. Computer arithmetic, however, does not recognize these exceptions.)\n\n\n\n\n\n\nFigure 5.17: The domain of power-law functions with non-integer power is \\(0 \\leq x < \\infty\\).\n\n\n\n\n\n5.3.1 Computing note\nWhen a function like \\(\\sqrt[3]{x}\\) is written as \\(x^{1/3}\\) make sure to include the exponent in grouping parentheses: x^(1/3). Similarly, later in the book you will encounter power-law functions where the exponent is written as a formula. Particularly common will be power-law functions written \\(x^{n-1}\\) or \\(x^{n+1}\\). In translating this to computer notation, make sure to put the formula within grouping parentheses, for instance x^(n-1) or x^(n+1).\n\n\n5.3.2 Some power-law relationships\nYou have been using power-law functions from early in your math and science education. Some examples:\n\n\nExamples of power-law relationships\n\n\n\n\n\n\n\n\n\n\n\nSetting\nFunction formula\nexponent\n\n\n\n\nCircumference of a circle\n\\(C(r) = 2 \\pi r\\)\n1\n\n\nArea of a circle\n\\(A(r) = \\pi r^2\\)\n2\n\n\nVolume of a sphere\n\\(V(r) = \\frac{4}{3} \\pi r^3\\)\n3\n\n\nDistance traveled by a falling object\n\\(d(t) = \\frac{1}{2} g t^2\\)\n2\n\n\nGas pressure versus volume\n\\(P(V) = \\frac{n R T}{V}\\)\n\\(-1\\)\n\n\n… perhaps less familiar …\n\n\n\n\nDistance traveled by a diffusing gas\n\\(X(t) = D \\sqrt{ \\strut t}\\)\n\\(1/2\\)\n\n\nAnimal lifespan (in the wild) versus body mass1\n\\(L(M) = a M^{0.25}\\)\n0.25\n\n\nBlood flow versus body mass\n\\(F(M) = b M^{0.75}\\)\n0.75\n\n\n\n\n\n\nOne reason why power-law functions are so important in science has to do with the logic of physical quantities such as length, mass, time, area, volume, force, power, and so on. We’ll discuss this at length later in the course and the principles will appear throughout calculus."
  },
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html#domains-of-pattern-book-functions",
    "href": "Preliminaries/05-pattern-book-functions.html#domains-of-pattern-book-functions",
    "title": "5  Pattern-book functions",
    "section": "5.4 Domains of pattern-book functions",
    "text": "5.4 Domains of pattern-book functions\nEach of our basic modeling functions, with two exceptions, has a domain that is the entire number line \\(-\\infty < x < \\infty\\). No matter how big or small is the value of the input, the function has an output. Such functions are particularly nice since we never have to worry about the input going out of bounds.\nThe two exceptions are:\n\nthe logarithm function, which is defined only for \\(0 < x\\).\nsome of the power-law functions: \\(x^p\\).\n\nWhen \\(p\\) is negative, the output of the function is undefined when \\(x=0\\). You can see why with a simple example: \\(g(x) \\equiv x^{-2}\\). Most students had it drilled into them that “division by zero is illegal,” and \\(g(0) = \\frac{1}{0} \\frac{1}{0}\\), a double law breaker.\nWhen \\(p\\) is not an integer, that is \\(p \\neq 1, 2, 3, \\cdots\\) the domain of the power-law function does not include negative inputs. To see why, consider the function \\(h(x) \\equiv x^{1/3}\\).\n\n\nIt can be tedious to make sure that you are on the right side of the law when dealing with functions whose domain is not the whole number line. The designers of the hardware that does computer arithmetic, after several decades of work, found a clever system to make it easier. It’s a standard part of such hardware that whenever a function is handed an input that is not part of that function’s domain, one of two special “numbers” is returned. To illustrate:\n\nsqrt(-3)\n## [1] NaN\n(-2)^0.9999\n## [1] NaN\n1/0\n## [1] Inf\n\nNaN stands for “not a number.” Just about any calculation involving NaN will generate NaN as a result, even those involving multiplication by zero or cancellation by subtraction or division.2 For instance:\n\n0 * NaN\n## [1] NaN\nNaN - NaN\n## [1] NaN\nNaN / NaN\n## [1] NaN\n\nDivision by zero produces Inf, whose name is reminiscent of “infinity.” Inf infiltrates any calculation in which it takes part:\n\n3 * Inf\n## [1] Inf\nsqrt(Inf)\n## [1] Inf\n0 * Inf\n## [1] NaN\nInf + Inf\n## [1] Inf\nInf - Inf\n## [1] NaN\n1/Inf\n## [1] 0\n\nTo see the benefits of the NaN / Inf system let’s plot out the logarithm function over the graphics domain \\(-5 \\leq x \\leq 5\\). Of course, part of that graphics domain, \\(-5 \\leq x \\leq 0\\) is not in the domain of the logarithm function and the computer is entitled to slap us on the wrists. The NaN provides some room for politeness.\nOpen an R console and make the plot:\n\nlibrary(Zcalc)\nslice_plot(log(x) ~ x, interval(x=c(-5,5)))"
  },
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html#symbolic-manipulations",
    "href": "Preliminaries/05-pattern-book-functions.html#symbolic-manipulations",
    "title": "5  Pattern-book functions",
    "section": "5.5 Symbolic manipulations",
    "text": "5.5 Symbolic manipulations\nSeveral of the pattern book functions appear so often in MOSAIC Calculus that it’s worth reviewing how to manipulate them symbolically. As an example, consider the function \\[g(x) \\equiv e^x e^x\\ .\\] This is a perfectly good way of defining \\(g()\\), but it’s helpful to be able to recognize that the following definitions are exactly equivalent \\[f(x) \\equiv e^{x+x}\\\\\nh(x) \\equiv e^{2 x}\\ .\\] The symbolic manipulation we touch on in this chapter involves being able to recall and apply such equivalences. We’ll need only a small set of them, all or most of which are found in a high-school algebra course.\n\nHow come some function names, like \\(\\sin()\\) are written with other parentheses, while others such as \\(e^x\\) have the input name shown?\nThe \\(x\\) in the name \\(e^x\\) is a placeholder. A better, if longer, name would be \\(\\exp()\\), which would signal that we mean the abstract concept of the exponential function, and not that function applied to an input named \\(x\\).\nThe same is true for functions like \\(x\\) or \\(1/t\\) or \\(z^2\\). If absolute consistency of notation were the prime goal, we could have written this book in a style that gives a name to every pattern-book function in the name/parentheses style. Something like this:\n\nreciprocal <- makeFun(1/t ~ t)\none <- makeFun(1 ~ z)\nsquare <- makeFun(x^2 ~ x)\n\nThese would be used in the ordinary way, for instance:\n\nreciprocal(7)\n## [1] 0.1428571\none(123.67)\n## [1] 1\nsquare(19)\n## [1] 361\n\nWriting reciprocal(\\(x\\)) instead of \\(1/x\\) is long-winded, which is perhaps why you never see it. But when you see \\(1/x\\) you should think of it as a function being applied to \\(x\\) and not as a bit of arithmetic.\nBy the way … I used different names for the inputs in these three functions just to remind the reader that, for functions with one input, the name has no significance. You just have to make sure to use the same name on the left- and right-hand sides of the tilde expression.\n\n\nWe’ll be working with exponential and power-law functions in this chapter. It is essential that you recognize that these are utterly different functions.\nAn exponential function, for instance, \\(e^x\\) or \\(2^x\\) or \\(10^x\\) has a constant quantity raised to a power set by the input to the function.\nA power-law function works the reverse way: the input is raised to a constant quantity, as in \\(x^2\\) or \\(x^10\\).\nA mnemonic phrase for exponentials functions is\n\nExponential functions have \\(x\\) in the exponent.\n\nOf course, the exponential function can have inputs with names other than \\(x\\), for instance, \\(f(y) \\equiv 2^y\\), but the name “x” makes for a nice alliteration in the mnemonic.\n\n\n5.5.1 Exponential and logarithm\nWe will Basic symbolic patterns for exponentials are (i) and (ii)\n\n\n\\(\\LARGE\\mathbf{(i)}\\ \\ \\ \\ \\ e^x e^y \\leftrightarrow e^{x+y}\\)\n\\(\\LARGE\\mathbf{(ii)}\\ \\ \\ \\ \\ \\left(e^x\\right)^y \\leftrightarrow e^{x y}\\)\nExponentials with a base other than \\(e\\) can be converted to base \\(e\\).\n\n\n\\(\\LARGE\\mathbf{(iii)}\\ \\ \\ \\ \\ 2^x \\leftrightarrow e^{\\ln(2) x} = e^{0.69315 x}\\)\n\\(\\LARGE\\mathbf{(iv)}\\  \\ \\  \\ 10^x \\leftrightarrow e^{\\ln(10) x} = e^{2.30259 x}\\)$\nFor mental arithmetic, easier use base 2 or 10. The base \\(e = 2.718282\\) is not conducive to such calculations. In Block 2 we’ll discuss why it’s standard to write an exponential function as \\(e^x\\).\nThe logarithms, which we’ll return to in Chapter 15 are the inverse function of the exponential: Rule (v).\n\n\n\\(\\LARGE\\mathbf{(v).a}\\ \\ \\ \\ \\ \\ln(\\exp(x)) = x\\)\n\\(\\LARGE\\mathbf{(v).b}\\ \\ \\ \\ \\ \\exp(\\ln(x)) = x\\)\nOne place that we will encounter rules (ii) and (v) is in Chapter 8 when we look a “doubling times” and “half lives.” There we will deal with expressions such as \\(2^{x/\\tau} = e^{\\ln(2) x/\\tau}\\).\nImportant symbolic patterns for logarithms are Rules (vi) through (vii).\n\n\n\\(\\LARGE\\mathbf{(vi)}\\ \\ \\ \\ \\ \\ln(x\\ \\ \\!y) \\leftrightarrow \\ln(x) + \\ln(y)\\)\n\\(\\mathbf{(vii)}\\ \\ \\ \\ \\! \\ln(x / y) \\leftrightarrow \\ln(x) - \\ln(y)\\)\n\\(\\LARGE\\mathbf{(viii)}\\ \\ \\ \\ \\ \\ln(x^p) \\leftrightarrow p \\ln(x)\\)$\n\nNotice that the symbolic patterns for logarithms involve multiplication, division, and exponentiation, but not addition: \\(\\ln(x + y) \\neq \\ln(x) + \\ln(y)\\).\n\n\n\n5.5.2 Power-law functions\nIn power-law functions, the quantity in the exponent is constant: we’ll call it \\(m\\), \\(n\\), or \\(p\\) in the following examples.\n\n\n\\(\\LARGE\\mathbf{(ix)}\\ \\ \\ \\ x^m x^n \\leftrightarrow x^{m+n}\\)\n\\(\\LARGE\\mathbf{(x)}\\ \\ \\ \\ \\ \\ \\frac{x^m}{x^n} \\leftrightarrow x^{m-n}\\)\n\\(\\Huge\\mathbf{(xi)}\\ \\ \\ \\ \\ \\left(x^n\\right)^p \\leftrightarrow x^{n\\,p}\\)\n\\(\\LARGE\\mathbf{(xii)}\\ \\ \\ \\ \\ \\ x^{-n} \\leftrightarrow \\frac{1}{x^n}\\)\n\\(\\LARGE\\mathbf{(xiii)}\\ \\ \\ \\ \\ \\ \\ x^0 \\leftrightarrow 1\\)\n\n\n5.5.3 Sinusoids\n\\(\\sin(x)\\) is periodic with period \\(2\\pi\\). Zero-crossings of \\(\\sin(x)\\) are at \\(x=..., -2\\pi, -\\pi, 0, \\pi, 2\\pi, ...\\)\nAs we mentioned earlier, we will be calling both the \\(\\sin()\\) and \\(\\cos()\\) function “sinusoids.” They are merely shifted versions of one another: \\[\\cos(x) = \\sin\\left(x + \\frac{\\pi}{2}\\right)\\ .\\]\n\n\n5.5.4 The straight-line function\nYou are probably used to a function that we call the “straight-line function” \\[\\line(x) \\equiv m x + b\\ .\\] The name comes from the shape of a graph of \\(\\line(x)\\) versus \\(x\\), which is a straight line. You are likely used to calling the parameter \\(m\\) the “slope” of the line, and the parameter \\(b\\) the “intercept.” (In general, by “intercept” we will mean the value of the function output when the input is zero. In high-school, this is often called the “y-intercept.”)\nThere are two simple symbolic manipulations that you will be using often in MOSAIC Calculus:\n\nFind the input \\(x=x_0\\) for which the output \\(\\line(x=x_0) = 0\\). This input has many names in mathematics: the “root,” the “\\(x\\)-intercept,” the “zero crossing,” etc. We will call any input value that corresponds to an output of zero to be “a zero of the function.”\n\nFor the straight-line function, the zero is readily found symbolically: \\[x_0 = - b/m\\ .\\]\n\nRe-write the straight-line function in the form \\[\\line(x) = a \\left(x - x_0\\right)\\ .\\]\n\nHere, the slope is designated with \\(a\\). And, of course, \\(x_0\\) is the zero of the function, as you can see by setting \\(x=x_0\\): \\(\\line(x=x_0) = a (x - x_0){\\LARGE\\left.\\right|}_{x = x_0} = 0\\ .\\)\n\nWhy is it called the “logarithm?”\nThe name “logarithm” is anything but descriptive. The name was coined by the inventor, John Napier (1550-1617), to emphasize the original purpose of his invention: to simplify the work of multiplication and exponentiation. The name comes from the Greek words logos, meaning “reasoning” or “reckoning,” and arithmos, meaning “number.” A catchy marketing term for the new invention, at least for those who speak Greek!\nAlthough invented for the practical work of numerical calculation, the logarithm function has become central to mathematical theory as well as modern disciplines such as thermodynamics and information theory. The logarithm is key to the measurement of information and magnitude. As you know, there are units of information used particularly to describe the information storage capacity of computers: bits, bytes, megabytes, gigabytes, and so on. Very much in the way that there are different units for length (cm, meter, kilometer, inch, mile, …), there are different units for information and magnitude. For almost everything that is measured, we speak of the “units” of measurement. For logarithms, instead of “units,” by tradition another word is used: the base of the logarithm. The most common units outside of theoretical mathematics are base-2 (“bit”) and base-10 (“decade”). But the unit that is most convenient in mathematical notation is “base e,” where \\(e = 2.71828182845905...\\). This is genuinely a good choice for the units of the logarithm, but that’s hardly obvious to anyone encountering it for the first time. To make the choice more palatable, it’s marketed as the “base of the natural logarithm.” In this book, we’ll be using this natural logarithm as our official pattern-book logarithm."
  },
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html#exercises",
    "href": "Preliminaries/05-pattern-book-functions.html#exercises",
    "title": "5  Pattern-book functions",
    "section": "5.6 Exercises",
    "text": "5.6 Exercises"
  },
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html#drill",
    "href": "Preliminaries/05-pattern-book-functions.html#drill",
    "title": "5  Pattern-book functions",
    "section": "5.7 Drill",
    "text": "5.7 Drill\n\n\nDrill 1 Which of the pattern book bofunctions has an output value very near zero over almost all of its domain?\n\\(\\ln()\\)\\(\\sin()\\)\\(\\text{dnorm()}\\)\\(e^x\\)\n\n\n\n\nDrill 2 Which of these basic modeling functions has value very near zero over almost half of its domain?\n\\(x^{2}\\)\\(\\ln()\\)\\(\\text{pnorm()}\\)\\(x^{-1}\\)\n\n\n\n\nDrill 3 Which of these basic modeling functions has value very near zero over almost half of its domain?\n\\(x^{2}\\)\\(\\ln()\\)\\(e^x\\)\\(x^{-1}\\)\n\n\n\n\nDrill 4 Which of these pattern-book functions is not a monomial?\n\\(x^2\\)\\(\\sqrt{x}\\)\\(x\\)\n\n\n\n\nDrill 5 Which value(s) is missing from the domain of \\(x^{-1}\\)?\n\nZero\nAll non-positive numbers\nNegative numbers\nNo numbers are missing.\n\n\n\n\n\nDrill 6 Which value(s) is missing from the domain of \\(x^{1/2}\\)?\n\nZero\nAll non-positive numbers\nNegative numbers\nNo numbers are missing.\n\n\n\n\n\nDrill 7 Which value(s) is missing from the domain of \\(\\ln(x)\\)?\n\nZero\nAll non-positive numbers\nNegative numbers\nNo numbers are missing.\n\n\n\n\n\nDrill 8 Which value(s) is missing from the domain of \\(\\sin(x)\\)?\n\nZero\nAll non-positive numbers\nNegative numbers\nNo numbers are missing.\n\n\n\n\n\nDrill 9 Which value(s) is missing from the domain of \\(x^{-1/2}\\)?\n\nZero\nAll non-positive numbers\nNegative numbers\nNo numbers are missing.\n\n\n\n\n\nDrill 10 Which of the following values is in the domain of the function \\(x^{0.429}\\)?\n-2-10All of them\n\n\n\n\nDrill 11 Is zero in the domain of the function \\(x^{-1}\\)?\nYesNo\n\n\n\n\nDrill 12 Which of the following values is in the domain of the function \\(\\sin(x)\\)?\n-2-10All of them\n\n\n\n\nDrill 13 What is the output from the command log(-1) ?\n0An error message.-InfNaN\n\n\n\n\nDrill 14 What is the output from the command log(0) ?\n0-InfNaN\n\n\n\n\nDrill 15 Which of the following values is in the domain of the function \\(\\ln()\\)?\n-101All of them\n\n\n\n\nDrill 16 Which of the following values is in the domain of the function \\(x^2\\)?\n-101All of them\n\n\n\n\nDrill 17 By how much should you increase the input to the exponential function in order to produce a doubling of the output?\n0.50.69311.3862none of these\n\n\n\n\nDrill 18 By how much should you increase the input to the exponential function in order to produce a quadrupling of the output?\n0.50.69311.3862none of these\n\n\n\n\nDrill 19 By how much should you decrease the input to the exponential function in order to cut the output value by a factor of one-half?\n0.50.69311.3862none of these"
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html",
    "href": "Preliminaries/06-describing-functions.html",
    "title": "6  Describing functions",
    "section": "",
    "text": "Knowing and correctly using a handful of phrases about functions with a single input goes a long way in being able to communicate with other people . Often, the words make sense in everyday speech (“steep”, “growing”, “decaying”, “goes up”, “goes down”, “flat”).\nSometimes the words are used in everyday speech but the casual person isn’t sure exactly what they mean. For instance, you will often hear the phrase “growing exponentially.” The graph of the exponential function illustrates exactly this sort of growth: flat for small \\(x\\) and growing steadily steeper and steeper as \\(x\\) increases.\nStill other words are best understood by those who learn calculus. “Concave up,” “concave down”, “approaching 0 asymptotically,” “continuous”, “discontinuous”, “smooth”, “having a minimum at …,” “having a minimum of …”, “approaching \\(\\infty\\) asymptotically,” “having a vertical asymptote.”\nThe next short sections describe seven simple function-shape concepts: slope, concavity, continuity, monotonicity, periodicity, asymptotes, and local extrema. Each of these concepts has the idea of a function at the core, because each one depends on how the function output changes as the input is changed.\nI’ll illustrate these concepts using three pattern-book functions graphed in Figure 6.1."
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#slope",
    "href": "Preliminaries/06-describing-functions.html#slope",
    "title": "6  Describing functions",
    "section": "6.1 Slope",
    "text": "6.1 Slope\nThe slope describes whether the output goes up or down, and to what extent, as the input changes. Typically, the slope is different for different input values. The only function type that has a slope that is the same for all inputs is the straight-line function.\nFigure 6.2 graphs the sinusoid function (black curve). At numerous points in the domain, the function has been overlaid with a straight-line segment that has the same slope as does the function itself. For \\(x\\) near \\(-3\\) the slope is negative; for \\(x\\) near zero the slope is positive, then swings back to negative again for \\(x\\) near \\(3\\).\n\n\n\n\n\nFigure 6.2: Short straight-line segments laid over a graph of the sinusoid. The slope of each line segment is selected to match the local slope of the sinusoid.\n\n\n\n\nWhen we speak of the slope of the sinusoid, or any other function, we mean the local slope as a function of the input. The value of the function doesn’t enter into it, just the slope. Figure 6.3 shows only the slope of the sinusoid, without the sinusoid output at all. Each line segment has a horizontal “run” of \\(0.1\\), so you can measure the slope of each segment—rise over run—as the vertical extent \\(\\Delta y\\) of the segment divided by \\(0.1\\).\n\n\n\n\n\nFigure 6.3: Showing just the slope of the sinusoid as a function of input \\(x\\). Top: representing the slope by the steepness of line segments. Bottom: The numerical value of the slope of each segment.\n\n\n\n\nFor instance, the \\(\\Delta y\\) for the slope segment at \\(x=0\\) is 0.1, so the slope at \\(x=0\\) is \\(\\Delta y/0.1 = 1\\). At \\(x=1\\), \\(\\Delta y \\approx 0.05\\), so the slope is 0.5. The graph colors the segment according to the slope, so large negative slopes are blue, slopes near zero are green, and large positive slopes are yellow.\n\nA more general word than “slope” for describing functions is rate of change. It’s absolutely crucial to distinguish between the change in the output value of a function and the rate of change of that output.\nTo illustrate, suppose we have a function \\(f(x) \\equiv x^2 + 3\\). When we talk about “change” we imagine a situation where we have to different values of the function input, say \\(x_1 = 3\\) and \\(x_2 = 6\\).\nThe “change” in output for these two different inputs is \\(f(x_2) - f(x_1)\\), or in this case \\(39 - 12 = 27\\).\nIn contrast, the “rate of change” is the change in output divided by the change in input, that is:\n\\[\\frac{f(x_2) - f(x_1)}{x_2 - x_1} = \\frac{27}{3} = 9\\ .\\]\nA “rate” in mathematics is a ratio: one measure divided by another. For instance, a heart rate is measured as beats-per-minute. To measure it, count the number of pulse waves in a given interval of time. A typical medical practice is to count for 15 seconds, an interval long enough to get a reliable count but short enough not to unduly prolong the process. If 18 pulse waves were counted in the 15 seconds, the heart rate is 18 beats per 15 seconds, more usually reported as 72 beats-per-minute.\nIn a rate of change, the ratio is the change in output divided by the change of input."
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#concavity",
    "href": "Preliminaries/06-describing-functions.html#concavity",
    "title": "6  Describing functions",
    "section": "6.2 Concavity",
    "text": "6.2 Concavity\nThe slope of a function at a given input tells how fast the function output is increasing or decreasing as the input changes slightly. Concavity is not directly about how the function output changes, but about how the function’s slope changes. For instance, a function might be growing slowly in some region of the domain and then gradually shift to larger growth in an adjacent region. Or, a function might be decaying steeply and then gradually shift to a slower decay. Both of these are instances of positive concavity. The opposite pattern of change in slope is called negative concavity. If the slope doesn’t change at all—only straight-line functions are this way— the concavity is zero.\nConcavity has a very clear appearance in a function graph. If a function is positive concave in a region, the graph looks like a smile or cup. Negative concavity looks like a frown. Zero concavity is a straight line.\nReferring to the three function examples in Figure 6.1, we’ll use the traditional terms concave up and concave down to refer to positive and negative concavity respectively.\n\nThe exponential is concave up everywhere in its domain.\nThe sinusoid alternates back and forth between concave up and concave down.\nThis particular power law \\(x^{-1}\\) is concave up for \\(0 < x\\) and concave down for \\(x < 0\\).\n\nWhen a function switches between positive concavity and negative concavity, as does the sinusoid as well as the gaussian and sigmoid functions, there is an input value where the switch occurs and the function has zero concavity. (Continuous functions that pass from negative to positive or vice versa must always cross zero.) Such in-between points of zero concavity are called inflection points. A function can have zero, one, or many inflection points. For instance, the sinusoid has inflection points at \\(x = \\ldots, -\\pi, 0, \\pi, 2\\pi, \\ldots\\), the cubic power function \\(f(x)\\equiv x^3\\) has one, and the exponential has none.\n\n\n\n\n\nA cubic function which is concave up until a point of inflection and concave down thereafter. [Source: Maj. Austin Davis]\n\n\n\n\n“Inflection point” appears in news stories, so it is important to know what it means in context. The mathematical definition is about the change in the direction of curvature of a graph. In business, however, it generally means something less esoteric, “a time of significant change in a situation” or “a turning point.”1 The business sense effectively means that the function—say profits as a function of time, or unemployment as a function of time—has a non-zero concavity, up or down. It’s about the existence of concavity rather than about the change in the sign of concavity.\nOne of the benefits of learning calculus is to gain a way to think about the previous paragraph that’s systematic, so it’s always easy to know whether you are talking about the slope of a function or the change in slope of a function."
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#sec-continuity-intro",
    "href": "Preliminaries/06-describing-functions.html#sec-continuity-intro",
    "title": "6  Describing functions",
    "section": "6.3 Continuity",
    "text": "6.3 Continuity\nA function is continuous if you can trace out the graph of the function without lifting pencil from the page. A function is continuous on an interval (a,b) if you can trace the function over that whole interval. All of the pattern-book functions are continuous over any interval in their domain except for power-law functions with negative exponents. (This includes the reciprocal since it is a power-law with a negative exponent: \\(1/x = x^{-1}\\).) Those exceptions are not defined at \\(x=0\\).\nTo illustrate, consider the power-law function graphed in Figure 6.1. On any interval (a,b) that does not include 0, the function is continuous. For inputs \\(x < 0\\), the function is negative. For inputs \\(0 < x\\), the function is positive. So, on an interval that includes \\(x=0\\) the function jumps discontinuously from negative to positive."
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#sec-monotonicity",
    "href": "Preliminaries/06-describing-functions.html#sec-monotonicity",
    "title": "6  Describing functions",
    "section": "6.4 Monotonicity",
    "text": "6.4 Monotonicity\nA function is monotonic on a domain when the sign of the slope never changes on that domain. Monotonic functions either steadily increase in value or, alternatively, steadily decrease in value.\nAnother way of thinking about monotonicity is to consider the order of inputs and outputs compared to a number line. If a function is monotonically increasing then it will preserve the order of inputs along the number line when it maps inputs to outputs, whereas a monotonically decreasing function will reverse the order. For instance, if the input \\(x\\) comes before an input \\(y\\) (i.e., \\(x<y\\)), then \\(f(x)<f(y)\\) for monotonically increasing functions (the order is preserved), but \\(f(y)<f(x)\\) for monotonically decreasing functions (the order of outputs is reversed).\nOf the pattern-book functions in Figure 6.1: both the exponential and the logarithm function are monotonic: the exponential grows monotonically as does the logarithm. The sinusoid is not monotonic over any domain longer than half a cycle: the function switchs between positive slope and negative slope in different parts of the cycle."
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#periodicity",
    "href": "Preliminaries/06-describing-functions.html#periodicity",
    "title": "6  Describing functions",
    "section": "6.5 Periodicity",
    "text": "6.5 Periodicity\nA phenomenon is periodic if it repeats a pattern over and over again. The pattern that is repeated is called a cycle; the periodic function as a whole is one cycle placed next to the previous one and so forth. The day-night cycle is an example of a periodic phenomenon, as is the march of the seasons. The period is the duration of one complete cycle; the period of the day-night cycle is 24 hours, the period of the seasonal progression is 1 year.\nReal-world periodic phenomena often show some slight variation from one cycle to the next. Of the pattern-book functions, only the sinusoid is periodic. And it is exactly periodic, repeating the same cycle over and over again. The period—that is, the length of an input interval that contains exactly one cycle—has a value of \\(2\\pi\\) for the pattern-book sinusoid. When used to model a periodic phenomenon, the model function needs to be tailored to match the period of the phenomena.\nThe idea of representing with sinusoids phenomena that are almost but not exactly periodic, for instance a communications signal or a vibration, is fundamental to many areas of physics and engineering."
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#asymptotic-behavior",
    "href": "Preliminaries/06-describing-functions.html#asymptotic-behavior",
    "title": "6  Describing functions",
    "section": "6.6 Asymptotic behavior",
    "text": "6.6 Asymptotic behavior\nAsymptotic refers to two possible situations depending on whether the input or output is being considered:\n\nWhen the input to a function gets bigger and bigger in size, going to \\(\\infty\\) or \\(-\\infty\\). If, as the input changes in this way the output gets closer and closer to a specific value, the function is said to have a horizontal asymptote of that value.\n\nAn example in Figure 6.1 is the exponential function. As \\(x \\rightarrow -\\infty\\), that is, as \\(x\\) goes more and more to the left of the domain, the output tends asymptotically to zero.\n\nWhen the output of a function gets bigger and bigger in size, going to \\(\\infty\\) or \\(-\\infty\\) without the input doing likewise. The visual appearance on a graph is like a sky-rocket: the output changes tremendously fast even though the input changes only a little. The vertical line that the skyrocket approaches is called a vertical asymptote. The power-law function \\(x^{-1}\\) has a vertical asymptote at \\(x=0\\). If you were to consider inputs closer and closer to \\(x=0\\), the outputs would grow larger and larger is magnitude, tending toward \\(\\infty\\) or \\(-\\infty\\).\n\nSeveral of the pattern-book functions have horizontal or vertical asymptotes or both. For instance, the function \\(x^{-1}\\) has a horizontal asymptote of zero for both \\(x \\rightarrow \\infty\\) and \\(x \\rightarrow -\\infty\\).\nThe sinusoid has neither a vertical nor a horizontal asymptote. As input \\(x\\) increases either to \\(-\\infty\\) or \\(\\infty\\), the output of the sinusoid continues to oscillate, never settling down to a single value. And, of course, the output of the sinusoid is everywhere \\(-1 \\leq \\sin(x) \\leq 1\\), so there is no possibility for a vertical asymptote."
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#sec-local-extremes",
    "href": "Preliminaries/06-describing-functions.html#sec-local-extremes",
    "title": "6  Describing functions",
    "section": "6.7 Locally extreme points",
    "text": "6.7 Locally extreme points\nMany continous functions have a region of the input domain where the output is gradually growing, then reaches a peak, then gradually diminishes. This peak is called a local maximum. “Maximum” because the output reaches a peak at a particular input, “local” because in the neighborhood of the peak the function output is smaller than at the peak.\n\n\n\nLikewise, functions can have a local minimum: the bottom of a bowl rather than the top of a peak.\nOf the three pattern-book functions in Figure 6.1, only the sinusoid has a local maximum, and, being periodic, it repeats that every cycle. The sinusoid similarly has a local minimum in every cycle..\nMany modeling applications involve finding an input where the function output is maximized. Such an input is called an argmax. “Argument” is a synonym for “input” in mathematical and computer functions, so “argmax” refers to the input at which the function reaches a maximum output. For instance, businesses attempt to set prices to maximize profit. At too low a price, sales are good but income is low. At too high a price, sales are too low to bring in much income. There’s a sweet spot in the middle.\nOther modeling applications involve finding an argmin, the input for which the output is minimized. For instance, aircraft have a speed at which fuel consumption is at a minimum for the distance travelled. All other things being equal, it’s best to operate at this speed.\nThe process of finding an argmin or an argmax is called optimization. And since maxima and minima are very much the same mathematically, collectively they are called extrema.\nAny function that has an extremum cannot possibly be monotonic, since the growth is positive on one side of the extremum and negative on the other side."
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#exercises",
    "href": "Preliminaries/06-describing-functions.html#exercises",
    "title": "6  Describing functions",
    "section": "6.8 Exercises",
    "text": "6.8 Exercises"
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#drill",
    "href": "Preliminaries/06-describing-functions.html#drill",
    "title": "6  Describing functions",
    "section": "6.9 Drill",
    "text": "6.9 Drill\n\n\nDrill 1 What is the value of \\(f(4)\\) when \\(f(x) \\equiv 2 x + 1\\ ?\\)\n-2-4249\n\n\n\n\nDrill 2 What is the change in the value of \\(f()\\) when the input goes from 2 to 4? Assume \\(f(x) \\equiv 2 x + 1\\)\n-4-2249\n\n\n\n\nDrill 3 What is the rate of change in the value of \\(f()\\) when the input goes from 2 to 4? Assume \\(f(x) \\equiv 2 x + 1\\)\n-2-4249\n\n\n\n\nDrill 4 What is the change in the value of \\(f()\\) when the input goes from 4 to 2? Assume \\(f(x) \\equiv 2 x + 1\\)\n-2-4249\n\n\n\n\nDrill 5 What is the rate of change in the value of \\(f()\\) when the input goes from 4 to 2? Assume \\(f(x) \\equiv 2 x + 1\\)\n-2-4249\n\n\n\n\nDrill 6 What is the rate of change of the function \\(f(x) \\equiv 3 x - 2\\) when the input is 4?\n-20.32310\n\n\n\n\nDrill 7 What is the change in value of the function \\(f(x) \\equiv 3 x - 2\\) as the input goes from 3 to 3.1?\n-20.32310\n\n\n\n\nDrill 8 What is the rate of change in value of the function \\[f(x) \\equiv 3 x - 2\\] as the input goes from 3 to 3.1?\n-20.32310\n\n\n\n\nDrill 9 What is the period of the \\(\\sin()\\) function?\n1\\(2/\\pi\\)\\(\\pi/2\\)\\(\\pi\\)\\(2 \\pi\\)\n\n\n\n\nDrill 10 Which of these words is most appropriate to describe the function \\(g(x) \\equiv 2 - 3 x + 4x^3 ?\\)\nPower-lawDiscontinuousPolynomialPeriodic\n\n\nNEED TO BRING IN THE IMAGES FOR The drill below this\n\n\nDrill 11 Which of these functions is concave up over the domain shown in the graph?\n\n\n\n\n\n\n\n\n\nDrill 12 Which of these functions has a vertical asymptote?\n\n\n\n\n\n\n\n\n\nDrill 13 Which of these functions has a vertical asymptote?\n\n\n\n\n\n\n\n\n\nDrill 14 For this function  Where is the vertical asymptote located?\n\nAt \\(x=0\\)\nAt \\(x=1\\)\nAt 20 as \\(x \\rightarrow \\pm\\infty\\)\n\n\n\n\n\nDrill 15 Does this function have an inflection point? \nyesnocan’t tell\n\n\n\n\nDrill 16 Does this function have an inflection point? \nyesnocan’t tell\n\n\n\n\nDrill 17 For this function  Where is the horizontal asymptote located?\n\nAt \\(x=0\\)\nAt \\(x=1\\)\nAt 20 as \\(x \\rightarrow \\pm\\infty\\)\n\n\n\n\n\nDrill 18 Which of these is a correct description of a horizontal asymptote in the function shown in the graph?\n\nThere is no horizontal asymptote.\nAt 2 as \\(x \\rightarrow -\\infty\\)\nAt 2 as \\(x \\rightarrow \\pm\\infty\\)\nAt 6 as \\(x \\rightarrow -\\infty\\)\n\n\n\n\n\nDrill 19 Which of these is a correct description of a horizontal asymptote in the function shown in the graph?\n\nThere is no horizontal asymptote.\nAt 2 as \\(x \\rightarrow \\infty\\)\nAt 2 as \\(x \\rightarrow \\pm\\infty\\)\nAt 6 as \\(x \\rightarrow \\infty\\)\n\n\n\n\n\nDrill 20 Which of these is the max of the function graphed?\n01234\n\n\n\n\nDrill 21 Which of these is an argmin of the function graphed?\n\\(t = -2.5\\) \\(t = -1.25\\)\\(t = 0\\) \\(t = 1.25\\) \\(t = 2.5\\)\n\n\n\n\nDrill 22 Which of these is an argmax of the function graphed?\n\\(t = -2.5\\) \\(t = -1.25\\)\\(t = 0\\) \\(t = 1.25\\) \\(t = 2.5\\)\n\n\n\n\nDrill 23 According to the graph, which of these values is the argmax of the function?   \n0123\n\n\n\n\nDrill 24 According to the graph, which of these values is the maximum of the function?  \n0123\n\n\n\n\nDrill 25 For the function shown in the graph, which of these properties does not apply?  \nno inflection pointmonotoniccontinuousconcave-down\n\n\n\n\nDrill 26 For the function shown in the graph, which of these properties does not apply?  \nno inflection pointmonotonicdiscontinuousconcave-down\n\n\n\n\nDrill 27 What’s the period of this sinusoid?\n12345\n\n\n\n\nDrill 28 Which of these pattern-book functions has a discontinuity?\n\\(\\sin(x)\\)\\(g(x) \\equiv - x^1\\)\\(g(x) \\equiv x^{-1}\\)\\(\\text{dnorm}(x)\\)"
  },
  {
    "objectID": "Preliminaries/07-data-and-data-graphics.html",
    "href": "Preliminaries/07-data-and-data-graphics.html",
    "title": "7  Data and data graphics",
    "section": "",
    "text": "The decade of the 1660s was hugely significant in the emergence of science, although no one realized it at the time. 1665 was a plague year, the last major outbreak of bubonic plague in England. The University of Cambridge closed to wait out the plague, and Isaac Newton, then a 24-year old Cambridge student returned home to Woolsthorpe where he lived and worked in isolation for two years. Biographer James Gleich wrote: “The plague year was his transfiguration. Solitary and almost incommunicado, he became the world’s paramount mathematician.” During his years of isolation, Newton developed what we now call “calculus” and, associated with that, his theory of Universal Gravitation. He wrote a tract on his work in 1669, but withheld it from publication until 1711.\nPlague was also the motivating factor in another important work, published in 1661, Natural and Political Observations … Made upon the Bills of Mortality by John Graunt (1620-1674). Bills of mortality, listings of the number and causes of deaths in London, had been published intermittently starting in in the plague year of 1532, and then continuously from the onset of plague in 1603. Graunt, a haberdasher by profession, performed what we might now call data science, the extraction of information from data. For instance, Graunt was the first to observe the high rate of child mortality and that the number of deaths attributed to plague was underestimated by about one quarter. Graunt’s work led to his election to the Royal Society, the same august group of scientists of which Isaac Newton was a member (and later president). Graunt is considered the first demographer and epidemiologist.\nGraunt’s publication marks the start of statistics. He built upon a century of work by the city of London, collecting and tabulating data on a quarter of a million deaths. Indeed the word “statistics” stems from “state,” the only entity large enough to collect data on populations and the economy.\nWhereas advances in calculus over the next two centuries could be accomplished by the creativity of individuals, statistics could only develop based on the development of the infrastructure of government data collection, a process that took almost two centuries. But in the last 50 years, extensive data collection has entered non-state domains, such as genetic sequencing, remote sensing of Earth, commercial records, and the vast data warehouses of the social media giants, among others.\nThe ability to draw conclusions from masses of data—shown originally by John Graunt—is now an essential skill throughout science, government, and commerce. As you will see, particularly in Block 5, many of those skills are mathematical, effectively a part of calculus.\nThis chapter introduces some pre-calculus basics of working with data which we’ll use extensively in later Blocks of MOSAIC Calculus."
  },
  {
    "objectID": "Preliminaries/07-data-and-data-graphics.html#data-frames",
    "href": "Preliminaries/07-data-and-data-graphics.html#data-frames",
    "title": "7  Data and data graphics",
    "section": "7.1 Data frames",
    "text": "7.1 Data frames\nMost people encounter data in the form of printed tables, such as the 1665 Bill of Mortality shown below. These tables were developed to be legible to humans and to be compact when printed.\n\n\n\n\n\n\nFigure 7.3: Tables of data from the 1665 Bill of Mortality for London.\n\n\n\nAlthough published more than 350 years ago, it’s still possible for a literate human to sort out what the table is saying. But the volume of data has exploded beyond any possibility of putting it in print. Instead, today’s data are stored and accessed electronically. But the process of accessing such data is very much rooted in the notation of “table,” albeit tables that follow a strict set of principles. We’ll call such tables data frames and it’s important for you to learn a few of the core principles of data organization.\nFirst, recognize that the data shown in Figure 7.3 consist of several different tables. The first table, just under the title, starts with\n\n\nThe first two lines of the main table from the Bill of Mortality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\ \\)\nBuried\nPlague\n\\(\\ \\)\nBuried\nPlague\n\\(\\cdots\\)\n\n\n\n\nSt Albans Woodstreet\n100\n121\nSt Clemens Eastcheap\n18\n20\n\\(\\cdots\\)\n\n\nSt Alhallowes Barking\n514\n330\nSt Diones Back-church\n78\n27\n\\(\\cdots\\)\n\n\n\n\n\n\nThe modern form of this is not spread out across the width of the page. It has a single set of columns rather than sets repeated side by side as in ?tbl-top-lines.\n\n\n\nIn a modern format, all the parishes are listed in one column, so that each row of the table corresponds to a single parish.\n\n\n\n\n\n\nparish\nburied\nplague\n\n\n\n\nSt Albans Woodstreet\n100\n121\n\n\nSt Clemens Eastcheap\n18\n20\n\n\nSt Alhallowes Barking\n514\n330\n\n\nSt Diones Back-church\n78\n27\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\n\n\nEach column of the modern table is called a variable. So there is a variable “buried” that contains the number buried and another variable “plague” containing the number who died of plague.\nEach row of the table is called a case, but often simply row is used. For each table, all the cases are the same kind of thing, for instance, here, a parish.\nAnother table displayed on the sheet is entitled, “The diseases and casualities this year.” In this table, the case is a disease or other cause of death, which we’ve put under the name “condition.”\n\n\n\ncondition\ndeaths\nyear\n\n\n\n\nAbortive and Stilborne\n617\n1665\n\n\nAged\n1545\n1665\n\n\nAgue and Feaver\n5257\n1665\n\n\nAppoplex and Suddenly\n116\n1665\n\n\nBedrid\n10\n1665\n\n\n\nWe’ve added the variable “year” with an eye toward consolidating many years of the Bills into one table.\n\nA modern organization for the data being presented in the Bill of Mortality would go back to the raw records that were collected in the field, stacking them into just two tables: Deaths and Births. The Deaths table might look like ?tbl-deaths.\n\n\nAn imagined re-organization of the data that went into the Bill of Mortality report.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\ndate\nparish\nsex\nage\ncause\n\n\n\n\nPercivell Bullingham\n1665-06-01\nSt Mary le Bow\nM\n29\nplague\n\n\nOwin Swancott\n1665-08-13\nTrinitie\nM\n2\nplague\n\n\nWinifred Romford\n1665-11-09\nSt Swithings\nF\n19\nchildbed\n\n\nElsebeth Cook\n1665-06-29\nSt Ethelborough\nF\n5\nplague\n\n\nHumfray Langham\n1665-06-05\nSt Bennet Fynch\nM\n53\naged\n\n\nAgnes Kirkwood\n1665-11-22\nSt Mary Hill\nF\n21\nague\n\n\nKatherine Murton\n1665-12-01\nSt Alholowes Lesse\nF\n24\nchildbed\n\n\nBainbridge Fletcher\n1665-03-17\nSt Martins\nM\n2\nplague\n\n\nCicely Ouston\n1665-03-08\nSt Austins\nF\n35\nplague\n\n\n\n\n\n\nThe modern conception of data makes a clear distinction between data and the construction of summaries of that data for human consumption. Such summaries might be graphical, or in the form model functions, or even in the form of a set of tables, such as seen in the Bill of Mortality. Learning how to generate such summaries is an essential task in statistics and data science. The automatic construction of model functions (without much human intervention) is a field called machine learning\nIn the Deaths table, which would have 97,306 rows for 1665, the each case is “a person who died.” Such a table could nowadays be re-tabulated into the “diseases and casualities” table, or the breakdown of burials by sex, or the parish-by-parish breakdown. But there are many other possibilities: looking at cause of death by age and season of year, or broken down by sex, etc."
  },
  {
    "objectID": "Preliminaries/07-data-and-data-graphics.html#accessing-data-tables",
    "href": "Preliminaries/07-data-and-data-graphics.html#accessing-data-tables",
    "title": "7  Data and data graphics",
    "section": "7.2 Accessing data tables",
    "text": "7.2 Accessing data tables\nIn a data science course you will learn several ways of storing and accessing tables of data. One of the most important in professional use is a relational database. (“Relation” is another word for “table,” just as functions are about the relationship between inputs and output.)\nData wrangling is a term used to describe working with and summarizing data. This includes merging multiple data frames. In MOSAIC Calculus our uses of data will be focused on constructing functions that show the patterns in data and plotting data to reveal those patterns to the eye.\nFor our work, you can access the data frames we need directly in R by name. For instance, the Engines data frame (?tbl-engine-table) records the characteristics of several internal combustion engines of various sizes:\n\n\nVarious attributes of internal combustion engines, from the very small to the very large.\n\n\n\n\n\n\nEngine\nmass\nBHP\nRPM\nbore\nstroke\n\n\n\n\nWebra Speed 20\n0.25\n0.78\n22000\n16.5\n16\n\n\nEnya 60-4C\n0.61\n0.84\n11800\n24.0\n22\n\n\nHonda 450\n34.00\n43.00\n8500\n70.0\n58\n\n\nJacobs R-775\n229.00\n225.00\n2000\n133.0\n127\n\n\nDaimler-Benz 609\n1400.00\n2450.00\n2800\n165.0\n180\n\n\nDaimler-Benz 613\n1960.00\n3120.00\n2700\n162.0\n180\n\n\nNordberg\n5260.00\n3000.00\n400\n356.0\n407\n\n\nCooper-Bessemer V-250\n13500.00\n7250.00\n330\n457.0\n508"
  },
  {
    "objectID": "Preliminaries/07-data-and-data-graphics.html#variable-names",
    "href": "Preliminaries/07-data-and-data-graphics.html#variable-names",
    "title": "7  Data and data graphics",
    "section": "7.3 Variable names",
    "text": "7.3 Variable names\nThe fundamental questions to ask first about any data frame are:\n\nWhat constitutes a row?\nWhat are the variables and what do they stand for?\n\nThe answers to these questions, for the data frames we will be using, are available via R documentation. To bring up the documentation for Engines, for instance, give the command:\n?Engines\nWhen working with data, it’s common to forget for a moment what are the variables, how they are spelled, and what sort of values each variable takes on. Two useful commands for reminding yourself are (illustrated here with Engines):\n\nnames(Engines) # the names of the variables\n## [1] \"Engine\"       \"mass\"         \"ncylinder\"    \"strokes\"      \"displacement\"\n## [6] \"bore\"         \"stroke\"       \"BHP\"          \"RPM\"\nhead(Engines) # the first several rows\n## # A tibble: 6 × 9\n##   Engine            mass ncylinder strokes displacement  bore stroke   BHP   RPM\n##   <chr>            <dbl>     <dbl>   <dbl>        <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1 Webra Speedy     0.135         1       2          1.8  13.5   12.5  0.45 22000\n## 2 Motori Cipolla   0.15          1       2          2.5  15     14    1    26000\n## 3 Webra Speed 20   0.25          1       2          3.4  16.5   16    0.78 22000\n## 4 Webra 40         0.27          1       2          6.5  21     19    0.96 15500\n## 5 Webra 61 Blackh… 0.43          1       2         10    24     22    1.55 14000\n## 6 Webra 6WR        0.49          1       2         10    24     22    2.76 19000\nnrow(Engines) # how many rows\n## [1] 39\n\nIn RStudio, the command View(Engines) is useful for showing a complete table of data."
  },
  {
    "objectID": "Preliminaries/07-data-and-data-graphics.html#plotting-data",
    "href": "Preliminaries/07-data-and-data-graphics.html#plotting-data",
    "title": "7  Data and data graphics",
    "section": "7.4 Plotting data",
    "text": "7.4 Plotting data\nWe will use just one graphical format for displaying data: the point plot. In a point plot, also known as a “scatterplot,” two variables are displayed, one on each graphical axis. Each case is presented as a dot, whose horizontal and vertical coordinates are the values of the variables for that case. For instance:\n\n\n\n\n\nFigure 7.4: A point plot showing the relationship between engine stroke and bore. Each individual point is one row of the data frame SHOWN IN GIVE TABLE A NAME\n\n\n\n\nThe data plotted here show a relationship between the stroke length of a piston and the diameter of the cylinder in which the piston moves. This relationships, however, is not being presented in the form of a function, that is, a single stroke value for each value of the bore diameter.\nFor many modeling purposes, it’s important to be able to represent a relationship as a function. At one level, this is straightforward: draw a smooth curve through the data and use that curve for the function.\nLater in MOSAIC Calculus, we’ll discuss ways to construct functions that are a good match to data using the pattern-book functions. Here, our concern is graphing such functions on top of a point plot. So, without explanation (until later chapters), we’ll construct a power-law function, called, stroke(bore), that might be a good match to the data. The we’ll add a second layer to the point-plot graphic: a slice-plot of the function we’ve constructed.\n\nstroke <- fitModel(stroke ~ A*bore^b, data = Engines)\ngf_point(stroke ~ bore, data = Engines) %>%\n  slice_plot(stroke(bore) ~ bore, color=\"blue\")\n\n\n\n\nFigure 7.5: A graphic composed of two layers: 1) a point plot; 2) a slice plot of a function fitted to the data in (1).\n\n\n\n\nThe second layer is made with an ordinary slice_plot() command. To place it on top of the point plot we connect the two commands with a bit of punctuation called a “pipe”: %>%.\nThe pipe punctuation can never go at the start of a line. Usually, we’ll use the pipe at the very end of a line; think of the pipe as connecting one line to the next.\nslice_plot() is a bit clever when it is used after a previous graphics command. Usually, you need to specify the interval of the domain over which you want to display the function, as with\n\n\n\n\n\n\n\n\n\nYou can do that also when slice_plot() is the second layer in a graphics command. But slice_plot() can also infer the interval of the domain from previous layers."
  },
  {
    "objectID": "Preliminaries/07-data-and-data-graphics.html#functions-as-data",
    "href": "Preliminaries/07-data-and-data-graphics.html#functions-as-data",
    "title": "7  Data and data graphics",
    "section": "7.5 Functions as data",
    "text": "7.5 Functions as data\nIn the previous chapters, we’ve used formulas to define functions. The link between functions and formulas is important, but not at all essential to the idea of functions.\nArguably more important in practice to the representation of functions are tables and algorithms. The computations behind the calculation of the output of functions such as \\(\\sin()\\) or \\(e^x\\) or other foundational functions that we’ll introduce in [Chapter -Section 5) relies on computer software that loops and iterates and which is invisible to almost everybody who uses it. Before the advent of modern computing, functions were presented as printed tables. For instance, the logarithm function, invented about 1600, relied almost complete on printed tables, such as the one shown in Figure 7.6.\n\n\n\n\n\nFigure 7.6: Part of the first table of logarithms, published by Henry Briggs in 1624.\n\n\n\n\nIn Section 5 we introduced a small set of pattern-book functions. Each of the functions is indeed a pattern that could be written down once and for all in tabular form. Generating such tables originally required the work of human “computers” who undertook extensive and elaborate arithmetical calculations by hand. What’s considered the first programmable engine, a mechanical device designed by Charles Babbage (1791-1871) and programmed by Ada Lovelace (1815-1852), was conceived for the specific purpose of generating printed tables of functions.\nIt’s helpful to think of functions, generally, as a sort of data storage and retrieval device that uses the input value to locate the corresponding output and return that output to the user. Any device capable of this, such as a table or graph with a human interpreter, is a suitable way of implementing a function.\nTo reinforce this idea, we ask you to imagine a long corridor with a sequence of offices, each identified by a room number. The input to the function is the room number. To evaluate the function for that input, you knock on the appropriate door and, in response, you’ll receive a piece of paper with a number to take away with you. That number is the output of the function.\nThis will sound at first too simple to be true, but … In a mathematical function each office gives out the same number every time someone knocks on the door. Obviously, being a worker in such an office is highly tedious and requires no special skill. Every time someone knocks on the worker’s door, he or she writes down the same number on a piece of paper and hands it to the person knocking. What that person will do with the number is of absolutely no concern to the office worker.\nThe utility of such functions depends on the artistry and insight of the person who creates them: the modeler. An important point of this course is to teach you some of that artistry. Hopefully you will learn through that artistry to translate your insight to the creation of functions that are useful in your own work. But even if you just use functions created by others, knowing how functions are built will be helpful in using them properly.\nIn the sort of function just described, all the offices were along a single corridor. Such functions are said to have one input, or, equivalently, to be “functions of one variable.” To operate the function, you just need one number: the address of the office from which you’ll collect the output.\nMany functions have more than one input: two, three, four, … tens, hundreds, thousands, millions, …. In this course, we’ll work mainly with functions of two inputs, but the skills you develop will be applicable to functions of more than two inputs.\nWhat does a function of two inputs look like in our office analogy? Imagine that the office building has many parallel corridors, each with a numeric ID. To evaluate the function, you need two numeric inputs: the number of the corridor and the number of the door along that corridor. With those two numbers in hand, you locate the appropriate door, knock on it and receive the output number in return.\nThree inputs? Think of a building with many floors, each floor having many parallel corridors, each corridor having many offices in sequence. Now you need three numbers to identify a particular office: floor, corridor, and door.\nFour inputs? A street with many three-input functions along it. Five inputs? A city with many parallel four-input streets. And on and on.\nApplying inputs to a function in order to receive an output is only a small part of most calculations. Calculations are usually organized as algorithms, which is just to say that algorithms are descriptions of a calculation. The calculation itself is … a function!\nHow does the calculation work? Think of it as a business. People come to your business with one or more inputs. You take the inputs and, following a carefully designed protocol, hand them out to your staff, perhaps duplicating some or doing some simple arithmetic with them to create a new number. Thus equipped with the relevant numbers, each member of staff goes off to evaluate a particular function with those numbers. (That is, the staff member goes to the appropriate street, building, floor, corridor, and door, returning with the number provided at that office.) The staff re-assembles at your roadside stand, you do some sorting out of the numbers they have returned with, again following a strict protocol. Perhaps you combine the new numbers with the ones you were originally given as inputs. In any event, you send your staff out with their new instructions—each person’s instructions consist simply of a set of inputs which they head out to evaluate and return to you. At some point, perhaps after many such cycles, perhaps after just one, you are able to combine the numbers that you’ve assembled into a single result: a number that you return to the person who came to your business in the first place.\nA calculation might involve just one function evaluation, or involve a chain of them that sends workers buzzing around the city and visiting other businesses that in turn activate their own staff who add to the urban tumult.\n\nThe reader familiar with floors and corridors and office doors may note that the addresses are discrete. That is, office 321 has offices 320 and 322 as neighbors. Calculus is about functions with a continuous domain. But it’s easy to create a continuous function out of a discrete table by adding on a small, standard calculation.\nIt works like this: for an input of, say, 321.487… the messenger goes to both office 321 and 322 and collects their respective outputs. Let’s imagine that they are -14.3 and 12.5 respectively. All that’s needed is a small calculation, which in this case will look like \\[-14.3 \\times (1 - 0.487...)   + 12.5 \\times 0.487...\\] This is called linear interpolation and lets us construct continuous functions out of discrete data. There are other types of interpolation have have desirable properties, like “smoothness,” which we’ll learn about later.\nIt’s very common in science to work with continuous domains by breaking them up into discrete pieces. As you’ll see, an important strategy in calculus to to make such discrete pieces very close together, so that they resemble a continuum.\n\nA table like REFERENCE TO THE TABLE PREVIOUS describes the general relationships between engine attributes. For instance, we might want to understand the relationship (if any) between RPM and engine mass, or relate the diameter (that is, “bore”) and depth (that is, “stroke”) of the cylinders to the power generated by the engine. Any single entry in the table doesn’t tell us about such general relationships; we need to consider the rows and columns as a whole.\nIf you examined the relationship between engine power (BHP) and bore, stroke, and RPM, you will find that (as a rule) the larger the bore and stroke, the more powerful the engine. That’s a qualitative description of the relationship. Most educated people are able to understand such a qualitative description. Even if they don’t know exactly what “power” means, they have some rough conception of it.\nOften, we’re interested in having a quantitative description of a relationship such as the one (bore, stroke) \\(\\rightarrow\\) power. Remarkably, many otherwise well-educated people are uncomfortable with the idea of using quantitative descriptions of a relationship: what sort of language the description should be written with; how to perform the calculations to use the description; how to translate between data (such as in the table) and a quantitative description; how to translate the quantitative description to address a particular question or make a decision.\n\nIn today’s world, software is the means by which expert knowledge and capability is communicated and applied. Before modern computers were available, the expertise was committed to print in the form of tables. Figure 7.7 shows part of the table from an 1899 A Short Table of Integrals.1 Incredibly, such tables were a standard feature of statistics textbooks up through 2010.\n\n\n\n\n\nFigure 7.7: Before the computer software era, some functions could only be presented using printed tables. This table supports calculations with a gaussian-like function for inputs from 0 to 1.\n\n\n\n\nIn addition to software being more compact and easier to use than printed tables, the interface to numerical integrals can be presented in the same format as any other mathematical function. That’s enabled us to include \\(\\pnorm()\\) among the pattern book functions."
  },
  {
    "objectID": "Preliminaries/07-data-and-data-graphics.html#exercises",
    "href": "Preliminaries/07-data-and-data-graphics.html#exercises",
    "title": "7  Data and data graphics",
    "section": "7.6 Exercises",
    "text": "7.6 Exercises"
  },
  {
    "objectID": "Preliminaries/07-data-and-data-graphics.html#drill",
    "href": "Preliminaries/07-data-and-data-graphics.html#drill",
    "title": "7  Data and data graphics",
    "section": "7.7 Drill",
    "text": "7.7 Drill\n\n\nDrill A In the book, what is meant by the word “variable”?\n\nIt’s the same as output.\nIt’s the same as input.\nA column in a data frame."
  },
  {
    "objectID": "modeling-part.html",
    "href": "modeling-part.html",
    "title": "Modeling",
    "section": "",
    "text": "This is where I’ll explain what the block is about and the overall goals."
  },
  {
    "objectID": "Modeling/01-parameters.html",
    "href": "Modeling/01-parameters.html",
    "title": "8  Parameters",
    "section": "",
    "text": "The variety of shapes of the nine pattern-book functions means that, often, one or another will be suitable for the modeling situation in hand. Marginal: Combining the functions to create a greater diversity of shapes is the subject of Chapter COMBINING\nEven if the shape of the function used is appropriate, the pattern still needs to be “adjusted” so that the units of output and input are well matched to the phenomenon being modeled. Let’s consider data from the outbreak of COVID-19 as an example. Figure 8.1 shows, day-by-day, the number of officially confirmed COVID-19 cases as the in the US in March 2020.\nDuring the outbreak, case numbers increased with time. As time went on, the rate of case-number increase itself grew faster and faster. This is the same pattern provided by the exponential function.\nAlongside the case-number data Figure 8.1 shows the function \\(\\text{cases}(t) \\equiv e^t\\) plotted as a \\(\\color{magenta}{\\text{magenta}}\\) curve.\nThere’s an obvious mismatch between the data and the function \\(e^t\\). Does this mean the COVID pattern is not exponential?\nThis chapter will introduce the way that modelers stretch and shift the individual patter-book functions so that they can be used in models of real-world situations such as the outbreak of COVID-19."
  },
  {
    "objectID": "Modeling/01-parameters.html#matching-numbers-to-quantities",
    "href": "Modeling/01-parameters.html#matching-numbers-to-quantities",
    "title": "8  Parameters",
    "section": "8.1 Matching numbers to quantities",
    "text": "8.1 Matching numbers to quantities\nThe coordinate axes in (figure19?) represent quantities. On the horizontal axis is time, measured in days. The vertical axis is denominated in “10000 cases,” meaning that the numbers on the vertical scale should be multiplied by 10000 to get the number of cases.\nThe exponential function takes as input a pure number and produces an output that is also a pure number. This is true for all the pattern-book functions. Since the graph axes don’t show pure numbers, it’s no surprise then that the pattern-book exponential function doesn’t align with the COVID case data.\nRecall that pure numbers, like 17.32, do not have units. Quantities, on the other hand, usually do have units, as in 17.3 days or 34 meters.\nIf we want the input to the model function \\(\\text{cases}(t)\\) to be denominated in days, we’ll have to convert \\(t\\) to a pure pure number (e.g. 10, not “10 days”) before the quantity is handed off as the argument to \\(\\exp()\\). We do this by introducing a parameter.\nIn every case, these parameters are arranged to translate a with-units quantity into a pure number suitable as an input to the pattern-book function. Similarly, parameters will translate the pure-number output from the pattern-book function into a quantity with units.\nThe standard parameterization for the exponential function is \\(e^{kt}\\). The parameter \\(k\\) will be a quantity with units of “per-day.” Suppose we set \\(k=0.2\\) per day. Then \\(k\\, t{\\LARGE\\left.\\right|}_{t=10 days} = 2\\). This “2” is a pure number because the units on the 0.2 (“per day”) and on the 10 (days) cancel out: \\[0.2\\, \\text{day}^{-1} \\cdot 10\\, \\text{days} = 2\\ .\\] The use of a parameter like \\(k\\) does more than handle the formality of converting input quantities into pure numbers. Having a choice for \\(k\\) allows us to stretch or compress the function to align with the data. Figure 8.2 plots the modeling version of the exponential function to the COVID-case data:\n\n\n\n\n\nFigure 8.2: Using the function form \\(A e^{kt}\\), with parameters \\(k=0.19\\) per day and \\(A = 0.0573\\) cases (in 10000s), matches the COVID-case data well."
  },
  {
    "objectID": "Modeling/01-parameters.html#parallel-scales",
    "href": "Modeling/01-parameters.html#parallel-scales",
    "title": "8  Parameters",
    "section": "8.2 Parallel scales",
    "text": "8.2 Parallel scales\n\n\n\n\n\n\n\n\n\nAt the heart of how we’re going to use the pattern-book functions to model the relationship between quantities is the idea of conversion between one scale and another. Consider these everyday objects: a thermometer and a ruler.\n\n\n\n\n\n\n\n\n\nEach object presents a read-out of what’s being measured—temperature or length—on two different scales. At the same time, the objects provide a way to convert one scale to another.\nA function gives the output for any given input. We represent the input value as a position on a number line—which we call an “axis”—and the output as a position on another output line, almost always drawn perpendicular to one another. But the two number lines can just as well be parallel to one another. To evaluate the function, find the input value on the input scale and read off the corresponding output.\nWe can translate the correspondance between one scale and the other into the form of a straight-line function. For instance, if we know the temperature in Fahrenheit (\\(^\\circ\\)F) and want to convert it to Celsius (\\(^\\circ C\\)) we have the following function: \\[C(F) \\equiv {\\small\\frac{5}{9}}(F-32)\\ .\\] Similarly, converting inches to centimeters can be accomplished with \\[\\text{cm(inches)} \\equiv 2.54 \\, (\\text{inches}-0)\\ .\\] Both of these scale conversion functions have the form of the straight-line function, which can be written as \\[f(x) \\equiv a x + b\\ \\ \\ \\text{or, equivalently as}\\ \\ \\ \\ f(x) \\equiv a(x-x_0)\\ ,\\] where \\(a\\), \\(b\\), and \\(x_0\\) are parameters.\nIn Section 8.3, we’ll use the \\(ax + b\\) form of scale conversion, to scale the input to pattern-book functions, but we could equally well have used \\(a(x-x_0)\\).\nIn Section 8.4 we’ll introduce a second scale conversion function, for the output from pattern-book functions. That scaling will also be in the form of a straight-line function: \\(A x + B\\). The use of the lower-case parameter names (\\(a\\), \\(b\\)) versus the upper-case parameter names (\\(A\\), \\(B\\)) will help us distinguish the two different uses for scale conversion, namely input scaling versus output scaling."
  },
  {
    "objectID": "Modeling/01-parameters.html#sec-input-scaling",
    "href": "Modeling/01-parameters.html#sec-input-scaling",
    "title": "8  Parameters",
    "section": "8.3 Input scaling",
    "text": "8.3 Input scaling\nFigure 8.3 is based on the data frame RI-tide which is a minute-by-minute record of the tide level in Providence, Rhode Island (USA) for the period April 1 to 5, 2010. The level variable is measured in meters; the hour variable gives the time of the measurement in hours after midnight at the start of April 1.\n\n\n\n\n\nFigure 8.3: Tide levels oscillate up and down over time. This is analogous to the \\(\\sin(t)\\) pattern-book function.\n\n\n\n\nThe pattern-book \\(\\sin()\\) and the function \\(\\color{magenta}{\\text{level}}\\color{blue}{(hour)}\\) have similar shapes, so it seems reasonable to model the tide data as a sinusoid. However, the scale of the axes is different on the two graphs.\nTo model the tide with a sinusoid, we need to modify the sinusoid to change the scale of the input and output. First, let’s look at how to accomplish the input scaling. Specifically, we want the pure-number input \\(t\\) to the sinusoid be a function of the quantity \\(hour\\). Our framework for this re-scaling is the straight-line function. We will replace the pattern-book input \\(t\\) with a function \\[t(\\color{blue}{hour}) \\equiv a\\, \\color{blue}{hour} + b\\ .\\]\nThe challenge is to find values for the parameters \\(a\\) and \\(b\\) that will transform the \\(\\color{blue}{\\mathbf{\\text{blue}}}\\) horizontal axis into the black horizontal axis, like this:\n\n\n\n\n\n\n\n\n\nBy comparing the two axes, we can estimate that \\(\\color{blue}{10} \\rightarrow 4\\) and \\(\\color{blue}{100} \\rightarrow 49\\). With these two coordinate points, we can find the straight-line function that turns \\(\\color{blue}{\\mathbf{\\text{blue}}}\\) into black by plotting the coordinate pairs \\((\\color{blue}{0},1)\\) and \\((\\color{blue}{100}, 51)\\) and finding the straight-line function that connects the points.\n\n\n\n\n\nFigure 8.4: The input scaling function must transform 10 into 4 and transform 100 into 49 to properly arrange the time scale with the scale for the pattern-book function.\n\n\n\n\nYou can calculate for yourself that the function that relates \\(\\color{blue}{\\mathbf{\\text{blue}}}\\) to black is \\[t(\\color{blue}{time}) = \\underbrace{\\frac{1}{2}}_a \\color{blue}{time}  \\underbrace{-1\\LARGE\\strut}_b\\]\nReplacing the pure number \\(t\\) as the input to pattern-book \\(\\sin(t)\\) with the transformed \\(\\frac{1}{2} \\color{blue}{time}\\) we get a new function: \\[g(\\color{blue}{time}) \\equiv \\sin\\left(\\strut {\\small\\frac{1}{2}}\\color{blue}{time} - 1\\right)\\ .\\] Figure 11.6 plots \\(g()\\) along with the actual tide data.\n\n\n\n\n\nFigure 8.5: The sinusoid with input scaling (black) aligns nicely with the tide-level data."
  },
  {
    "objectID": "Modeling/01-parameters.html#sec-output-scaling",
    "href": "Modeling/01-parameters.html#sec-output-scaling",
    "title": "8  Parameters",
    "section": "8.4 Output scaling",
    "text": "8.4 Output scaling\nJust as the natural input needs to be scaled before it reaches the pattern-book function, so the output from the pattern-book function needs to be scaled before it presents a result suited for interpreting in the real world.\n\n\n\n\n\nFigure 8.6: Natural quantities must be scaled to pure numbers before being suited to the pattern-book functions. The output from the pattern-book function is a pure number which is scaled to the natural quantity of interest.\n\n\n\n\nThe overall result of input and output scaling is to tailor the pattern-book function so that it is ready to be used in the real world.\nLet’s return to Figure 11.6 which shows that the function \\(g(\\color{blue}{time})\\), which scales the input to the pattern-book sinusoid, has a much better alignment to the tide data. Still, the vertical axes of the two graphs in the figure are not the same.\nThis is the job for output scaling, which takes the output of \\(g(\\color{blue}{time})\\) (bottom graph) and scales it to match the \\(\\color{magenta}{level}\\) axis on the top graph. That is, we seek to align the black vertical scale with the \\(\\color{magenta}{\\mathbf{\\text{magenta}}}\\) vertical scale. To do this, we note that the range of the \\(g(\\color{blue}{time})\\) is -1 to 1, whereas the range of the tide-level is about 0.5 to 1.5. The output scaling will take the straight-line form \\[{\\color{magenta}{\\text{level}}}({\\color{blue}{time}}) = A\\, g({\\color{blue}{time}}) + B\\] or, in graphical terms\n\n\n\n\n\n\n\n\n\nWe can figure out parameters \\(A\\) and \\(B\\) by finding the straight-line function that connects the coordinate pairs \\((-1, \\color{magenta}{0.5})\\) and \\((1, \\color{magenta}{1.5})\\) as in Figure 8.7.\n\n\n\n\n\nFigure 8.7: Finding the straight-line function that converts \\(-1 \\rightarrow \\color{magenta}{0.5}\\) and converts \\(1 \\rightarrow \\color{magenta}{1.5}\\)\n\n\n\n\nYou can confirm for yourself that the function that does the job is \\[{\\color{magenta}{\\text{level}}} = 0.5 g({\\color{blue}{time}}) + 1\\ .\\]\nPutting everything together, that is, scaling both the input to pattern-book \\(\\sin()\\) and the output from pattern-book \\(\\sin()\\), we get\n\\[{\\color{magenta}{\\text{level}}}({\\color{blue}{time}}) = \\underbrace{0.5}_A \\sin\\left(\\underbrace{\\small\\frac{1}{2}}_a {\\color{blue}{time}}  \\underbrace{-1}_b\\right) + \\underbrace{1}_B\\]"
  },
  {
    "objectID": "Modeling/01-parameters.html#a-procedure-for-building-models",
    "href": "Modeling/01-parameters.html#a-procedure-for-building-models",
    "title": "8  Parameters",
    "section": "8.5 A procedure for building models",
    "text": "8.5 A procedure for building models\nWe’ve been using pattern-book functions as the intermediaries between input scaling and output scaling, using this format.\n\\[f(x) \\equiv A e^{ax + b} + B\\ .\\] We can use the other pattern-book functions—the gaussian, the sigmoid, the logarithm, the power-law functions—in the same way. That is, the basic framework for modeling is this:\n\\[\\text{model}(x) \\equiv A\\, {g_{pattern\\_book}}(ax + b) + B\\ ,\\] where \\(g_{pattern\\_book}()\\) is one of the pattern-book functions. To construct a basic model, you task has two parts:\n\nPick the specific pattern-book function whose shape resembles that of the relationship you are trying to model. For instance, we picked \\(e^x\\) for modeling COVID cases versus time (at the start of the pandemic). We picked \\(\\sin(x)\\) for modeling tide levels versus time.\nFind numerical values for the parameters \\(A\\), \\(B\\), \\(a\\), and \\(b\\). In Chapter ?chap-fitting-by-eye we’ll show you some ways to make this part of the task easier.\n\nIt’s remarkable that models of a very wide range of real-world relationships between pairs of quantities can be constructed by picking one of a handful of functions, then scaling the input and the output. As we move on to other Blocks in MOSAIC Calculus, you’ll see how to generalize this to potentially complicated relationships among more than two quantities. That’s a big part of the reason you’re studying calculus."
  },
  {
    "objectID": "Modeling/01-parameters.html#other-formats-for-scaling",
    "href": "Modeling/01-parameters.html#other-formats-for-scaling",
    "title": "8  Parameters",
    "section": "8.6 Other formats for scaling",
    "text": "8.6 Other formats for scaling\nOften, modelers choose to use input scaling in the form \\(a (x - x_0)\\) rather than \\(a x + b\\). The two are completely equivalent when \\(x_0 = - b/a\\). The choice between the two forms is largely a matter of convention. But almost always the output scaling is written in the format \\(A y + B\\).\n\nFor the COVID case-number data shown in Figure 8.2, we found that a reasonable match to the data can be had by input- and output-scaling the exponential: \\[\\text{cases}(t) \\equiv  \\underbrace{573}_A e^{\\underbrace{0.19}_a\\ t}\\ .\\]\nYou might wonder why the parameters \\(B\\) and \\(b\\) aren’t included in the model. One reason is that cases and the exponential function already have the same range: zero and upwards. So there’s no need to shift the output with a parameter B.\nAnother reason has to do with the algebraic properties of the exponential function. Specifically, \\[e^{a x + b}= e^b e^{ax} = {\\cal A} e^{ax}\\] where \\({\\cal A} \\equiv e^b\\).\nIn the case of exponentials, writing the input scaling in the form \\(e^{a(x-x_0)}\\) can provide additional insight.\nA bit of symbolic manipulation of the model can provide some additional insight. As you know, the properties of exponentials and logarithms are such that \\[A e^{at} = e^{\\log(A)} e^{at} = e^{a t + \\log(A)} = e^{a\\left(\\strut t + \\log(A)/a\\right)} = e^{a(t-t_0)}\\ ,\\] where \\[t_0 = - \\log(A)/a = - \\log(593)/0.19 = -33.6\\ .\\] You can interpret \\(t_0\\) as the starting point of the pandemic. When \\(t = t_0\\), the model output is \\(e^{k 0} = 1\\): the first case. According to the parameters we matched to the data for March, the pandemic’s first case would have happened about 33 days before March 1, which is late January. We know from other sources of information, the outbreak began in late January. It’s remarkable that even though the curve was constructed without any data from January or even February, the data from March, translated through the curve-fitting process, pointed to the start of the outbreak. This is a good indication that the exponential form for the model is fundamentally correct."
  },
  {
    "objectID": "Modeling/01-parameters.html#parameterization-conventions",
    "href": "Modeling/01-parameters.html#parameterization-conventions",
    "title": "8  Parameters",
    "section": "8.7 Parameterization conventions",
    "text": "8.7 Parameterization conventions\nThere are conventions for the symbols used for input-scaling parameterization of the pattern-book functions. Knowing these conventions makes it easier to read and assimilate mathematical formulas. In several cases, there is more than one conventional option. For instance, the sinusoid has a variety of parameterization forms that get used depending on which feature of the function is easiest to measure. ?tbl-param that are used in practice.\n\n\nSome standard forms of input scaling parameterizations\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nWritten form\nParameter 1\nParameter 2\n\n\n\n\nExponential\n\\(e^{kt}\\)\n\\(k\\)\nNot used\n\n\nExponential\n\\(e^{t/\\tau}\\)\n\\(\\tau\\) “time constant”\nNot used\n\n\nExponential\n\\(2^{t/\\tau_2}\\)\n\\(\\tau_2\\) “doubling time”\nNot used\n\n\nExponential\n\\(2^{-\\tau_{1/2}}\\)\n\\(-\\tau_{1/2}\\) “half life”\nNot used\n\n\nPower-law\n\\([x - x_0]^p\\)\n\\(x_0\\) x-intercept\nexponent\n\n\nSinusoid\n\\(\\sin\\left(\\frac{2 \\pi}{P} (t-t_0)\\right)\\)\n\\(P\\) “period”\n\\(t_0\\) “time shift”\n\n\nSinusoid\n\\(\\sin(\\omega t + \\phi)\\)\n\\(\\omega\\) “angular frequency”\n\\(\\phi\\) “phase shift”\n\n\nSinusoid\n\\(\\sin(2 \\pi \\omega t + \\phi)\\)\n\\(\\omega\\) “frequency”\n\\(\\phi\\) “phase shift”\n\n\nGaussian\ndnorm(x, mean, sd)\n“mean” (center)\nsd “standard deviation”\n\n\nSigmoid\npnorm(x, mean, sd)\n“mean” (center)\nsd “standard deviation”\n\n\nStraight-line\n\\(mx + b\\)\n\\(m\\) “slope”\n\\(b\\) “y-intercept”\n\n\nStraight-line\n\\(m (x-x_0)\\)\n\\(m\\) “slope”\n\\(x_0\\) “center”"
  },
  {
    "objectID": "Modeling/01-parameters.html#exercises",
    "href": "Modeling/01-parameters.html#exercises",
    "title": "8  Parameters",
    "section": "8.8 Exercises",
    "text": "8.8 Exercises"
  },
  {
    "objectID": "Modeling/01-parameters.html#drill",
    "href": "Modeling/01-parameters.html#drill",
    "title": "8  Parameters",
    "section": "8.9 Drill",
    "text": "8.9 Drill\n\n\nPart i What is the period of the function \\(\\sin(6\\pi t)\\)?\n1/31/2236\n\n\n\n\nPart ii What is the period of \\(g(t)\\)? \\[g(t) \\equiv \\frac{5}{\\sin(2 \\pi t)}\\]\n\n1\n5\n\\(2 \\pi/5\\)\n\\(5/2\\pi\\)\n\\(g(t)\\) isn’t periodic.\n\n\n\n\n\nPart iii What is the period of \\(g(t)\\)? \\[g(t) \\equiv \\text{dnorm}\\left(\\frac{2\\pi}{5}(t-3)\\right)\\]\n\n1\n5\n\\(2 \\pi/5\\)\n\\(5/2\\pi\\)\n\\(g(t)\\) isn’t periodic.\n\n\n\n\n\nPart iv One of the following choices is the standard deviation of the function graphed. Which one?\n01234\n\n\n\n\nPart v What is the value of the parameter “mean” for the function shown in the graph?\n\n-2\n-1\n0.5\n1\n2\n“mean” is not a parameter of this function.\n\n\n\n\n\nPart vi What is the value of the parameter “sd” for the function shown in the graph?\n\n-2\n-1\n0.5\n1\n2\n“sd” is not a parameter of this function.\n\n\n\n\n\nPart vii What is the value of the parameter “mean” for the function shown in the graph?\n\n-2\n-1\n0.5\n1\n2\n“mean” is not a parameter of this function.\n\n\n\n\n\nPart viii What is the value of the parameter “sd” for the function shown in the graph?\n\n-2\n-1\n0.5\n1\n2\n“sd” is not a parameter of this function."
  },
  {
    "objectID": "Modeling/02-assembling-functions.html",
    "href": "Modeling/02-assembling-functions.html",
    "title": "9  Assembling functions",
    "section": "",
    "text": "When we need a new function for some purpose, we practically always build it out of existing functions. For illustrate, a function like \\[f(x) \\equiv A \\sin\\left(\\frac{2 \\pi}{P}x\\right) + B\\] is built by assembling a straight-line input scaling (\\(2\\pi/P\\)), a pattern-book \\(\\sin()\\) function, and another straight-line function \\(A f(x) + B\\) for scaling the output from \\(\\sin()\\).\nIn this chapter, we’ll review four general frameworks for combining functions: linear combination of functions, function composition, function multiplication, and “piecewise” splitting of the domain. You have almost certainly seen all four of these frameworks in your previous mathematical studies, although you might not have known that they have names."
  },
  {
    "objectID": "Modeling/02-assembling-functions.html#linear-combination",
    "href": "Modeling/02-assembling-functions.html#linear-combination",
    "title": "9  Assembling functions",
    "section": "9.1 Linear combination",
    "text": "9.1 Linear combination\nOne of the most widely used sorts of combination is called a linear combination. The mathematics of linear combination is, it happens, at the core of the use of math in a large variety of real-world applications, whether that be constructing a Google-like search engine or analyzing medical data to see if a treatment has a positive effect.\nThere is a special name for a quantity used to scale the output of a function: a scalar. Scalars are ordinary quantities; the word “scalar” is merely a way to distinguish the quantity from the function it is multiplying. Scalars generally come with units. So we might well have a metric polynomial and an equivalent traditional-unit polynomial.\nTo illustrate how linear combination is used to create new functions, consider polynomials, for instance, \\[f(x) \\equiv 3 x^2 + 5 x - 2\\ .\\] There are three pattern-book functions in this polynomial. In polynomials the functions being combined are all power-law functions: \\(g_0(x) \\equiv 1\\), \\(g_1(x) \\equiv x\\), and \\(g_2(x) \\equiv x^2\\). With these functions defined, we can write the polynomial \\(f(x)\\) as \\[f(x) \\equiv 3 g_2(x) + 5 g_1(x) - 2 g_0(x)\\] Each of the functions is being scaled by a quantity: 3, 5, and -2 in this example. Then the scaled functions are added up. That’s a linear combination; scale and add.\nIn high school, polynomials are often presented as puzzles—factor them to find the roots! In calculus, however, polynomials are used as functions for modeling. They are a kind of modeling “clay,” which can be shaped as needed.\nThere are other places where you have seen linear combinations:\nNote that neither the parameterized exponential nor the parameterized sinusoid is a polynomial. A polynomial is always a linear combination of monomials. (As Section Section 5.3, describes, monomials are the power-law functions \\(x^0\\), \\(x_1\\), \\(x_2\\), and so on.)\n\nThe parameterized sinusoid \\[A \\sin\\left(\\frac{2 \\pi}{P}t\\right) + B\\] is a linear combination of the functions \\(h_1(t) \\equiv \\sin\\left(\\frac{2 \\pi}{P} t\\right)\\) and \\(h_2(t) \\equiv 1\\). The linear combination is \\(A\\,h_1(t) + B\\, h_2(t)\\).\nThe parameterized exponential \\[A e^{kt} + B\\] The functions being combined are \\(e^{kt}\\) and \\(1\\). The scalars are, again, \\(A\\) and \\(C\\).\nThe straight line function, such as \\(\\mbox{output}(x) \\equiv A x + B\\) and \\(\\mbox{input}(x) \\equiv a x + b\\). The functions being combined are \\(x\\) and \\(1\\), the scalars are \\(a\\) and \\(b\\).\n\nThere are a few reasons for us to be introducing linear combinations here.\n\nYou will see linear combinations everywhere once you know to look for them.\nThere is a highly refined mathematical theory of linear combinations that gives us powerful ways to think about them as well as computer software that can quickly find the best scalars to use to match input-output data.\nThe concept of linear combination generalizes the simple idea that we have been calling “scaling the output.” From now on, we’ll use the linear-combination terminology and avoid the narrower idea of “scaling the output.”\nMany physical systems are described by linear combinations. For instance, the motion of a vibrating molecule, a helicopter in flight, or a building shaken by an earthquake are described in terms of simple “modes” which are linearly combined to make up the entire motion. More down to Earth, the timbre of a musical instrument is set by the scalars in a linear combination of pure tones.\nMany modeling tasks can be put into the framework of choosing an appropriate set of simple functions to combine and then figuring out the best scalars to use in the combination. (Generally, the computer does the figuring.)\n\n\n\nThe declination angle is the latitude of the point on the earth’s surface pierced by an imagined line connecting the centers of the earth and the sun. This angle is a function of the “day of year” (\\(doy\\)) measured as 0 at midnight before January 1 and 365.25 at the midnight ending December 31.\n\nDeclination angle as a function of day-of-year:\n\\(\\delta(doy) = 23.44 \\sin\\left({\\small\\frac{2\\pi}{365.25}} (doy-9)\\right)\\)\nThe output has units in degrees.\nImage source\nComposing day_length\\((L, \\delta)\\) onto \\(\\delta(doy)\\) gives the length of daylight as a function of day of the year: \\[\\text{daylight}(L, doy) \\equiv {\\small\\frac{2}{15}} \\arccos\\left(-\\tan(L)*\\tan(\\delta(doy))\\right)\\ .\\] Function composition enables us to transform a function that takes one kind of thing as input (declination in this example) and turn it into a function that takes another kind of thing as input (day of year). :::"
  },
  {
    "objectID": "Modeling/02-assembling-functions.html#sec-function-composition",
    "href": "Modeling/02-assembling-functions.html#sec-function-composition",
    "title": "9  Assembling functions",
    "section": "9.2 Function composition",
    "text": "9.2 Function composition\nTo compose two functions, \\(f(x)\\) and \\(g(x)\\), means to apply one of the functions to the output of the other. “\\(f()\\) composed with \\(g()\\)” means \\(f(g(x))\\). This is generally very different from “\\(g()\\) composed with \\(f()\\)” which means \\(g(f(x))\\).\nFor instance, suppose you have recorded the outdoor temperature over the course of a day and packaged this into a function \\(\\text{AirTemp}(t)\\): temperature as a function of time \\(t\\). Your digital thermometer uses degrees Celsius, but you want the output units to be degrees Kelvin. The conversion function is \\[\\text{CtoK}(C) \\equiv C + 273.15\\] Notice that CtoK() takes temperature in \\(^\\circ C\\) as input. With this, we can write the “Kelvin as a function of time” as \\[\\text{CtoK}\\left(\\text{AirTemp}(t)\\right)\\]\nIt’s important to distinguish the above time \\(\\rightarrow\\) Kelvin function from something that looks very much the same but is utterly different: \\(\\text{AirTemp}\\left(\\text{CtoK}(C)\\right)\\). In the first, the input is time. In the second, it is temperature in celsius.\n\nAnyone who lives far from the equator is familiar with the annual cycle of short winter days and long summer days. The magnitude of this cycle is a function of latitude; the further away from the equator the larger the winter-summer day-length difference.\nA simple model accounts for the length of daylight (in hours) as a function of latitude \\(L\\) and the declination angle \\(\\delta\\) of the sun.\n\\[\\text{daylight}(L, \\delta) \\equiv {\\small\\frac{2}{15}} \\arccos\\left(-\\tan(L)*\\tan(\\delta)\\right)\\]"
  },
  {
    "objectID": "Modeling/02-assembling-functions.html#sec-function-multiplication",
    "href": "Modeling/02-assembling-functions.html#sec-function-multiplication",
    "title": "9  Assembling functions",
    "section": "9.3 Function multiplication",
    "text": "9.3 Function multiplication\nMultiplication is the third in our repertoire of methods for making new functions. With two functions \\(f(x)\\) and \\(g(x)\\), the product is simply \\(f(x)g(x)\\).\nIt’s essential to distinguish between function multiplication and function composition:\n\n\nIn function composition, the order of the functions matters: \\(f(g(x))\\) and \\(g(f(x))\\) are in general completely different functions.\nIn function multiplication, the order doesn’t matter because multiplication is commutative, that is, if \\(f()\\) and \\(g()\\) are the functions to be multiplied \\(f(x) \\times g(x) = g(x)\\times f(x)\\).\n\\[\\underbrace{f(x) g(x)}_\\text{multiplication}\\ \\ \\ \\ \\underbrace{f(g(x)) \\ \\ \\text{or}\\ \\ \\ g(f(x))}_\\text{composition}\\]\nIn function composition, only one of the functions—the interior function is applied to the overall input, \\(x\\) in the above example. The exterior function is fed its input from the output of the interior function.\nIn multiplication, each of the functions is applied to the input individually. Then their outputs are multiplied to produce the overall output.\n\nTransient vibration\nA guitar string is plucked to produce a note. The sound is, of course, vibrations of the air created by vibrations of the string.\nAfter plucking, the note fades away. An important model of this is a sinusoid (of the correct period to correspond to the frequency of the note) times an exponential.\nFunction multiplication is used so often in modeling that you’ll see it in many modeling situations. Here’s one example that is important in physics and communication: the wave packet. Overall, the wave packet is a localized oscillation as in Figure 9.2. The packet can be modeled with the product of two pattern-book functions: a gaussian times a sinusoid.\n\n\n\n\n\n\n\nFigure 9.1: The two components of the wave packet in Figure 9.2: an “envelope” and an oscillation. Multiplying these components together produces the wave packet.\n\n\n\n\n\n\n\n\nFigure 9.2: A wave packet constructed by multiplying a sinusoid and a gaussian function.\n\n\n\n\n\nThe initial rise in popularity of the social media platform Yik Yak was exponential. Then popularity leveled off, promising a steady, if static, business into the future. But, the internet being what it is, popularity collapsed to near zero and the company closed.\nOne way to model this pattern is by multiplying a sigmoid by an exponential. (See Figure 9.3.)\n\n\n\n\n\nFigure 9.3: Subscriptions to the web messaging service Yik Yak grew exponentially in 2013 and 2014, then collapsed. The company closed in 2017.\n\n\n\n\n\nFunctions constructed as a product of simple functions can look like this in tradition notation: \\[h(t) \\equiv \\sin(t) e^{-t}\\] and like this in computer notation:\n\nh <- makeFun(sin(t)*exp(-t) ~ t)"
  },
  {
    "objectID": "Modeling/02-assembling-functions.html#sec-piecewise-intro",
    "href": "Modeling/02-assembling-functions.html#sec-piecewise-intro",
    "title": "9  Assembling functions",
    "section": "9.4 Splitting the domain",
    "text": "9.4 Splitting the domain\nConsider the familiar absolute-value function:\n\n\n\n\n\n\nFigure 9.4: The absolute-value function\n\n\n\n\\[abs(x) \\equiv \\left|x\\right|\\] Written this way, the definition of \\(abs()\\) is a tautology: unless you already know what \\(\\left|x\\right|\\) means, you will have no clue what’s going on.\nCan we assemble \\(abs(x)\\) out of pattern-book functions? What’s distinctive about \\(abs(x)\\) is the break at \\(x=0\\). There’s no similarly sharp transition in any of the pattern-book functions.\nOne way to construct the sharp transition is to view \\(abs(x)\\) as two functions, one whose domain is the negative half of the number line and the other having a domain that is the non-negative half. That is, we’ll break the domain of \\(abs()\\) into two pieces. For the right piece of the domain, \\(abs(x)\\) is simply proportional\\((x)\\). For the left piece of the domain, \\(abs(x)\\) is \\(-\\)proportional\\((x)\\).\nA function defined separately on different pieces of its domain is called a piecewise function. In the conventional mathematical notation, there is a large \\(\\LARGE\\left\\{\\right.\\) followed by two or more lines. Each line gives a formula for that part of the function and indicates to which interval the formula applies.\n\\[abs(x) \\equiv \\left\\{\n\\begin{array}{rl}  x & \\text{for}\\ 0 \\leq x \\\\\n- x & \\text{otherwise}\\\\\\end{array}\n\\right.\\]\n\n\n\n\n\n\nFigure 9.5: The Heaviside function\n\n\n\nAnother piecewise function widely used in technical work, but not as familiar as \\(abs()\\) is the Heaviside function, which has important uses in physics and engineering.\n\\[\\text{Heaviside}(x) \\equiv \\left\\{\n\\begin{array}{cl} 1 & \\text{for}\\ 0 \\leq x \\\\0 & \\text{otherwise}\\end{array}\n\\right.\\]\nThe Heaviside function is defined on the same two pieces of the number line as \\(abs()\\). To the right of zero, Heaviside is identical to constant(). To the left, it’s identical to \\(0\\) times constant\\(()\\).\nThe vertical gap between the two pieces of the Heaviside function is called a discontinuity. Intuitively, you cannot draw a discontinuous function without lifting the pencil from the paper. The Heaviside’s discontinuity occurs at input \\(x=0\\).\n\n9.4.1 Computing notation\nThe usual mathematical notation for piecewise functions, spread out over multiple lines that are connected with a tall brace, is an obvious non-candidate for computer notation. In R, the stitching together of the two pieces can be done with the function ifelse(). The name is remarkably descriptive. The ifelse() function takes three arguments. The first is a question to be asked, the second is the value to return if the answer is “yes,” and the third is the value to return for a “no” answer.\nTo define \\(abs()\\) or Heaviside\\(()\\) the relevant question is, “Is the input on the right or left side of zero on the number line?” In widely-used computing languages such as R, the format for asking a question does not involve a question mark. For example, to ask the question, “Is 3 less than 2?” use the expression:\n\n3 < 2\n\nIn mathematics notation, \\(3 < 2\\) is a declarative statement and is an impossibility. More familiar would be \\(x < 2\\), which is again a declarative statement putting a restriction on the possible values of the quantity \\(x\\).\nIn computing notation, 3 < 2 or x < 2 is not a declaration, it is an imperative  statement that directs the computer to do the calculation to find out if the statement is true or false, or, as written in R, TRUE or FALSE.\n\n\nRemember that the tilde-expressions given as input to makeFun() are declarative, not imperative. makeFun() stores the tilde expression exactly as is, with symbols such as x being names rather than quantities. makeFun() packages up the stored tilde expression in the form of an R function. The assignment command Heaviside <- ... gives the name Heaviside to the function created by makeFun().\nOnly when you apply the function created by makeFun() to an input quantity will the tilde-expression be turned into an imperative statement that asks the question 0 <= x and then chooses the second or third argument to ifelse() as the result.\nHere’s a definition of Heaviside() written with ifelse().\n\nHeaviside <- makeFun(ifelse(0 <= x, 1, 0) ~ x)\n\n?tbl-R-questions shows computer notation for some common sorts of questions.\n\n\nEach of these imperative statements in R asks a question about numbers.\n\n\n\n\n\n\n\n\n\n\nR notation\nEnglish\n\n\n\n\nx > 2\n“Is \\(x\\) greater than 2?”\n\n\ny >= 3\n“Is \\(y\\) greater than or equal to 3?”\n\n\nx == 4\n“Is \\(x\\) exactly 4?”\n\n\n2 < x & x < 5\n“Is \\(x\\) between 2 and 5?” Literally, “Is \\(x\\) both greater than 2 and less than 5?”\n\n\nx < 2 | x > 6\n“Is \\(x\\) either less than 2 or greater than 6?”\n\n\nabs(x-5) < 2\n“Is \\(x\\) within two units of 5?”\n\n\n\n\n\n\n\nFigure 9.6 is a graph of monthly natural gas use in the author’s household versus average temperature during the month. (Natural gas is measured in cubic feet, abbreviated ccf.)\n\n\n\n\n\nFigure 9.6: The amount of natural gas used for heating the author’s home varies with the outside temperature.\n\n\n\n\nThe graph looks somewhat like a hockey stick. A sloping straight-line dependence of ccf on temperature for temperatures below \\(60^\\circ\\)F and constant for higher temperatures. The shape originates from the dual uses of natural gas. Gas is used for cooking and domestic hot water, the demand for which is more or less independent of outdoor temperature at about 15 ccf per month. Gas is also used for heating the house, but that’s needed only when the temperature is less than about \\(60^\\circ\\)F.\nWe can accomplish the hockey-stick shape with a linear combination of the ramp() function and a constant. The ramp function represents gas used for heating, the constant is the other uses of gas (which are modeled as not depending on temperature. Overall, the model is \\[\\text{gas}(x) \\equiv 4.3\\,  \\text{ramp}(62-x)  + 15\\ .\\] Even simpler is the model for the other uses of natural gas: \\[\\text{other}(x) \\equiv 15\\ .\\]"
  },
  {
    "objectID": "Modeling/02-assembling-functions.html#computing-outside-the-domain",
    "href": "Modeling/02-assembling-functions.html#computing-outside-the-domain",
    "title": "9  Assembling functions",
    "section": "9.5 Computing outside the domain",
    "text": "9.5 Computing outside the domain\nEach of our pattern-book functions, with two exceptions, has a domain that is the entire number line \\(-\\infty < x < \\infty\\). No matter how big or small is the value of the input, the function has an output. Such functions are particularly nice to work with since we never have to worry about the input going out of bounds.\nThe two exceptions are:\n\nthe logarithm function, which is defined only for \\(0 < x\\).\nsome of the power-law functions: \\(x^p\\).\n\nWhen \\(p\\) is negative, the output of the function is undefined when \\(x=0\\). You can see why with a simple example: \\(g(x) \\equiv x^{-2}\\). Most students had it drilled into them that “division by zero is illegal,” and \\(g(0) = \\frac{1}{0} \\frac{1}{0}\\), a double law breaker.\nWhen \\(p\\) is not an integer, that is \\(p \\neq 1, 2, 3, \\cdots\\) the domain of the power-law function does not include negative inputs. To see why, consider the function \\(h(x) \\equiv x^{1/3}\\).\n\n\n\nIt can be tedious to make sure that you are on the right side of the law when dealing with functions whose domain is not the whole number line. The designers of the hardware that does computer arithmetic, after several decades of work, found a clever system to make it easier. It’s a standard part of such hardware that whenever a function is handed an input that is not part of that function’s domain, one of two special “numbers” is returned. To illustrate:\n\nsqrt(-3)\n## [1] NaN\n(-2)^0.9999\n## [1] NaN\n1/0\n## [1] Inf\n\nNaN stands for “not a number.” Just about any calculation involving NaN will generate NaN as a result, even those involving multiplication by zero or cancellation by subtraction or division.1 For instance:\n\n0 * NaN\n## [1] NaN\nNaN - NaN\n## [1] NaN\nNaN / NaN\n## [1] NaN\n\nDivision by zero produces Inf, whose name is reminiscent of “infinity.” Inf infiltrates any calculation in which it takes part:\n\n3 * Inf\n## [1] Inf\nsqrt(Inf)\n## [1] Inf\n0 * Inf\n## [1] NaN\nInf + Inf\n## [1] Inf\nInf - Inf\n## [1] NaN\n1/Inf\n## [1] 0\n\nTo see the benefits of the NaN / Inf system let’s plot out the logarithm function over the graphics domain \\(-5 \\leq x \\leq 5\\). Of course, part of that graphics domain, \\(-5 \\leq x \\leq 0\\) is not in the domain of the logarithm function and the computer is entitled to give us a slap on the wrists. The NaN provides some room for politeness."
  },
  {
    "objectID": "Modeling/02-assembling-functions.html#exercises",
    "href": "Modeling/02-assembling-functions.html#exercises",
    "title": "9  Assembling functions",
    "section": "9.6 Exercises",
    "text": "9.6 Exercises"
  },
  {
    "objectID": "Modeling/02-assembling-functions.html#drill",
    "href": "Modeling/02-assembling-functions.html#drill",
    "title": "9  Assembling functions",
    "section": "9.7 Drill",
    "text": "9.7 Drill\n\n\nPart i   Which of the following tilde-expressions could be used to generate the graph?\n\nifelse(abs(x) > 1, x^3, x) ~ x\nifelse(x > 0, sin(x), x) ~ x\nifelse(abs(x) > 1, x, x^3) ~ x\nifelse(abs(x) > 1, x, exp(x^2)) ~ x\nifelse(x > 1, 1, x^2) ~ x\n\n\n\n\n\nPart ii   Which of the following tilde-expressions could be used to generate the graph?\n\nifelse(abs(x) > 1, x^3, x) ~ x\nifelse(x > 0, sin(x), x) ~ x\nifelse(abs(x) > 1, x, x^3) ~ x\nifelse(abs(x) > 1, x, exp(x^2)) ~ x\nifelse(x > 1, 1, x^2) ~ x\n\n\n\n\n\nPart iii   Which of the following tilde-expressions could be used to generate the graph?\n\nifelse(abs(x) > 1, x^3, x) ~ x\nifelse(x > 0, sin(x), x) ~ x\nifelse(abs(x) > 1, x, x^3) ~ x\nifelse(abs(x) > 1, x, exp(x^2)) ~ x\nifelse(x > 1, 1, x^2) ~ x\n\n\n\n\n\nPart iv   Which of the following tilde-expressions could be used to generate the graph?\n\nifelse(abs(x) > 1, x^3, x) ~ x\nifelse(x > 0, sin(x), x) ~ x\nifelse(abs(x) > 1, x, x^3) ~ x\nifelse(abs(x) > 1, x, exp(x^2)) ~ x\nifelse(x > 1, 1, x^2) ~ x\n\n\n\n\n\nPart v   Which of the following tilde-expressions could be used to generate the graph?\n\nifelse(abs(x) > 1, x^3, x) ~ x\nifelse(x > 0, sin(x), x) ~ x\nifelse(abs(x) > 1, x, x^3) ~ x\nifelse(abs(x) > 1, x, exp(x^2)) ~ x\nifelse(x > 1, 1, x^2) ~ x"
  },
  {
    "objectID": "Modeling/03-functions-with-multiple-inputs.html",
    "href": "Modeling/03-functions-with-multiple-inputs.html",
    "title": "10  Functions with multiple inputs",
    "section": "",
    "text": "We can use linear combination and function multiplication to build up custom functions from the basic modeling functions. Similarly, linear combination and function multiplication provide ways to construct functions of multiple inputs."
  },
  {
    "objectID": "Modeling/03-functions-with-multiple-inputs.html#linear-combinations",
    "href": "Modeling/03-functions-with-multiple-inputs.html#linear-combinations",
    "title": "10  Functions with multiple inputs",
    "section": "10.1 Linear combinations",
    "text": "10.1 Linear combinations\nHousing prices are determined by several (or many!) factors. Translating the previous sentence into the language of functions, we can say that the price is a function of multiple inputs. Plausible inputs to the function include the amount of living area and the number of bedrooms and bathrooms. The inputs may also include quality of the neighborhood, length of commute, and so on.\nOften, the starting point for building a function with multiple inputs is a data frame whose variables include the function output (price) and the inputs to the function. Modelers often begin by constructing a function that is a linear combination of the input variables. To demonstrate what such functions look like, we can use the SaratogaHouses dataset, which records the sales price of 1728 houses in Saratoga County, New York, USA and 15 other variables for each house, such as livingArea and the number of bedrooms and bathrooms.\nThe techniques for constructing functions from data will be introduced in Block ?sec-vectors-linear-combinations. For now, let’s simply see what such functions look like. From SaratogaHouses we constructed this function:\n[These are not made-up scalars in the linear combination. They have been derived from the SaratogaHouses data using a method called linear regression. The “linear” in the name refers to “linear combination.”]\n\\[\\mathtt{price}(\\mathtt{livingArea}, \\mathtt{bedrooms}, \\mathtt{bathrooms}) \\equiv\\\\  21000 + 105\\, \\mathtt{livingArea}\\\\ - 13000\\,\\mathtt{bedrooms} + 26000\\, \\mathtt{bathrooms}\\]\nThe model function is a simple linear combination, but it effectively quantifies how different aspects of a house contribute to its sales price. The model (which is based on data from two decades ago) indicates that an additional square foot of living area is worth about 105 dollars per foot2. An extra bathroom is worth about $25,000. Bedrooms, strangely, are assigned a negative value by the model.\nPossibly you already understand what is meant by “an additional square foot” or “an extra bathroom.” These ideas can be intuitive, but they can be best understood with a grounding in calculus, which we turn to in Block ?sec-differentiation-block. For instance, the negative scalar on bedrooms will make sense when you understand “partial derivatives,” the subject of Chapter Section 25."
  },
  {
    "objectID": "Modeling/03-functions-with-multiple-inputs.html#fx-times-gt",
    "href": "Modeling/03-functions-with-multiple-inputs.html#fx-times-gt",
    "title": "10  Functions with multiple inputs",
    "section": "10.2 f(x) times g(t)",
    "text": "10.2 f(x) times g(t)\nWhen a guitar string is at rest it forms a straight line connecting its two fixed ends: one set by finger pressure along the neck of the guitar and the other at the bridge near the center of the guitar body. When a guitar string is plucked, its oscillations follow a sinusoid pattern of displacement. With the right camera and lighting setup, we can see these oscillations in action:\n\n\n\n\n\n\n\n\n\nFor a string of length \\(L\\), the string displacement is a function of position \\(x\\) along the string and is a linear combination of functions of the form \\[g_k(x) \\equiv \\sin(k \\pi x /L)\\] where \\(k\\) is an integer. A few of these functions are graphed in ?fig-guitar-string-modes with \\(k=1\\), \\(k=2\\), and \\(k=3\\).\n\n\n\n\n\nFigure 10.1: Vibrational modes of a guitar string.\n\n\n\n\n\n\n\nFigure 10.2: Vibrational modes of a guitar string.\n\n\n\n\nShapes of the sort in ?fig-guitar-string-modes are a stop-motion flash snapshot of the string. The string’s shape also changes in time, so the string’s displacement is a function of both \\(x\\) and \\(t\\). The displacement itself is a sinusoid whose time period depends on the length and tension of the string as well as the number of cycles of the spatial sine: \\[g_k(x, t) \\equiv \\sin(\\frac{k \\pi}{L} x) \\ \\sin(\\frac{k \\pi}{P}t)\\] Figure @ref(fig:string-motion) shows a few snapshots of the 1.5 cycle string at different moments in time, and the motion of the linear combination.\n\n\n\n\n\nString position changes over time.\n\n\n\n\n\n\n\nString position changes over time.\n\n\n\n\n\nWe left function composition out of the list of ways to build multi-input functions out of simpler functions with a single input.\nFor instance, consider the two functions \\(f(x)\\) and \\(g(t)\\). The composition \\(f(g(t))\\) has only one input: \\(t\\). Similarly, \\(g(f(x))\\) has only one input: \\(x\\)."
  },
  {
    "objectID": "Modeling/03-functions-with-multiple-inputs.html#constructing-your-own-from-data",
    "href": "Modeling/03-functions-with-multiple-inputs.html#constructing-your-own-from-data",
    "title": "10  Functions with multiple inputs",
    "section": "10.3 Constructing your own from data",
    "text": "10.3 Constructing your own from data\nEven if you don’t yet have a theoretical understanding of how to construct functions with multiple inputs from data, you can R/mosaic operations for doing so. A key function is fitModel(), which like makeFun(), constructs a function. And, like makeFun(), you need to use a tilde expression to specify the model formula. But, unlike makeFun(), you can leave it to the computer to find the parameters that will make the function align with data.\nTo illustrate, we can construct a housing-price model from the SaratogaHouses data:\n\nprice <- fitModel(price ~ A + B*livingArea + C*bedrooms + D*bathrooms,\n                  data = SaratogaHouses)\n\nNote that in applying the price() function to inputs, we are using the names of the inputs explicitly. To write the command price(2000,3,2) risks mixing up which input is which.\nUse the function in the ordinary way. For instance, here’s what the model has to say about the anticipated sales price of a house with 2000 square feet of living area, three bedrooms, and two bathrooms.\n\nprice(livingArea=2000, bedrooms=3, bathrooms=2)\n## [1] 242448.1\n\nThe units of the output are the same as the units of price in the SaratogaHouses data frame: dollars.\n\n\nTo look at a function, give the function name without parentheses, e.g. price. In contrast, you’ll always use parentheses when applying the function to inputs.\nFor technical reasons, the functions created by fitModel() have a lot of computer-programming jargon in them, as you can see by constructing the model itself and then looking at the function. But you will also see clearly the values of the parameters found by fitModel()."
  },
  {
    "objectID": "Modeling/03-functions-with-multiple-inputs.html#exercises",
    "href": "Modeling/03-functions-with-multiple-inputs.html#exercises",
    "title": "10  Functions with multiple inputs",
    "section": "10.4 Exercises",
    "text": "10.4 Exercises"
  },
  {
    "objectID": "Modeling/04-fitting-by-eye.html",
    "href": "Modeling/04-fitting-by-eye.html",
    "title": "11  Fitting features",
    "section": "",
    "text": "For more than three centuries, there has been a standard calculus model of an everyday phenomenon: a hot object such as a cup of coffee cooling off to room temperature. The model, called Newton’s Law of Cooling, posits that the rate of cooling is proportional to the difference between the object’s temperature and the ambient temperature. The technology for measuring temperature (Figure 11.1) was rudimentary in Newton’s era, raising the question of how Newton formulated a quantitative theory of cooling. (We’ll return to this question in Section 12.)\nUsing today’s technology, Prof. Stan Wagon of Macalester College investigated the accuracy of Newton’s “Law.” ?fig-Fun-4-intro-1 shows some of Wagon’s data from experiments with cooling water. He poured boiling water from a kettle into an empty room-temperature mug (26 degrees C) and measured the temperature of the water over the next few hours.\nThis chapter is about fitting, finding parameters that will align the functions with the data such as in Figure 11.2. In this chapter, we’ll work the the exponential, sinusoid, and gaussian functions. In Chapter Section 14 we’ll consider the power-law and logarithm functions.\nIn every instance, the first step, before finding parameters, is to determine that the pattern shown in the data is a reasonable match to the shape of the function you are considering. Here’s a reminder of the shapes of the functions we’ll be fitting to data in this chapter. If the shapes don’t match, there’s little point in looking for the parameters to fit the data!"
  },
  {
    "objectID": "Modeling/04-fitting-by-eye.html#gaussian",
    "href": "Modeling/04-fitting-by-eye.html#gaussian",
    "title": "11  Fitting features",
    "section": "11.1 Gaussian",
    "text": "11.1 Gaussian\nThe ability to perceive color comes from “cones”: specialized light-sensitive cells in the retina of the eye. Human color perception involves three sets of cones. The L cones are most sensitive to relatively long wavelengths of light near 570 nanometers. The M cones are sensitive to wavelengths near 540 nm, and the S cones to wavelengths near 430nm.\nThe current generation of Landsat satellites uses nine different wavelength-specific sensors. This makes it possible to distinguish features that would be undifferentiated by the human eye.\n\n\n\n\n\n\nFigure 11.3: Two views of the same scene synthesized by combining the output of different types of cones. The top picture uses V, M, and S cones; the bottom only S, M, and L cones. The dark geen leaves clearly revealed in the top picture are not distinguishable in the bottom picture. (Source: Tedore and Nilsson)\n\n\n\nBack toward Earth, birds have five sets of cones that cover a wider range of wavelengths than humans. (Figure 11.4) Does this give them a more powerful sense of the differences between natural features such as foliage or plumage? One way to answer this question is to take photographs of a scene using cameras that capture many narrow bands of wavelengths. Then, knowing the sensitivity spectrum of each set of cones, new “false-color” pictures can be synthesized recording the view from each set.1\n\n\n\n\n\nFigure 11.4: Sensitivity to wavelength for each of the five types of bird cones. [Source: Tedore and Nilsson]\n\n\n\n\nCreating the false-color pictures on the computer requires a mathematical model of the sensitivities of each type of cone. The graph of each sensitivity function resembles a Gaussian function.\nThe Gaussian has two parameters: the “mean” and the “sd” (short for standard deviation). It’s straightforward to estimate values of the parameters from a graph, as in Figure 11.5.\n\n\n\n\n\n\nFigure 11.5: A Gaussian function annotated to identify the parameters mean (location of peak of graph) and sd (half-width at half-height).\n\n\n\nThe parameter “mean” is the location of the peak. The standard deviation is, roughly, half the width of the graph at a point halfway down from the peak."
  },
  {
    "objectID": "Modeling/04-fitting-by-eye.html#sinusoid",
    "href": "Modeling/04-fitting-by-eye.html#sinusoid",
    "title": "11  Fitting features",
    "section": "11.2 Sinusoid",
    "text": "11.2 Sinusoid\nWe’ll use three parameters for fitting a sinusoid to data: \\[A \\sin\\left(\\frac{2\\pi}{P}\\right) + B\\] where\n\n\\(A\\) is the “amplitude”\n\\(B\\) is the “baseline”\n\\(P\\) is the period.\n\n\n\n\n\n\n\nFigure 11.6: A reproduction of the data originally shown in Figure 8.3. The baseline for the sinusoid is midway between the top of the oscillation and the bottom.\n\n\n\nThe baseline for the sinusoid is the value mid-way between the top of the oscillations and the bottom. For example, Figure 8.3 (reproduced in the margin) shows the sinusoidal-like pattern of tide levels. Dashed horizontal lines (\\(\\color{brown}{\\text{brown}}\\)) have been drawn roughly going through the top of the oscillation and the bottom of the oscillation. The baseline (\\(\\color{magenta}{\\text{magenta}}\\)) will be halfway between these top and bottom levels.\nThe amplitude is the vertical distance between the baseline and the top of the oscillations. Equivalently, the amplitude is half the vertical distance between the top and the bottom of the oscillations.\nIn a pure, perfect sinusoid, the top of the oscillation—the peaks—is the same for every cycle, and similarly with the bottom of the oscillation—the troughs. The data in Figure 8.3 is only approximately a sinusoid so the top and bottom have been set to be representative. In Figure 11.6, the top of the oscillations is marked at level 1.6, the bottom at level 0.5. The baseline is therefore \\(B \\approx = (1.6 + 0.5)/2 = 1.05\\). The amplitude is \\(A = (1.6 - 0.5)/2 = 1.1/2 = 0.55\\).\nTo estimate the period from the data, mark the input for a distinct point such as a local maximum, then count off one or more cycles forward and mark the input for the corresponding distinct point for the last cycle. For instance, in Figure 11.6, the tide level reaches a local maximum at an input of about 6 hours, as marked by a black dotted line. Another local maximum occurs at about 106 hours, also marked with a black dotted line. In between those two local maxima you can count \\(n=8\\) cycles. Eight cycles in \\(106-6 = 100\\) hours gives a period of \\(P = 100/8 = 12.5\\) hours."
  },
  {
    "objectID": "Modeling/04-fitting-by-eye.html#sec-exponential-water",
    "href": "Modeling/04-fitting-by-eye.html#sec-exponential-water",
    "title": "11  Fitting features",
    "section": "11.3 Exponential",
    "text": "11.3 Exponential\nTo fit an exponential function, we estimate the three parameters: \\(A\\), \\(B\\), and \\(k\\) in \\[A \\exp(kt)+ B\\]\n\n\n\n\n\n\n\n\n\n\n\nExp. growth\nExp. decay\n.\n.\n\n\n\n\n\n\n.\n.\n\n\n\nExponential decay is a left-to-right flip of exponential growth.\nFigure 11.2 illustrates the procedure. The first question to ask is whether the pattern shown by the data resembles an exponential function. After all, the exponential pattern book function grows in output as the input gets bigger, whereas the water temperature is getting smaller—the word decaying is used—as time increases. To model exponential decay, use \\(\\exp(-k t)\\), where the negative sign effectively flips the pattern-book exponential left to right.\nThe exponential function has a horizontal asymptote for negative inputs. The left-to-right flipped exponential \\(\\exp(-k t)\\) also has a horizontal asymptote, but for positive inputs.\nThe parameter \\(B\\), again called the “baseline,” is the location of the horizontal asymptote on the vertical axis. Figure 11.2 suggests the asymptote is located at about 25 deg. C. Consequently, the estimated value is \\(B \\approx 25\\) deg C.\n\n11.3.1 Estimating A\nThe parameter \\(A\\) can be estimated by finding the value of the data curve at \\(t=0\\). In Figure Figure 11.7 that’s just under 100 deg C. From that, subtract off the baseline you estimated earlier: (\\(B = 25\\) deg C). The amplitude parameter \\(A\\) is the difference between these two: \\(A = 99 - 25 = 74\\) deg C.\n\n\n11.3.2 Estimating k\nThe exponential has a unique property of “doubling in constant time” as described in Section Section 5.2. We can exploit this to find the parameter \\(k\\) for the exponential function.\n\nThe procedure starts with your estimate of the baseline for the exponential function. In Figure 11.7 the baseline has been marked in \\(\\color{magenta}{\\text{magenta}}\\) with a value of 25 deg C.\nPick a convenient place along the horizontal axis. You want a place such that the distance of the data from the baseline to be pretty large. In Figure 11.7 the convenient place was selected at \\(t=25\\).\n\n\n\n\n\n\n\nFigure 11.7: Determining parameter \\(k\\) for the exponential function using the doubling time.\n\n\n\n\nMeasure the vertical distance from the baseline at the convenient place. In Figure 11.7 the data curve has a value of about 61 deg C at the convenient place. This is \\(61-25 = 36\\) deg C from the baseline.\nCalculate half of the value from (c). In Figure 11.7 this is \\(36/2=18\\) deg C. But you can just as well do the calculation visually, by marking half the distance from the baseline at the convenient place.\nScan horizontally along the graph to find an input where the vertical distance from the data curve to the baseline is the value from (d). In Figure 11.7 that half-the-vertical-distance input is at about \\(t=65\\). Then calculate the horizontal distance between the two vertical lines. In Figure 11.7 that’s \\(65 - 25 = 40\\) minutes. This is the doubling time. Or, you might prefer to call it the “half-life” since the water temperature is decaying over time.\nCalculate the magnitude \\(\\|k\\|\\) as \\(\\ln(2)\\) divided by the doubling time from (e). That doubling time is 40 minutes, so \\(\\|k\\|= \\ln(2) / 40 = 0.0173\\). We already know that the sign of \\(k\\) is negative since the pattern shown by the data is exponential decay toward the baseline. So, \\(k=-0.0173\\)."
  },
  {
    "objectID": "Modeling/04-fitting-by-eye.html#graphics-layers",
    "href": "Modeling/04-fitting-by-eye.html#graphics-layers",
    "title": "11  Fitting features",
    "section": "11.4 Graphics layers",
    "text": "11.4 Graphics layers\nWhen fitting a function to data, it’s wise to plot out the resulting function on top of the data. This involves making graphics with two layers, as described in Chapter Section 7. As a reminder, here’s an example comparing the cooling-water data to the exponential function we fitted in Section Section 11.3.\nThe fitted function we found was \\[T_{water}(t) \\equiv 74 \\exp(-0.0173 t) + 25\\] where \\(T\\) stands for “temperature.”\nTo compare \\(T_{water}()\\) to the data, we’ll first plot out the data with gf_point(), then add a slice plot of the function. We’ll also show a few bells and whistles of plotting: labels, colors, and such.\n\nT_water <- makeFun(74*exp(-0.0173*t) + 25 ~ t)\ngf_point(temp ~ time, data=CoolingWater, alpha=0.5 ) %>%\n  slice_plot(T_water(time) ~ time, color=\"blue\") %>%\n  gf_labs(x = \"Time (minutes)\", y=\"Temperature (deg. C)\")\n\n\n\n\n\nFigure 11.8: A graphic with two layers: one for the cooling-water data and the other with the exponential function fitted to the data.\n\n\n\nThe slice_plot() command inherited the domain interval from the gf_point() command. This happens only when the name of the input used in slice_plot() is the same as that in gf_point(). (It’s time in both.) You can add additional data or function layers by extending the pipeline.\nBy the way, the fitted exponential function is far from a perfect match to the data. We’ll return to this mismatch in Chapter Section 16 when we explore the modeling cycle."
  },
  {
    "objectID": "Modeling/04-fitting-by-eye.html#fitting-other-pattern-book-functions",
    "href": "Modeling/04-fitting-by-eye.html#fitting-other-pattern-book-functions",
    "title": "11  Fitting features",
    "section": "11.5 Fitting other pattern-book functions",
    "text": "11.5 Fitting other pattern-book functions\nThis chapter has looked at fitting the exponential, sinusoid, and Gaussian functions to data. Those are only three of the nine pattern-book functions. What about the others?\n\n\nShapes of the pattern-book functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst\nprop\nsquare\nrecip\ngaussian\nsigmoid\nsinusoid\nexp\nln\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Blocks ?sec-differentiation-part and ?sec-accumulation-part, you’ll see how the Gaussian and the sigmoid are intimately related to one another. Once you see that relationship, it will be much easier to understand how to fit a sigmoid to data.\nThe remaining five pattern-book functions, the ones we haven’t discussed in terms of fitting, are the logarithm and the four power-law functions included in the pattern-book set. In Chapter Section 14 we’ll introduce a technique for estimating from data the exponent of a single power-law function.\nIn high school, you may have done exercises where you estimated the parameters of straight-line functions and other polynomials from graphs of those functions. In professional practice, such estimations are done with an entirely different and completely automated method called regression. We’ll introduce regression briefly in Chapter Section 16. However, the subject is so important that all of Block ?sec-vectors-linear-combinations is devoted to it and its background."
  },
  {
    "objectID": "Modeling/04-fitting-by-eye.html#exercises",
    "href": "Modeling/04-fitting-by-eye.html#exercises",
    "title": "11  Fitting features",
    "section": "11.6 Exercises",
    "text": "11.6 Exercises"
  },
  {
    "objectID": "Modeling/04-fitting-by-eye.html#drill",
    "href": "Modeling/04-fitting-by-eye.html#drill",
    "title": "11  Fitting features",
    "section": "11.7 Drill",
    "text": "11.7 Drill\n\n\nPart 1 What’s the period of this sinusoid?\n12345\n\n\n\n\nPart 2 Which function(s) have \\(k < 0\\)?\nblueblackbothneither\n\n\n\n\nPart 3 Which function(s) have \\(k < 0\\)?\nblueblackbothneither\n\n\n\n\nPart 4 Which function(s) have \\(k < 0\\)?\nblueblackbothneither\n\n\n\n\nPart 5 One of the functions has a half-life, the other a doubling time. Which is bigger, the half-life or the doubling time?\n\ndoubling time\nhalf-life\nabout the same\nthey aren’t exponential, so the concept of half-life/doubling-time doesn’t apply.\n\n\n\n\n\nPart 6 In this book, what is meant by the word “variable”?\n\nIt’s the same as output.\nIt’s the same as input.\nA column in a data table."
  },
  {
    "objectID": "Modeling/05-low-order-polynomials.html",
    "href": "Modeling/05-low-order-polynomials.html",
    "title": "12  Low-order polynomials",
    "section": "",
    "text": "Chapter Section 11 looked at the task of modifying a pattern-book function to display a desired pattern, focusing on patterns originating in graphs of data. The procedure involved identifying an appropriate pattern-book function, then using input and output scaling to stretch, flip, and lift that function so that it overlays, as much as possible, the desired pattern.\nIn this chapter, we’ll take on a different strategy for constructing appropriately shaped functions using linear combinations of a handful of simple functions: the monomials."
  },
  {
    "objectID": "Modeling/05-low-order-polynomials.html#polynomials",
    "href": "Modeling/05-low-order-polynomials.html#polynomials",
    "title": "12  Low-order polynomials",
    "section": "12.1 Polynomials",
    "text": "12.1 Polynomials\nRecall that the monomials are the power-law functions with non-negative, integer exponents: \\(x^0\\), \\(x^1\\), \\(x^2\\), \\(x^3\\), and so on. The “and so on” refers to even higher integer exponents such as \\(x^4\\) or \\(x^{51}\\) or \\(x^{213}\\), to name but a few. The more common name for a linear combination of monomials is polynomial.\n\n\nThe first two terms in the polynomial \\(g(t)\\) could be written using exponents, like this: \\[ g(t) \\equiv a_0 t^0 + a_1 t^1 + \\cdots\\] In practice, nobody writes out explicitly the \\(t^0\\) function. Instead, recognizing that \\(t^0 = 1\\), we write the first term simply as \\(a_0\\). Similarly, rather than writing \\(t^1\\) in the second term, we write \\(a_1 t\\), without the exponent. This practice makes the formulas for polynomials more concise but at the cost of failing to remind the reader that all the functions in the linear combination are monomials.\nFor instance, a fifth-order polynomial consists of a linear combination of monomials up to order 5. That is, up to \\(x^5\\). This will have six terms because we count the order of the monomials starting with 0. \\[g(t) \\equiv a_0 + a_1 t + a_2 t^2 + a_3 t^3 + a_4 t^4 + a_5 t^5\\ .\\]\nThe challenge in shaping a polynomial is to find the scalar multipliers—usually called coefficients when it comes to polynomials—that give us the shape we want. This might seem to be a daunting task, and it is for a human. But it can easily be handled using volumes of arithmetic, too much arithmetic for a human to take on but ideally suited for computing machines."
  },
  {
    "objectID": "Modeling/05-low-order-polynomials.html#low-order-polynomial-models",
    "href": "Modeling/05-low-order-polynomials.html#low-order-polynomial-models",
    "title": "12  Low-order polynomials",
    "section": "12.2 Low-order polynomial models",
    "text": "12.2 Low-order polynomial models\nPolynomials in general can show a wide variety of snake-like patterns. A fifth-order polynomial can have up to four internal curves. A tenth-order polynomial can have 9 internal curves, and so. There is, however, rarely a need for generating functions with all those curves. Instead, a great deal of modeling work can be accomplished with just first-order polynomials (no internal curves) or second-order polynomials (one internal curve).\n\\[\\begin{eqnarray}\n\\textbf{First-order: }\\ \\ \\ \\ \\ & f_1(t) \\equiv b_0 + b_1 t\\\\\n\\textbf{Second-order: }\\ \\ \\ \\ \\ & f_2(t) \\equiv c_0 + c_1 t + c_2 t^2\n\\end{eqnarray}\\]\n\n\nNote that we’re using different names for the coefficients in each of the polynomial examples. The only significance of this is a reminder that each of the coefficients can be any number at all and isn’t necessarily related to any of the other coefficients. In addition to the usual \\(a\\), \\(b\\), \\(c\\), we’ve used the Greek alpha, beta, and gamma, that is \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\). The subscript on the coefficient name indicates which term it belongs to. For instance, the coefficient on the \\(y^2\\) term of the \\(h_c\\) polynomial is named \\(\\gamma_{yy}\\) while the coefficient on the \\(x y\\) term has the subscript \\(_{xy}\\). Always, the coefficients are constant quantities and not functions of \\(x\\) or any other input.\nIn high-school mathematics, polynomials are often written without subscript, for instance \\(a x^2 + b x + c\\). This can be fine when working with only one polynomial at a time, but in modeling we often need to compare multiple, related polynomials.\nYou may prefer to think about a first-order polynomial as a straight-line function. Similarly, a second-order polynomial is also known as a “quadratic” or even a “parabola.” Nonetheless, it’s good to see them as polynomials distinguished by their order. This puts them into a general framework, all of which can be handled by the technology of linear combinations. And polynomials can also involve more than one input. For instance, here are three polynomial forms that involve inputs \\(x\\) and \\(y\\):\n\\[\\begin{eqnarray}\nh_a(x, y) &\\equiv & \\alpha_0 + \\alpha_x\\, x + \\alpha_y\\, y\\\\\nh_b(x, y) &\\equiv & \\beta_0 + \\beta_x\\, x + \\beta_y\\, y + \\beta_{xy}\\, x y\\\\\nh_c(x, y) &\\equiv & \\gamma_0 + \\gamma_x\\, x + \\gamma_y\\, y + \\gamma_{xy}\\, x y + \\gamma_{xx}\\, x^2 + \\gamma_{yy}\\, y^2\n\\end{eqnarray}\\]\nThe reason to work with first- and second-order polynomials is rooted in the experience of modelers. Second-order polynomials provide a useful amount of flexibility while remaining simple and avoiding pitfalls."
  },
  {
    "objectID": "Modeling/05-low-order-polynomials.html#eight-simple-shapes",
    "href": "Modeling/05-low-order-polynomials.html#eight-simple-shapes",
    "title": "12  Low-order polynomials",
    "section": "12.3 Eight simple shapes",
    "text": "12.3 Eight simple shapes\nAn easy way to think about how to use low-order polynomials in modeling is to think about the shapes of their graphs. Figure 12.1 shows eight simple shapes for functions with a single input that occur often in modeling.\n\n\n\n\n\nFigure 12.1: The eight simple shapes of functions with one input.\n\n\n\n\nRecall that Chapter Section 6 introduced terms such as concavity, monotonicity, and slope for describing functions. To choose among these shapes, consider your modeling context:\n\nis the relationship positive (slopes up) or negative (slopes down)?\nis the relationship monotonic or not?\nis the relationship concave up, concave down, or neither?\n\nEach of the eight simple shapes corresponds to a particular set of answers to these equations. Consider these modeling contexts as examples:\n\nHow many minutes can you run as a function of speed? Concave down and downward sloping: Shape (F). In everyday terms, you wear out faster if you run at high speed.\nHow much fuel is consumed by an aircraft as a function of distance? For long flights, the function is concave up and positive sloping: Shape (D). In everyday terms: fuel use increases with distance, but the amount of fuel you have to carry also increases with distance. A heavy aircraft uses more fuel per mile.\nHow far can you walk as a function of time? Steep-then-shallow and concave down: Shape (E). Your pace slows as you get tired.\nHow does the stew taste as a function of saltiness? There’s a local maximum: Shape (H). The taste improves as the amount of salt increases … up to a point. Too much salt and the stew is unpalatable.\nThe incidence of an out-of-control epidemic versus time is concave up, but shallow-then-steep. As the epidemic is brought under control, the decline is steep-then-shallow and concave up. Over the whole course of an epidemic, there is a maximum incidence. Experience shows that epidemics can have a phase where incidence reaches a local minimum: a decline as people practice social distancing followed by an increase as people become complacent.\nIn micro-economic theory there are production functions that describe how much of a good is produced at any given price, and demand functions that describe how much of the good will be purchased as a function of price. As a rule, production increases with price and demand decreases with price.\nIn the short term, production functions tend to be concave down, since it’s hard to squeeze increased production out of existing facilities. Production functions are Shape (E).\n\nFor demand in the short term, functions will be concave up when there is some group of consumers who have no other choice than to buy the product. Downward sloping and concave up: Shape (C). In the long term, consumption functions can be concave down as consumers find alternatives to the high-priced good. For example, high prices of gasoline may, in the long term, prompt a switch to more efficient cars, hybrids, or electric vehicles. This will push demand down steeply.\n\n\nRemarkably, all the eight simple shapes can be generated by appropriate choices for the coefficients in a second-order polynomial: \\(g(x) = a_0 + a_1 x + a_2 x^2\\). So long as \\(a_2 \\neq 0\\), the graph of the second-order polynomial will be a parabola.\n\nThe parabola opens upward if \\(0 < a_2\\). That’s the shape of a local minimum.\nThe parabola opens downward if \\(a_2 < 0\\). That’s the shape of a local maximum\n\nConsider what happens if \\(a_2 = 0\\). The function becomes simply \\(a_0 + a_1\\, x\\), the straight-line function.\n\nWhen \\(0 < a_1\\) the line slopes upward.\nWhen \\(a_1 < 0\\) the line slopes downward.\n\nTo produce the steep-then-shallow or shallow-then-steep shapes, you also need to restrict the function domain to be on one side or another of the turning point of the parabola as shown in Figure 26.3.\n\n\n\n\n\nFigure 12.2: Four of the eight simple shapes correspond to the sides of the parabola. The labels refer to the graphs in Figure 12.1."
  },
  {
    "objectID": "Modeling/05-low-order-polynomials.html#sec-low-order-two",
    "href": "Modeling/05-low-order-polynomials.html#sec-low-order-two",
    "title": "12  Low-order polynomials",
    "section": "12.4 Polynomials with two inputs",
    "text": "12.4 Polynomials with two inputs\nFor functions with two inputs, the low-order polynomial approximation looks like this:\n\\[g(x, y) \\equiv a_0 + a_x x + a_y y + a_{xy} x y + a_{yy} y^2 + a_{xx} x^2\\]\nIt helps to have different names for the various terms. It’s not too bad to say something like, “the \\(a_{xy}\\) term.” (Pronunciation: “a sub x y” or “a x y”) But the proper names are: linear terms, quadratic terms, and interaction term. And a shout out to \\(a_0\\), the constant term.\n\\[g(x, y) \\equiv a_0 + \\underbrace{a_x x + a_y y}_\\text{linear terms} \\ \\ \\ +\n\\underbrace{a_{xy} x y}_\\text{interaction term} +\\ \\ \\  \\underbrace{a_{yy} y^2 + a_{xx} x^2}_\\text{quadratic terms}\\]\nThe interaction term arises in models of phenomena such as the spread of epidemics, the population dynamics of predator and prey animals, and the rates of chemical reactions. In each of these situations, one thing is interacting with another: a predator killing a prey animal, an infective individual meeting a person susceptible to the disease, one chemical compound reacting with another.\nUnder certain circumstances, modelers include one or both quadratic terms, as in \\[h_3(x, y) \\equiv c_0 + c_x\\, x + c_y\\, y + c_{xy}\\,x\\, y + \\underbrace{c_{yy}\\, y^2}_\\text{quadratic in y}\\] The skilled modeler can often deduce which terms to include from basic facts about the system being modeled. We’ll need some additional calculus concepts before we can explain this straightforwardly.\n\n\n\n\n\n\nA second-order polynomial with two inputs can take on any one of three shapes: a bowl, a hilltop, or a saddle.\n\n\n\n\n\nFigure 12.3: The three forms for a second-order polynomial with two inputs.\n\n\n\n\nOther shapes for modeling can be extracted from these three basic shapes. For example, the lower-right quadrant of the Saddle has the shape of seats in an amphitheater."
  },
  {
    "objectID": "Modeling/05-low-order-polynomials.html#theory-out-of-a-hat",
    "href": "Modeling/05-low-order-polynomials.html#theory-out-of-a-hat",
    "title": "12  Low-order polynomials",
    "section": "12.5 Theory out of a hat",
    "text": "12.5 Theory out of a hat\nThe start of Chapter Section 11 introduced a little mystery. Newton introduced his Law of Cooling in the 17th century: The rate at which an object cools depends on the difference in temperature between the object and its ambient environment. But in the 17th century, there was no precise way to measure a rate of temperature change. So how did Newton do it?\nEven with primitive thermometers, one can confirm that a mug of hot water will cool and a glass of cold water will warm to room temperature and stay there. So Newton could deduce that the rate of temperature change is zero when the object’s temperature is the same as the environment. Similarly, it’s easy to observe with a primitive thermometer that a big difference in temperature between an object and its environment produces a rapid change in temperature, even if you can’t measure the rate precisely. So the rate of cooling is a function of the temperature difference \\(\\Delta T\\) between object and environment.\nWhat kind of function?\nLow-order polynomials to the rescue! The simplest model is that the rate of cooling will be \\(a_0 + a_1 \\Delta T\\), a first-order polynomial. But we know that the rate of cooling is zero when \\(\\Delta T = 0\\), implying that \\(a_0=0\\). All that’s left is the first-order term \\(\\Delta T\\), which you can recognize as the proportional() function."
  },
  {
    "objectID": "Modeling/05-low-order-polynomials.html#exercises",
    "href": "Modeling/05-low-order-polynomials.html#exercises",
    "title": "12  Low-order polynomials",
    "section": "12.6 Exercises",
    "text": "12.6 Exercises"
  },
  {
    "objectID": "Modeling/05-low-order-polynomials.html#drill",
    "href": "Modeling/05-low-order-polynomials.html#drill",
    "title": "12  Low-order polynomials",
    "section": "12.7 Drill",
    "text": "12.7 Drill\n\n\nPart i In the polynomial \\(a_0 + a_x x + a_y y + a_{xy} xy\\), what is the term \\(a_{xy}xy\\) called?\nConstant term Quadratic termInteraction term Linear term\n\n\n\n\nPart ii In the polynomial \\(a_0 + a_x x + a_y y + a_{xx} xx\\), what is the coefficient on the interaction term?\n\\(a_0\\)\\(a_{xy}\\)0\\(a_{xx}\\)\n\n\n\n\nPart iii Imagine a second-order polynomial in three inputs: \\(x\\), \\(y\\), and \\(z\\), like this: \\[b_0 + b_x x + b_y y + b_z z + b_{xy} xy + b_{xz} xz + b_{xx} x^2 + b_{yy} y^2 + b_zz z^2\\ .\\] All of the possible second-order (or less) terms are shown, except for one. Which term is missing?\n\nthe constant term\nthe quadratic term in \\(z\\)\nthe interaction between \\(y\\) and \\(z\\)\nthe linear term in \\(y\\)"
  },
  {
    "objectID": "Modeling/06-operations.html",
    "href": "Modeling/06-operations.html",
    "title": "13  Operations on functions",
    "section": "",
    "text": "Chapters Section 8 through Section 11 introduced concepts and techniques for constructing functions. This is an important aspect of building models, but it is not the only one. Typically, a modeler, after constructing appropriate functions, will manipulate them in ways that provide the information required to answer questions that motivated the modeling work.\nThis will introduce some of the operations and manipulations used to extract information from model functions.1 There are five such operations that you will see many times throughout this book. They are:\nThis chapter will introduce the first three of these. The remaining two—differentiation and integration—are the core operations of calculus. They will be introduced starting in Block ?sec-differentiation.\nWe will look at the three operations from two different perspectives: graphical and computational. Often, a graph can let you carry out the operation with sufficient precision for the purpose at hand. Graphs are relatively modern, coming into mainstream use only in the 1700s. Much of mathematics was developed before graphs were invented. One consequence of this is that function tasks that are easy using a graph might be very hard with the previous algebraic ways of implementing functions.\nMore refined work is done with a computer. We’ll show you software that will let you direct the computer to do precise calculations. The computing algorithms used for inversion and optimization are often based on concepts from calculus that we have not yet encountered. The magic of software is the way it allows experts in a field to communicate with newbies so that people new to a field can use the operation in practice without necessarily developing a complete theoretical understanding of the algorithm. At this stage, our computational focus will be on how to set up the calculation and how to interpret the results."
  },
  {
    "objectID": "Modeling/06-operations.html#zero-finding",
    "href": "Modeling/06-operations.html#zero-finding",
    "title": "13  Operations on functions",
    "section": "13.1 Zero finding",
    "text": "13.1 Zero finding\nA function is a mechanism for turning any given input into an output. Zero finding is about going the other way: given an output value, find the corresponding input. As an example, consider the exponential function \\(e^x\\). Given a specific input, say \\(x=2.135\\) you can easily compute the corresponding output:\n\nexp(2.135)\n## [1] 8.457047\n\nBut suppose the information you have at hand is in the form of an output from the function, say \\(e^{x_0} = 4.93\\). We don’t (yet) know \\(x_0\\) but, whatever it might be, we know that \\(e^{x_0}\\) will produce the value 4.93.\nHow do you find the specific input \\(x_0\\) that will produce that output? The answer typically presented in high school is to apply another function, \\(\\ln()\\), to the output:\n\nlog(4.93)\n## [1] 1.595339\n\nTo confirm that the result 1.595339 is correct, apply the exponential function to it and check that the output is the same as the original, given output 4.93.\n\nexp(1.595339)\n## [1] 4.93\n\nThis process works because we happen to have a function at hand, the logarithm, that is perfectly set up to “undo” the action of the exponential function. In high school, you learned a handful of function/inverse pairs: exp() and log() as you’ve just seen, sin() and arcsin(), square and square root, etc.\nAnother situation that is usually addressed in high school is inverting low-order polynomial functions. For instance, suppose your modeling function is \\(g(x) \\equiv 1.7 - 0.85 x + 0.063 x^2\\) and you seek the \\(x_0\\) such that \\(g(x_0) = 3\\). High school students are taught to approach such problems in a process using the quadratic formula. In order to apply the quadratic formula, you need to place the problem into a standard format, not \\[1.7 - 0.85 x + 0.063 x^2 = 3\\] but \\[0.063\\, x^2 - 0.85\\, x - 1.4 = 0\\]\nOne reason that low-order polynomials are popular in modeling is that such operations are straightforward.\nIf none of the high-school approaches are suited to your modeling function, as is often the case, you can still carry out the zero-finding operation.\n\n13.1.1 Graphical zero-finding\n\n\n\n\n\n\nFigure 13.1: Finding an \\(x_0\\) such that \\(h(x_0) = 3\\)\n\n\n\nSuppose you may have a function \\(h(x)\\) that you constructed by linear combination and/or function multiplication and/or function composition. To illustrate, we’ll work with the function \\(h(x)\\) graphed in Figure 13.1. And suppose the output for which we want to find a corresponding input is 3, that is, we want to find \\(x_0\\) such that \\(h(x_0)=3\\).\nThe steps of the process are:\n\n\nThe name “zero-finding” can be a little misleading. The objective is find \\(x_0\\) such that \\(h(x_0) = b\\). In this sense, “b-finding” would be a more appropriate name. Instead of chasing after honey as “b-finding” suggests, we reformat the problem into finding \\(x_0\\) such that \\(h(x_0) - b = 0\\). In other words, we look for zeros of the function \\(h(x) - b\\).\n\nGraph the function \\(h(x)\\) over a domain interval of interest.\nDraw a horizontal line located at the value on the right-hand side of the equation \\(h(x_0) = 3\\). (This is the \\(\\color{magenta}{\\text{magenta}}\\) line in Figure 13.1.)\nFind the places, if any, where the horizontal line intersects the graph of the function. In Figure 13.1, there are two such values: \\(x_0 = -3.5\\) or \\(x_0 = 2.75\\).\n\n\nThe graph shows a function \\(g(t)\\). Find a value \\(t_0\\) such that \\(g(t_0) = 5\\).\n\n\n\n\n\n\n\n\n\n\nDraw a horizontal line at output level 5.\nFind the t-value where the horizontal line intersects the function graph. There’s only one such intersection and that’s at about \\(t=1.2\\).\n\nConsequently, \\(t_0 = 1.2\\), at least to the precision possible when reading a graph.\n\nThe graphical approach to zero finding is limited by your ability to locate positions on the vertical and horizontal axis. If you need more precision than the graph provides, you have two options:\n\nTake a step-by-step approach. Use the graph to locate a rough value for the result. Then refine that answer by drawing another graph, zooming in on a small region around the result from the first step. You can iterate this process, repeatedly zooming in on the result you got from the previous step.\nUse software implementing a numerical zero-finding algorithm. Such software is available in many different computer languages and a variety of algorithms is available, each with its own merits and demerits.\n\n\n\n13.1.2 Numerical zero finding\nIn this book, we will use the R/mosaic Zeros() function. The first argument is a tilde expression and the second argument an interval of the domain over which to search.\nZeros() is set up to find inputs where the function defined in the tilde expression produces zero as an output. But suppose you are dealing with a problem like \\(f(x) = 10\\)? You can modify the tilde expression so that it implements a slightly different function: \\(f(x) - 10\\). If we can find \\(x_0\\) such that \\(f(x_0) - 10 = 0\\), that will also be the \\(x_0\\) satisfying \\(f(x_0) = 10\\).\n\nThe point of this example is to show how to use Zeros(), so we’ll define a function \\(f(x)\\) using rfun() from R/mosaic. This constructs a function by taking a linear combination of other functions selected at random. The argument seed=579 determines which functions will be in the linear combination.\n\nf <- rfun( ~ x, seed=579)\n\nWe want to find the zeros of the function \\(f(x) - 10\\) which corresponds to solving \\(f(x) = 10\\).\n\nZeros(f(x) - 10 ~ x, domain(x=-4:4))\n## # A tibble: 0 × 2\n## # … with 2 variables: x <dbl>, .output. <dbl>\n\nThe output produced by Zeros() is a data frame with one row for each of the \\(x_0\\) found. Here, two values were found: \\(x_0 = -2.92\\) and \\(x_0 = 0.0795\\). The .output column reports \\(f(x_0)\\) which should be zero. It’s not always feasible to reach zero exactly, since computer arithmetic is not always exactly precise.\nThink about Zeros() as a way to refine answers you found graphically. So before using Zeros(), make the graph. ::: {.cell .column-margin layout-align=“center” fig.showtext=‘false’}\nslice_plot(f(x) ~ x, domain(x=-4:4)) %>%\n  gf_hline(yintercept = ~ 10, color=\"magenta\")\n\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "Modeling/06-operations.html#optimization",
    "href": "Modeling/06-operations.html#optimization",
    "title": "13  Operations on functions",
    "section": "13.2 Optimization",
    "text": "13.2 Optimization\nOptimization problems consist of both a modeling phase and a solution phase.\n\n13.2.1 Graphical optimization\nLook for local peaks, then read off the input that generates the value at the peak.\n\n\n13.2.2 Numerical optimization\nWhen it comes to functions, maximization is the process of finding an input to the function that produces a larger output than any of the other, nearby inputs.\nTo illustrate, Figure 13.2 shows a function with two peaks.\n\n\n\n\n\nFigure 13.2: a function with two peaks\n\n\n\n\nJust as you can see a mountain top from a distance, so you can see where the function takes on its peak values. Draw a vertical line through each of the peaks. The input value corresponding to each vertical line is called an argmax, short for “the argument2 at which the function reaches a local maximum value.\nMinimization refers to the same technique, but where the vertical lines are drawn at the deepest point in each “valley” of the function graph. An input value located in one of those valleys is called an argmin.\nOptimization is a general term that covers both maximization and minimization.\n\n\n13.2.3 Numerical optimization\nThe R/mosaic argM() function a functions argmax and argmin over a given domain. It works in exactly the same way as slice_plot(), but rather than drawing a graphic it returns a data frame giving the argmax in one row and the argmin in another. For instance, the function shown in Figure 13.2 is \\(h()\\), generated by rfun():\n\nh <- rfun(~ x, seed=7293)\nargM(h(x) ~ x, domain(x=-5:5))\n## # A tibble: 2 × 3\n##        x .output. concavity\n##    <dbl>    <dbl>     <dbl>\n## 1 -1.68      1.93         1\n## 2  0.173     8.25        -1\n\nThe x column holds the argmax and argmin, the .output. column gives the value of the function output for the input x. The concavity column tells whether the function’s concavity at x is positive or negative. Near a peak, the concavity will be negative; near a valley, the concavity is positive. Consequently, you can see that the first row of the data frame corresponds to a local minimum and the second row is a local maximum.\nargM() is set up to look for a single argmax and a single argmin in the domain interval given as the second argument. In Figure 13.2 there are two local peaks and two local valleys. argM() gives only the largest of the peaks and the deepest of the valleys."
  },
  {
    "objectID": "Modeling/06-operations.html#iteration",
    "href": "Modeling/06-operations.html#iteration",
    "title": "13  Operations on functions",
    "section": "13.3 Iteration",
    "text": "13.3 Iteration\nMany computations involve starting with a guess followed by a step-by-step process of refining the guess. A case in point is the process for calculating square roots. There isn’t an operational formula for a function that takes a number as an input and produces the square root of that number as the output. When we write \\(\\sqrt{\\strut x}\\) we aren’t saying how to calculate the output, just describing the sort of output we are looking for.\nThe function that is often used to calculate \\(\\sqrt{x}\\) is better():\n\\[\\text{better(guess)} = \\frac{1}{2}\\left( \\text{guess} + \\frac{x}{\\text{guess}}\\right)\\ .\\]\nIt may not be at all clear why this formula is related to finding a square root. Let’s put that matter off until the end of the section and concentrate our attention on how to use it.\nTo start, let’s define the function for the computer:\n\nbetter <- makeFun((guess + x/guess)/2 ~ guess)\n\nNotice that \\(x\\) is cast in the role of a parameter of the function rather than an input to the function.\nSuppose we want to apply the square root function to the input 55, that is, calculate \\(\\sqrt{\\strut x=55}\\). The value we should assign to \\(x\\) is therefore 55.\nTo calculate better(guess) we need not only \\(x=55\\) but a value for the guess. What should be this value and what will we do with the quantity better(guess) when we’ve calculated it.\nWithout explanation, we’ll use guess = 1, regardless of the value of \\(x\\). Calculating the output …\n\nbetter(1, x=55)\n## [1] 28\n\nNeither our guess 1 nor the output 28 are \\(\\sqrt{\\strut x=55}\\). (Having long-ago memorized the squares of integers, we know \\(\\sqrt{\\strut x=55}\\) will be somewhere between 7 and 8. Neither 1 nor 28 are in that interval.)\nThe people—more than two thousand years ago—who invented the ideas behind the better() function were convinced that better() constructs a better guess for the answer we seek. It’s not obvious why 28 should be a better guess than 1 for \\(\\sqrt{\\strut x=55}\\) but, out of respect, let’s accept their claim.\nThis is where iteration comes in. Even if 28 is a better guess than 1, 28 is still not a good guess. But we can use better() to find something better than 28:\n\nbetter(28, x=55)\n## [1] 14.98214\n\nTo iterate an action means to perform that action over and over again. (“Iterate” stems from the Latin word iterum, meaning “again.”) A bird iterates its call, singing it over and over again. In mathematics, “iterate” has a twist. When we repeat the mathematical action, we will draw on the results of the previous angle rather than simply repeating the earlier calculation.\nWe’ll continue our iteration of better(). ::: {.cell layout-align=“center” fig.showtext=‘false’}\nbetter(14.98214, x=55)\n## [1] 9.326589\nbetter(9.326589, x=55)\n## [1] 7.611854\nbetter(7.611854, x=55)\n## [1] 7.418713\nbetter(7.418713, x=55)\n## [1] 7.416199\nbetter(7.416199, x=55)\n## [1] 7.416198\n::: In the last step, the output of better() is practically identical to the input, so no reason to continue. We can confirm that the last output is a good guess for \\(\\sqrt{\\strut x=55}\\): ::: {.cell layout-align=“center” fig.showtext=‘false’}\n7.416198^2\n## [1] 54.99999\n:::\n\n13.3.1 Graphical iteration\n\nggcobweb <- function(tilde, domain, x0, n=5) {\n  myarrow = grid::arrow(ends=\"last\", length=unit(.1, \"cm\"),\n                        type=\"closed\")\n  f <- makeFun(tilde)\n  Seq <- Iterate(f, x0=1, n=n)\n  names(Seq) <- c(\"step\", \"xstart\")\n  Seq <- Seq %>% \n    mutate(xend=lead(xstart))\n  \n    \n    gf_abline(intercept = ~ 0, slope = 1, color=\"blue\", linetype = \"dotted\") %>%\n    gf_refine(coord_fixed()) %>%\n    gf_segment(xstart + xend ~ xstart + xstart, data= Seq, color=\"magenta\", arrow = myarrow, inherit=FALSE) %>%\n    gf_segment(xend+xend ~ xstart + xend, data = Seq, color=\"magenta\", arrow = myarrow) %>%\n    slice_plot(f(x) ~ x, domain, npts=500)  %>%\n    gf_lims(y=range(unlist(domain))) \n}\n\nTo iterate graphically, we graph the function to be iterated and mark the initial guess on the horizontal axis. For each iteration step, trace vertically from the current point to the function, then horizontally to the line of identity (blue dots). The result will be the starting point for the next guess.\n\n\n\n\n\nFigure 13.3: Three steps of iteration of better() starting with an initial guess of 1.\n\n\n\n\n\n\n13.3.2 Numerical iteration\nUse the R/mosaic Iterate() function. The first argument is a tilde expression defining the function to be iterated. The second is the starting guess. The third is the number of iteration steps. For instance:\n\nIterate(better(guess, x=55) ~ guess, x0=1, n=8)\n##   n     guess\n## 1 0  1.000000\n## 2 1 28.000000\n## 3 2 14.982143\n## 4 3  9.326590\n## 5 4  7.611854\n## 6 5  7.418713\n## 7 6  7.416199\n## 8 7  7.416198\n## 9 8  7.416198\n\nThe output produced by Iterate() is a data frame. The initial guess is in the row with \\(n=0\\). Successive rows give the output, step by step, with each new iteration step.\n\nWhere does better() come from?\nFor calculating square roots, we used the function \\[\\text{better}(y) = \\frac{1}{2}\\left( y + \\frac{x}{y}\\right)\\ .\\] Let’s suppose you happened on a guess that is exactly right, that is \\(y = \\sqrt{x}\\). There’s no way to improve on a guess that is exactly right, so the best better() can do is to return the guess unaltered. Indeed it does: \\[\\text{better}\\left(y=\\!\\!\\sqrt{\\strut x}\\ \\right) = \\frac{1}{2}\\left( \\sqrt{\\strut x} + \\frac{x}{\\sqrt{x}} \\right)\\ = \\frac{1}{2}\\left(\\sqrt{\\strut x} + \\sqrt{\\strut x}\\right) = \\sqrt{\\strut x}.\\]\nOf course, the initial guess \\(y\\) might be wrong. There are two ways to be wrong:\n\nThe guess is too small, that is \\(y < \\sqrt{\\strut x}\\).\nThe guess is too big, that is \\(\\sqrt{\\strut x} < y\\).\n\nThe formula for better() is the average of the guess \\(y\\) and another quantity \\(x/y\\). If \\(y\\) is too small, then \\(x/y\\) must be too big. If \\(y\\) is too big, then \\(x/y\\) must be too small.\nAs guesses, the two quantities \\(y\\) and \\(x/y\\) are equivalent in the sense that \\(\\text{better}(y) = \\text{better}(x/y)\\). The average of \\(y\\) and \\(x/y\\) will be closer to the true result than the worst of \\(y\\) or \\(x/y\\); the average will be a better guess."
  },
  {
    "objectID": "Modeling/06-operations.html#functions-with-multiple-inputs-draft",
    "href": "Modeling/06-operations.html#functions-with-multiple-inputs-draft",
    "title": "13  Operations on functions",
    "section": "13.4 Functions with multiple inputs [DRAFT]",
    "text": "13.4 Functions with multiple inputs [DRAFT]\nDO I NEED THIS???\nSHOW A contour plot: find the zeros and extrema graphically.\nWe can’t readily graph functions with three or more inputs. So zero finding or optimization with such functions requires other methods. The calculus concepts and tools we’ll study in Block ?sec-differentiation-part will provide the basis for these methods."
  },
  {
    "objectID": "Modeling/06-operations.html#exercises",
    "href": "Modeling/06-operations.html#exercises",
    "title": "13  Operations on functions",
    "section": "13.5 Exercises",
    "text": "13.5 Exercises"
  },
  {
    "objectID": "Modeling/07-magnitudes.html",
    "href": "Modeling/07-magnitudes.html",
    "title": "14  Magnitude",
    "section": "",
    "text": "People accomplish a familiar mathematical task with hardly any mental effort: comparing two numbers to determine which is bigger. This is easy because we have adopted a system for writing numbers that makes it easy. For the Romans and Europeans up through the 13th century, numbers were hard to work with. For instance, which of these three numbers is bigger? \\[\\text{MLI or CXII or XXXIII}\\]\nNow try the same task with Arabic numerals: Which is bigger?\n\\[\\text{512 or 33 or 1051}\\] You can see the answer at a glance. The algorithm is straightforward: select the number with the largest number of digits. If there is a tie, refer to the first digit. If there is still a tie, refer to the next digit. In contrast, it takes much more work with Roman numerals. For instance, IC is about fifteen times bigger than VI, even though I is much smaller than V."
  },
  {
    "objectID": "Modeling/07-magnitudes.html#order-of-magnitude",
    "href": "Modeling/07-magnitudes.html#order-of-magnitude",
    "title": "14  Magnitude",
    "section": "14.1 Order of magnitude",
    "text": "14.1 Order of magnitude\nWe’ll refer to judging the size of numbers by their count of digits as reading the magnitude of the number. To get started, consider numbers that start with 1 followed by zeros, e.g. 100 or 1000. We’ll quantify the magnitude as the number of zeros: 100 has a magnitude of 2 and 1000 has a magnitude of 3. In comparing numbers by magnitude, we way things like, “1000 is an order of magnitude greater than 100,” or “1,000,000” is five orders of magnitude larger than 10.\nMany phenomena and quantities are better understood in terms of magnitude than in terms of number. An example: Animals, including humans, go about the world in varying states of illumination, from the bright sunlight of high noon to the dim shadows of a half-moon. To be able to see in such diverse conditions, the eye needs to respond to light intensity across many orders of magnitude.\nThe lux is the unit of illuminance in the Système international. This table1 shows the illumination in a range of familiar outdoor settings:\n\n\n\nIlluminance\nCondition\n\n\n\n\n110,000 lux\nBright sunlight\n\n\n20,000 lux\nShade illuminated by entire clear blue sky, midday\n\n\n1,000 lux\nTypical overcast day, midday\n\n\n400 lux\nSunrise or sunset on a clear day (ambient illumination)\n\n\n0.25 lux\nA full Moon, clear night sky\n\n\n0.01 lux\nA quarter Moon, clear night sky\n\n\n\nFor a creature active both night and day, the eye needs to be sensitive over 7 orders of magnitude of illumination. To accomplish this, eyes use several mechanisms: contraction or dilation of the pupil accounts for about 1 order of magnitude, photopic (color, cones) versus scotopic (black-and-white, rods, nighttime) covers about 3 orders of magnitude, adaptation over minutes (1 order), squinting (1 order).\nMore impressively, human perception of sound spans more than 16 orders of magnitude in terms of the energy impinging on the eardrum. The energy density of perceptible sound ranges from the threshold of hearing at 0.000000000001 Watt per square meter to a conversational level of 0.000001 W/m2 to 0.1 W/m2 in the front rows of a rock concert. But in terms of our subjective perception of loudness, each order of magnitude change is perceived in the same way, whether it be from street traffic to vacuum cleaner or from whisper to normal conversation. (The unit of sound measurement is the decibel (dB), with 10 decibels corresponding to an order of magnitude in the energy density of sound.)\n\n\n\nEnergy density of sound in various situations. Sound at 85 dB, for extended periods, can cause permanent hearing loss. Exposure to sound at 120 dB over 30 seconds is dangerous.\n\n\n\n\n\n\nSituation\nEnergy level (dB)\n\n\n\n\nRustling leaves\n10 dB\n\n\nWhisper\n20 dB\n\n\nMosquito buzz\n40 dB\n\n\nNormal conversation\n60 dB\n\n\nBusy street traffic\n70 dB\n\n\nVacuum cleaner\n80 dB\n\n\nLarge orchestra\n98 dB\n\n\nEarphones (high level)\n100 dB\n\n\nRock concert\n110 dB\n\n\nJackhammer\n130 dB\n\n\nMilitary jet takeoff\n140 dB\n\n\n\n\n\n6, 60, 600, and 6000 miles-per-hour are quantities that differ in size by orders of magnitude. Such differences often point to a substantial change in context. A jog is 6 mph, a car on a highway goes 60 mph, a cruising commercial jet goes 600 mph, and a rocket passes through 6000 mph on its way to orbital velocity. From an infant’s crawl to highway cruising is 2 orders of magnitude in speed.\nOf course, many phenomena are not well represented in terms of orders of magnitudes. For example, the difference between normal body temperature and high fever is 0.01 orders of magnitude in temperature.2 An increase of 1 order of magnitude in blood pressure from the normal level would cause instant death! The difference between a very tall adult and a very short adult is about 1/4 of an order of magnitude.\nOrders of magnitude are used when the relevant comparison is a ratio. “A car is 10 times faster than a person,” refers to the ratio of speeds. In contrast, quantities such as body temperature, blood pressure, and adult height are compared using a difference. Fever is 2\\(^circ\\)C higher in temperature than normal. A 30 mmHg increase in blood pressure will likely correspond to developing hypertension. A very tall and a very short adult differ by about 2 feet.\nOne clue that thinking in terms of orders of magnitude is appropriate is when you are working with a set of objects whose range of sizes spans one or many factors of 2. Comparing baseball and basketball players? Probably no need for orders of magnitudes. Comparing infants, children, and adults in terms of height or weight? Orders of magnitude may be useful. Comparing bicycles? Mostly they fit within a range of 2 in terms of size, weight, and speed (but not expense!). Comparing cars, SUVs, and trucks? Differences by a factor of 2 are routine, so thinking in terms of order of magnitude is likely to be appropriate.\nAnother clue is whether “zero” means “nothing.” Daily temperatures in the winter are often near “zero” on the Fahrenheit or Celcius scales, but that in no way means there is a complete absence of heat. Those scales are arbitrary. Another way to think about this clue is whether negative values are meaningful. If so, thinking in terms of orders of magnitude is not likely to be useful."
  },
  {
    "objectID": "Modeling/07-magnitudes.html#counting-digits",
    "href": "Modeling/07-magnitudes.html#counting-digits",
    "title": "14  Magnitude",
    "section": "14.2 Counting digits",
    "text": "14.2 Counting digits\nImagine having a digit counting function called digit(). It takes a number as input and produces a number as output. We don’t have a formula for digit(), but for some inputs, the output can be calculated just by counting. For example:\n\ndigit(10) \\(\\equiv\\) 1\ndigit(100) \\(\\equiv\\) 2\ndigit(1000) \\(\\equiv\\) 3\n… and so on …\ndigit(1,000,000) \\(\\equiv\\) 6\n… and on.\n\nThe digit() function easily can be applied to the product of two numbers. For instance:\n\ndigit(1000 \\(\\times\\) 100) = digit(1000) + digit(100) = 3 + 2 = 5.\n\nSimilarly, applying digit() to a ratio gives the difference of the digits of the numerator and denominator, like this:\n\ndigit(1,000,000 \\(\\div\\) 10) = digit(1,000,000) - digit(10) = 6 - 1 = 4\n\nIt is not clear that \\(\\ln()\\) is a better user interface as a pattern-book function than digit(), or, as it is written, \\(\\log_{10}()\\) and log10(). People find it much easier to count by magnitudes of 10 than by the natural logarithm’s 2.718282….\nIn practice, digit() is so useful that it could well have been one of our basic modeling functions: \\[\\text{digit(x)} = 2.302585 \\ln(x)\\] or, in R, log10(). We elected the natural logarithm \\(\\ln()\\) rather than digit() for reasons that will be seen when we study differentiation.\nYou may have guessed that digits() is handy for computing differences in terms of orders of magnitude. Here’s how:\n\nMake sure that the quantities are expressed in the same units.\nCalculate the difference between the digits() of the numerical part of the quantity.\n\n\nWhat is the order-of-magnitude difference in velocity between a snail and a walking human? A snail slides at about 1 mm/sec, a human walks at about 5 km per hour. Putting human speed in the same units as snail speed: \\[\\begin{eqnarray}5 \\frac{km}{hr} = \\left[\\frac{1}{3600} \\frac{hr}{sec}\\right] 5 \\frac{km}{hr} &=& \\\\\n\\left[10^6 \\frac{mm}{km}\\right] \\left[\\frac{1}{3600} \\frac{hr}{sec}\\right] 5 \\frac{km}{hr} &=& 1390 \\frac{mm}{sec}\n\\end{eqnarray}\\] Calculating the difference in digits() between 1 and 1390:\n\nlog10(1390) - log10(1)\n## [1] 3.143015\n\nSo, about 3 orders of magnitude difference in speed. To a snail, we walking humans must seem like rockets on their way to orbit!\n\nThe use of factors of 10 in counting orders of magnitude is arbitrary. A person walking and a person jogging are on the edge of being qualitatively different, although their speeds differ by a factor of only 2. Aircraft that cruise at 600 mph and 1200 mph are qualitatively different in design, although the speeds are only a factor of 2 apart. A professional basketball player (height 2 meters or more) is qualitatively different from a third grader (height about 1 meter)."
  },
  {
    "objectID": "Modeling/07-magnitudes.html#sec-magnitude-graphics",
    "href": "Modeling/07-magnitudes.html#sec-magnitude-graphics",
    "title": "14  Magnitude",
    "section": "14.3 Magnitude graphics",
    "text": "14.3 Magnitude graphics\nTo display a variable from data that varies over multiple orders of magnitude, it helps to plot the logarithm rather than the variable itself. Let’s illustrate using the Engine data frame, which contains measurements of many different internal combustion engines of widely varying sizes. For instance, we can graph engine RPM (revolutions per second) versus engine mass, as in Figure 14.2.\n\ngf_point(RPM ~ mass, data = Engines)\n\n\n\n\n\nFigure 14.2: Engine RPM versus mass for 39 different enginges plotted on the standard linear axis.\n\n\n\nIn the graph, most of the engines have a mass that is … zero. At least that’s what it appears to be. The horizontal scale is dominated by the two huge 100,000-pound monster engines plotted at the right end of the graph.\nPlotting the logarithm of the engine mass spreads things out, as in Figure 14.3.\n\ngf_point(RPM ~ mass, data = Engines) %>%\n  gf_refine(scale_x_log10())\n\n\n\n\n\nFigure 14.3: Engine RPM versus mass on semi-log axes.\n\n\n\nNote that the horizontal axis has been labeled with the actual mass (in pounds), with the labels evenly spaced in terms of their logarithm. This presentation, with the horizontal axis constructed this way, is called a semi-log plot.\nWhen both axes are labeled this way, we have a log-log plot, as shown in Figure 14.4.\n\ngf_point(RPM ~ mass, data = Engines) %>%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n    )\n\n\n\n\n\nFigure 14.4: Engine RPM versus mass on log-log axes.\n\n\n\nSemi-log and log-log axes are widely used in science and economics, whenever data spanning several orders of magnitude need to be displayed. In the case of the engine RPM and mass, the log-log axis shows that there is a graphically simple relationship between the variables. Such axes are very useful for displaying data but can be hard for the newcomer to read quantitatively. For example, calculating the slope of the evident straight-line relationship in Figure 14.4 is extremely difficult for a human reader and requires translating the labels into their logarithms.\n\nRobert Boyle (1627-1691) was a founder of modern chemistry and the scientific method in general. As any chemistry student already knows, Boyle sought to understand the properties of gasses. His results are summarized in Boyle’s Law.\nThe data frame Boyle contains two variables from one of Boyle’s experiments as reported in his lab notebook: pressure in a bag of air and volume of the bag. The units of pressure are mmHg and the units of volume are cubic inches.3\nFamously, Boyle’s Law states that, at a constant temperature, the pressure of a constant mass of gas is inversely proportional to the volume occupied by the gas. Figure 14.5 shows a cartoon of the relationship.\n\n\n\n\n\nFigure 14.5: A cartoon illustrating Boyle’s Law. Source: NASA Glenn Research Center\n\n\n\n\nFigure 14.6 plots out Boyle’s actual experimental data. I\n\ngf_point(pressure ~ volume, data = Boyle) %>%\n  gf_lm()\n\n\n\n\n\nFigure 14.6: A plot of Boyle’s pressure vs volume data on linear axes. The straight line model is a poor representation of the pattern seen in the data.\n\n\n\nYou can see a clear relationship between pressure and volume, but it’s hardly a linear relationship.\nPlotting Boyle’s data on log-log axes reveals that, in terms of the logarithm of pressure and the logarithm of volume, the relationship is linear.\n\ngf_point(log(pressure) ~ log(volume), data = Boyle) %>%\n  gf_lm()\n\n\n\n\n\nFigure 14.7: Plotting the logarithm of pressure against the logarithm of volume reveals a straight-line relationship.\n\n\n\n\nFigure 14.7 shows that Boyle’s log-pressure and log-volume data are a straight-line function. In other words:\n\\[\\ln(\\text{Pressure}) = a + b \\ln(\\text{Volume})\\]\nYou can find the slope \\(b\\) and intercept \\(a\\) from the graph. For now, we want to point out the consequences of the straight-line relationship between logarithms.\nExponentiating both sides gives \\[e^{\\ln(\\text{Pressure})} = \\text{Pressure} = e^{a + b \\ln(\\text{Volume})} = e^a\\  \\left[e^{ \\ln(\\text{Volume})}\\right]^b = e^a\\, \\text{Volume}^b\\] or, more simply (and writing the number \\(e^a\\) as \\(A\\))\n\\[\\text{Pressure} = A\\,  \\text{Volume}^b\\] A power-law relationship!"
  },
  {
    "objectID": "Modeling/07-magnitudes.html#sec-reading-log-axes",
    "href": "Modeling/07-magnitudes.html#sec-reading-log-axes",
    "title": "14  Magnitude",
    "section": "14.4 Reading logarithmic scales",
    "text": "14.4 Reading logarithmic scales\nPlotting the logarithm of a quantity gives a visual display of the magnitude of the quantity and labels the axis as that magnitude. A useful graphical technique is to label the axis with the original quantity, letting the position on the axis show the magnitude.\nTo illustrate, ?fig-mag-scales-1(left) is a log-log graph of horsepower versus displacement for the internal combustion engines reported in the Engines data frame. The points are admirably evenly spaced, but it is hard to translate the scales to the physical quantity. The right panel in ?fig-mag-scales-1 shows the same data points, but now the scales are labeled using the original quantity.\n\ngf_point(log(BHP) ~ log(displacement), data = Engines)\ngf_point(BHP ~ displacement, data = Engines) %>%\n  gf_refine(scale_y_log10(), scale_x_log10()) \n\n\n\n\nFigure 14.8: Horsepower versus displacement from the Engines data.frame plotted with log-log scales.\n\n\n\n\n\n\n\nFigure 14.9: Horsepower versus displacement from the Engines data.frame plotted with log-log scales.\n\n\n\n\nThe tick marks on the vertical axis in the left pane are labeled for 0, 2.5, 5.0, 7.5, and 10. That doesn’t refer to the horsepower itself, but to the logarithm of the horsepower. The right pane has tick labels that are in horsepower at positions marked 1, 10, 100, 1000, and 10000.\nSuch even splits of a 0-100 scale are not appropriate for logarithmic scales. One reason is that 0 cannot be on a logarithmic scale in the first place since \\(\\log(0) = -\\infty\\).\nAnother reason is that 1, 3, and 10 are pretty close to an even split of a logarithmic scale running from 1 to 10. It’s something like this:\n1              2            3          5            10     x\n|----------------------------------------------------|\n0               1/3         1/2        7/10          1     log(x)\nIt’s nice to have the labels show round numbers. It’s also nice for them to be evenly spaced along the axis. The 1-2-3-5-10 convention is a good compromise; almost evenly separated in space yet showing simple round numbers."
  },
  {
    "objectID": "Modeling/07-magnitudes.html#exercises",
    "href": "Modeling/07-magnitudes.html#exercises",
    "title": "14  Magnitude",
    "section": "14.5 Exercises",
    "text": "14.5 Exercises"
  },
  {
    "objectID": "Modeling/07-magnitudes.html#drill",
    "href": "Modeling/07-magnitudes.html#drill",
    "title": "14  Magnitude",
    "section": "14.6 Drill",
    "text": "14.6 Drill\n\n\nPart i What is the correct form for the relationship shown in the graph? \n\n\\(g(x) \\equiv e^{-1.5} e^{10 x}\\)\n\\(g(x) \\equiv 10 e^{-2 x}\\)\n\\(g(x) \\equiv e^{10} e^{-1.5 x}\\)\n\\(g(x) \\equiv 10 x^{-1.5}\\)\n\\(g(x) \\equiv e^{10} e^{-2 x}\\)\n\n\n\n\n\nPart ii What is the correct form for the relationship shown in the graph? \n\n\\(g(x) \\equiv e^{10} x^{-2}\\)\n\\(g(x) \\equiv 10 x^{-2}\\)\n\\(g(x) \\equiv e^{10} e^{-1.5 x}\\)\n\\(g(x) \\equiv e^10 x^{-1.5}\\)\n\\(g(x) \\equiv e^{10} e^{-2 x}\\)\n\n\n\n\n\nPart iii What is the correct form for the relationship shown in the graph? \n\n\\(g(x) \\equiv e^{2} x^{2}\\)\n\\(g(x) \\equiv 2 x^{1.5}\\)\n\\(g(x) \\equiv e^{2} x^{1.5}\\)\n\\(g(x) \\equiv e^2 x^{-1.5}\\)\n\\(g(x) \\equiv e^{2} e^{1.5 x}\\)\n\n\n\n\n\nPart iv The figure shows a horizontal axis for a graph. How can you tell that this is a logarithmic axis? \n\nThe labels are evenly spaced and each label is a factor of 2 larger than the previous one.\nThe labels are all multiples of 2.\nTrick question. It’s not a log scale.\n\n\n\n\n\nPart v The figure shows a horizontal axis for a graph. How can you tell that this is a logarithmic axis? \n\nTrick question. It’s not a log scale.\nThe labels are evenly spaced and each label is a factor of 3 larger than the previous one.\nThe labels are 1, 3, 5, 10, …\nThe 3 label is about halfway between the 1 and 10 label for each decade, and the 1 and 10 labels have the same spacing for every decade.\n\n\n\n\n\nPart vi The figure shows a horizontal axis for a graph. How can you tell that this is a logarithmic axis? \n\nTrick question. It’s not a linear scale.\nEach label is 10 + the previous label.\nThe labels are 10, 20, 30, 40, …\nThe 3 label is about halfway between the 1 and 10 label for each decade, and the 1 and 10 labels have the same spacing for every decade."
  },
  {
    "objectID": "Modeling/08-dimensions.html",
    "href": "Modeling/08-dimensions.html",
    "title": "15  Dimensions and units",
    "section": "",
    "text": "Next time you’re at a family gathering with your 10-year-old cousin, give her the following math quiz.\nI don’t know your cousin, but I suspect she will have an easy time answering (1) and (2) correctly. As for (3), she might give the correct answer, “5 miles,” or just say “5.” If so, you’ll follow up with “5 what?” at which point she’ll respond, “miles.”\n10-year-olds are pretty creative, so I’m not sure how she’ll answer (5). But if you ask your Ph.D. aunt, she’ll answer along the lines of “silly question,” or “there’s no such thing.” That’s true.\nConsider these everyday quantities:\nHow would you measure such things?\nIt makes sense to multiply and divide different types of quantities: feet, gallons, kilometers, kilograms, pounds, hours, etc. But you won’t ever see a quantity constructed by adding or subtracting miles and hours or gallons and square feet. You can square feet and cube centimeters, but can you take the square root of a gallon? Does it make sense to raise 2 to the power of 3 yards?\nThis section is about the mathematical structure of combining quantities; which kinds of mathematical operations are legitimate and which are not."
  },
  {
    "objectID": "Modeling/08-dimensions.html#mathematics-of-quantity",
    "href": "Modeling/08-dimensions.html#mathematics-of-quantity",
    "title": "15  Dimensions and units",
    "section": "15.1 Mathematics of quantity",
    "text": "15.1 Mathematics of quantity\n\nThe first step in understanding the mathematics of quantity is to make an absolute distinction between two concepts that, in everyday life, are used interchangeably: dimension and unit.\nLength is a dimension. Meters is a unit of length. We also measure length in microns, mm, cm, inches, feet, yards, kilometers, and miles, to say nothing of furlongs, fathoms, astronomical units (AU), and parsecs.\nTime is a dimension. Seconds is a unit of time. We also measure time in micro-seconds, milliseconds, minutes, hours, days, weeks, months, years, decades, centuries, and millenia.\nMass is a dimension. Kilograms is a unit of mass.\nLength, time, and mass are called fundamental dimensions. This is not because length is more important than area or volume. It’s because you can construct area and volume by multiplying lengths together. This is evident when you consider units of area like square inches or cubic centimeters, but obscured in the names of units like acre, liter, gallon.\n\n\nWe’ll write the few basic fundamental dimensions using capital letters: L, T, M, P, S, \\(\\Theta\\). Dimensions are never expressed in terms of units. In contrast, quantities are a number value combined with the unit that value refers to.\nThe square brackets \\([\\) and \\(]\\) signify that we are looking at the dimension of the quantity inside the brackets. For instance, the population of the US state Colorado is about 5.8 million people. Surround that with square brackets and you get [5.8 million people] which is a dimension, namely, P.\nWe use the notation L, T, and M to refer to the fundamental dimensions. (Electrical current Q is also a fundamental dimension, but we won’t have much use for it in our examples. Also useful are \\(\\Theta\\) (“theta”) for temperature, S for money, and P for a count of organisms such as the population of the US or the size of a sheep herd.)\nBrackets translate between a quantity and the dimension. For instance, [1 yard] = L, [1000 kg] = M, [3 years] = T, [10 \\(\\mu\\) (microns)] = L,"
  },
  {
    "objectID": "Modeling/08-dimensions.html#compound-dimensions",
    "href": "Modeling/08-dimensions.html#compound-dimensions",
    "title": "15  Dimensions and units",
    "section": "15.2 Compound dimensions",
    "text": "15.2 Compound dimensions\nThere are other dimensions: volume, force, pressure, energy, torque, velocity, acceleration, and such. These are called compound dimensions because we represent them as combinations of the fundamental dimensions, L, T, and M. The notation for these combinations involves multiplication and division. For instance:\n\nVolume is L \\(\\times\\) L \\(\\times\\) L \\(=\\) L\\(^3\\), as in “cubic centimeters”\nVelocity is L/T, as in “miles per hour”\nForce is M L/T\\(^2\\), which is obscure unless you remember Newton’s Second Law that \\(\\text{F} = \\text{m}\\,\\text{a}\\): “force equals mass times acceleration.” In terms of dimension, mass is M, acceleration is L/T\\(^2\\). Multiply the two together and you get the dimension “force.”\n\nMultiplication and division are used to construct a compound dimension from the fundamental dimensions L, T, and M.\nAddition and subtraction are never used to form a compound dimension.\nMuch of the work in understanding dimensions involves overcoming the looseness of everyday speech. Remember the weight scale graduated in pounds and kilograms. The unit kilograms is a way of measuring M, but the unit of pounds is a way of measuring force: M L/T\\(^2\\).\nWeight is not the same as mass. This makes no sense to most people and doesn’t matter in everyday life. It’s only when you venture off the surface of the Earth that the difference shows up. The Mars rover Perseverance weighs 1000 kg on Earth. It was weightless for most of its journey to Mars. After landing on Mars, Perseverence weighed just 380 kg. But the rover’s mass didn’t change at all.\nAnother source of confusion carried over from everyday life is that sometimes we measure the same quantity using different dimensions. You can measure a volume by weighing water; a gallon of water weighs 8 pounds; a liter of water has a mass of 1 kg. Serious bakers measure flour by weight; a casual baker uses a measuring cup. We can measure water volume with length because water has a (more-or-less) constant mass density. But 8 pounds of gasoline is considerably more than a gallon. It turns out that the density of flour varies substantially depending on how it’s packed, humidity, etc. This is why it matters whether you weigh flour for baking or measure it by volume. You can measure time by the swing of a pendulum. To measure the same time successfully with different pendula they need to have the same length, not the same mass.\nA unit is a conventional amount of a quantity of a given dimension. All lengths are the same dimensionally, but they can be measured with different conventions: inches, yards, meters, … Units for the same dimension can all be converted unambiguously one into the other. A meter is the same quantity of length as 39.3701 inches, a mile is the same length as 1609.34 meters. Liters and gallons are both units of volume (L\\(^3\\)): a gallon is the same as 3.78541 liters.\nYou will hear it said that a kilogram is 2.2 pounds. That’s not strictly true. A kilogram has dimension M and a pound has dimension ML/T\\(^2\\). Quantities with different dimensions cannot be “equal” or even legitimately compared to one another. Unless you bring something else into the game that physically changes the situation, for instance, gravity (dimension of acceleration due to gravity (dimension \\(\\text{L}/\\text{T}^2\\)). The weight of a kilogram on the surface of the Earth is 2.2 pounds because gravitational acceleration is (almost) the same everywhere on the surface of the Earth.\nIt’s also potentially confusing that sometimes different dimensions are used to get at the same idea. For instance, the same car that gets 35 miles / gallon in the US (dimension \\(\\text{L}/\\text{L}^3 = 1/\\text{L}^2\\)) will use 6.7 liters per 100 kilometers (\\(\\text{L}^3 / L = \\text{L}^2\\)) in Europe. Same car. Same fuel. Different conventions using different dimensions.\nKeeping track of the various compound dimensions can be tricky. For many people, it’s easier to keep track of the physical relationships involved and use that knowledge to put together the dimensions appropriately. Often, the relationship can be described using specific calculus operations, so knowing dimensions and units helps you use calculus successfully.\nEasy compound dimensions that you likely already know:\n\n[Area] \\(= \\text{L}^2\\). Some corresponding units to remind you: “square feet”, “square miles”, “square centimeters.”\n[Volume] \\(= \\text{L}^3\\). Units to remind you: “cubic centimeters”, “cubic feet”, “cubic yards.” (What landscapers informally call a “yard,” for instance “10 yards of topsoil” should properly be called “10 cubic-yards of topsoil.”)\n[Velocity] \\(= \\text{L}/\\text{T}\\). Units: “miles per hour,” “inches per second.”\n[Momentum] \\(= \\text{M}\\text{L}/\\text{T}\\). Units: “kilogram meters per second.”\n\nAnticipating that you will return to this section for reference, we’ve also added some dimensions that can be understood through the relevant calculus operations.\n\n[Acceleration] \\(= \\text{L}/\\text{T}^2\\). Units: “meters per second squared,” In calculus, acceleration is the derivative of velocity with respect to time, or, equivalently, the 2nd derivative of position with respect to time.\n[Force] \\(= \\text{M}\\, \\text{L}/\\text{T}^2\\) In calculus: force is the derivative of momentum with respect to time.\n[Energy] or [Work] \\(= \\text{M}\\, \\text{L}^2/\\text{T}^2\\) In calculus, energy is the integral of force with respect to length.\n[Power] \\(= \\text{M}\\, \\text{L}^2/\\text{T}^3\\) In the language of calculus, power is the derivative of energy with respect to time.\n\n\nDensity sounds like a specific concept, but there are many different kinds of densities. These have in common that they are a ratio of a physical amount to a geometric extent:\n\na physical amount: which might be mass, charge, people, etc.\na geometric extent: which might be length, area, or volume.\n\nSome examples:\n\n“paper weight” is the mass per area, typically grams-per-square-meter\n“charge density” is the amount of electrical charge, usually per area or volume\n“lineal density of red blood cells” is the number of cells in a capillary divided by the length of the capillary. (Capillaries are narrow. Red blood cells go through one after the other.)\n“population density” is people per area of ground.\n\n\n\nThe theory of dimensions and units was developed for the physical sciences. Consequently, the fundamental dimensions are those of physics: length, mass, time, electrical current, and luminous intensity.\nSince proper use of units is important even outside the physical sciences, it’s helpful to recognize the dimension of several other kinds of quantity.\n\n“people” / “passengers” / “customers” / “patients” / “cases” / “passenger deaths”: these are different different ways to refer to people. We’ll consider such quantities to have dimension P, for population.\n“money”: Units are dollars (in many varieties: US, Canadian, Australian, New Zealand), euros, yuan (synonym: renminbi), yen, pounds (many varieties: UK, Egypt, Syria, Lebanon, Sudan, and South Sudan), pesos (many varieties), dinar, franc (Swiss, CFA), rand, riyal, rupee, won, and many others. Conversion rates depend on the situation and national policy, but we will consider money a dimension, denoted by S (from the name of the first coinage, the Mesopotamian Shekel).\n\nExamples:\n\nPassenger-miles is a standard unit of transport.\nPassenger-miles-per-dollar is an appropriate unit of the economic efficiency of transport.\nPassenger-deaths per million passenger-mile is one way to describe the risk of transport."
  },
  {
    "objectID": "Modeling/08-dimensions.html#arithmetic-with-dimensions",
    "href": "Modeling/08-dimensions.html#arithmetic-with-dimensions",
    "title": "15  Dimensions and units",
    "section": "15.3 Arithmetic with dimensions",
    "text": "15.3 Arithmetic with dimensions\nRecall the rules for arithmetic dimensioned quantities. We restate them briefly with the square-bracket notation for “the dimension of.” For instance, “the dimension of \\(b\\)” is written \\([b]\\). We also write \\([1]\\) to stand for the dimension of a pure number, that is, a quantity without dimension.\n::: {#tbl-allowed-operations .column-page-right}\n\n\n\n\n\n\n\n\n\nOperation\nResult\nOnly if satisfies\nMetaphor\n\n\n\n\nMultiplication\n\\([a \\times b] = [a] \\times [b]\\)\nanything goes\npromiscuous\n\n\nDivision\n\\([a \\div b] = [a] \\div [b]\\)\nanything goes\npromiscuous\n\n\nAddition\n\\([a + b] = [a]\\)\n\\([a] = [b]\\)\nmonogomous\n\n\nSubtraction\n\\([a - b] = [a]\\)\n\\([a] = [b]\\)\nmonogomous\n\n\nTrigonometric\n\\([\\sin(a)] = [1]\\)\n\\([a] = [1]\\)\ncelibate\n\n\nExponential\n\\([e^a] = [1]\\)\n\\([a] = [1]\\) (of course, \\([e] = [1]\\))\ncelibate\n\n\nPower-law\n\\([b ^ a] = \\underbrace{[b]\\times[b]\\times ...\\times [b]}_{a\\ \\ \\text{times}}\\)\n\\([a] = [1]\\) with \\(a\\) an integer\nexponent celibate\n\n\nSquare root\n\\([\\sqrt{b}] = [c]\\)\n\\([b] = [c\\times c]\\)\nidiosyncratic\n\n\nCube root\n\\([\\sqrt[3]{b}] = [c]\\)\n\\([b] = [c \\times c \\times c]\\)\nidiosyncratic\n\n\nHump\n\\([\\text{hump}(a)] = [1]\\)\n\\([a] = [1]\\)\ncelibate\n\n\nSigmoidal\n\\([\\text{sigmoid}(a)] = [1]\\)\n\\([a] = [1]\\)\ncelibate\n\n\n\nConditions under which functions can be applied to dimensionful quantitities. Note that \\([a] = [b]\\) means that the dimension of \\(a\\) and of \\(b\\) are the same. For instance, even though 1 mm and 500 miles are very different distances, [1 mm]\\(=\\)[500 miles]. Both [1 mm] and [500 miles] are dimension L."
  },
  {
    "objectID": "Modeling/08-dimensions.html#sec-pendulum-dimensions",
    "href": "Modeling/08-dimensions.html#sec-pendulum-dimensions",
    "title": "15  Dimensions and units",
    "section": "15.4 Example: Dimensional analysis",
    "text": "15.4 Example: Dimensional analysis\nWe want to relate the period (in T) of a pendulum to its length and mass. Acceleration due to gravity also plays a role; that has dimension \\(\\text{L}\\cdot \\text{T}^{-2}\\). For simplicity, we’ll assume that only the bob at the end of the pendulum cable or rod has mass.\nThe analysis strategy is to combine the four quantities we think play a role into one total quantity that is dimensionless. Since it is dimensionless, it can be constant regardless of the mass, length, period, or gravity of each situation.\n\\[\\text{[Period]}^a \\cdot \\text{[Mass]}^b \\cdot \\text{[Length]}^c \\cdot \\text{[Gravity]}^d = T^a \\cdot M^b \\cdot L^c \\cdot L^d \\cdot T^{-2d} = [1]\\] To be dimensionless:\n\n\\(c = -d\\), cancel out the L\n\\(a = 2d\\), cancel out the T\n\\(b=0\\), there’s no other mass term, and we need to cancel out the M\n\nAll of the exponents can be put in terms of \\(d\\). That doesn’t tell us what \\(d\\) should be, but whatever value for \\(d\\) we decide to choose, we get a ratio that’s equivalent to:\n\\[ \\frac{[\\text{Gravity}]\\cdot [\\text{Period}]^2}{[\\text{Length}]} = [1]\\]\nThis is a relationship between dimensions of quantities. To render it into a formula involving the quantities themselves we need to take into account the units.\n\\[ \\frac{\\text{Gravity}\\cdot \\text{Period}^2}{\\text{Length}} = B\\]\nWe can experimentally determine the numerical value of the dimensionless constant \\(B\\) by measuring the period and length of a pendulum and (on Earth) recognizing that gravitational acceleration on Earth’s surface is 9.8 meters-per-second-squared. Such experiments and mathematical models using differential equations give \\(B = (2\\pi)^2\\)."
  },
  {
    "objectID": "Modeling/08-dimensions.html#conversion-flavors-of-1",
    "href": "Modeling/08-dimensions.html#conversion-flavors-of-1",
    "title": "15  Dimensions and units",
    "section": "15.5 Conversion: Flavors of 1",
    "text": "15.5 Conversion: Flavors of 1\nNumbers are dimensionless but not necessarily unitless. Failure to accept this distinction is one of the prime reasons people have trouble figuring out how to convert from one unit to another.\nThe number one is a favorite of elementary school students because its multiplication and division tables are completely simple. Anything times one, or anything divided by one, is simply that thing. Addition and subtraction are pretty simple, too, a matter of counting up or down.\nWhen it comes to quantities, there’s not just one one but many. And often they look nothing like the numeral 1. Some examples of 1 as a quantity:\n\n\\(\\frac{180}{\\pi} \\frac{\\text{degrees}}{\\text{radians}}\\)\n\\(0.621371 \\frac{\\text{mile}}{\\text{kilometer}}\\)\n\\(3.78541 \\frac{\\text{liter}}{\\text{gallon}}\\)\n\\(\\frac{9}{5} \\frac{^\\circ F}{^\\circ C}\\)\n\\(\\frac{1}{12} \\frac{\\text{dozen}}{\\text{item}}\\)\n\nI like to call these and others different flavors of one.\nIn every one of the above examples, the dimension of the numerator matches the dimension of the denominator. The same is true when comparing feet and meters ([feet / meter] is L/L = [1]), or comparing cups and pints ([cups / pint] is \\(\\text{L}^3/\\text{L}^3 = [1]\\)) or comparing miles per hour and feet per second ([miles/hour / ft per sec] = L/T / L/T = [1]). Each of these quantities has units but it has no dimension.\nIt’s helpful to think about conversion between units as a matter of multiplying by the appropriate flavor of 1. Such conversion will not change the dimension of the quantity but will render it in new units.\n\nExample: Convert 100 feet-per-second into miles-per-hour. First, write the quantity to be converted as a fraction and alongside it, write the desired units after the conversion. In this case that will be \\[100 \\frac{\\text{feet}}{\\text{second}} \\ \\ \\ \\text{into} \\ \\ \\ \\frac{\\text{miles}}{\\text{hour}}\\]\nFirst, we’ll change feet into miles. This can be accomplished by multiplying by the flavor of one that has units miles-per-foot. Second, we’ll change seconds into hours. Again, a flavor of 1 is involved.\nWhat number will give a flavor of one? One mile is 5280 feet, so \\[\\frac{1}{5280} \\frac{\\text{miles}}{\\text{foot}}\\] is a flavor of one.\nNext, we need a flavor of one that will turn \\(\\frac{1}{\\text{second}}\\) into \\(\\frac{\\text{1}}{\\text{hour}}\\). We can make use of a minute being 60 seconds, and an hour being 60 minutes. \\[\\underbrace{\\frac{60\\  \\text{s}}{\\text{minute}}}_\\text{flavor of 1}\\  \\underbrace{\\frac{60\\ \\text{minutes}}{\\text{hour}}}_\\text{flavor of 1} = \\underbrace{3600\\frac{\\text{s}}{ \\text{hour}}}_\\text{flavor of 1}\\]\nMultiplying our carefully selected flavors of one by the initial quantity, we get \\[\n\\underbrace{\\frac{1}{5280} \\frac{\\text{mile}}{\\text{foot}}}_\\text{flavor of 1} \\times \\underbrace{3600 \\frac{\\text{s}}{\\text{hour}}}_\\text{flavor of 1} \\times \\underbrace{100 \\frac{\\text{feet}}{\\text{s}}}_\\text{original quantity} = 100 \\frac{3600}{5280} \\frac{\\text{miles}}{\\text{hour}} = 68.18 \\frac{\\text{miles}}{\\text{hour}}\\]"
  },
  {
    "objectID": "Modeling/08-dimensions.html#dimensions-and-linear-combinations",
    "href": "Modeling/08-dimensions.html#dimensions-and-linear-combinations",
    "title": "15  Dimensions and units",
    "section": "15.6 Dimensions and linear combinations",
    "text": "15.6 Dimensions and linear combinations\nLow-order polynomials are a useful way of constructing model functions. For instance, suppose we want a model of the yield of corn in a field per inch of rain over the growing season, will call it corn(rain). The output will have units of bushels (of corn). The input will have units of inches (of rain). A second-order polynomial will be appropriate for reasons to be discussed in Chapter 24.\n\\[\\text{corn(rain)} \\equiv a_0 + a_1\\, \\text{rain} + \\frac{1}{2} a_2\\, \\text{rain}^2\\] Of course, the addition in the linear combination will only make sense if all three terms \\(a_0\\), \\(a_1\\,\\text{rain}\\), and \\(\\frac{1}{2}\\, a_2\\, \\text{rain}^2/2\\) have the same dimension. But clearly \\([\\text{rain}] \\neq [\\text{rain}^2]\\). In order for things to work out, the coefficients must themselves have dimension. We know the output of the function will have dimension \\([\\text{volume}] = \\text{L}^3\\). Thus, \\([a_0] = \\text{L}^3\\).\n\\([a_1]\\) must be different, because it has to combine with the \\([\\text{rain}] = \\text{L}\\) and produce \\(\\text{L}^3\\). Thus, \\([a_1] = \\text{L}^2\\).\nFinally, \\([a_2] = \\text{L}\\). Multiplying that by \\([\\text{rain}]^2\\) will give the required \\(\\text{L}^3\\)\n\n\nIn everyday communication as well as in most domains such as construction, geography, navigation, and astronomy we measure angles in degrees. 90 degrees is a right angle. But in mathematics, the unit of angle is radians where a right angle is 1.5708 radians. (1.5708 is the decimal version of \\(\\pi/2\\).) The conversion function, which we’ll call raddeg(), is \\[\\text{raddeg}(r) \\equiv \\frac{180}{\\pi} r\\] The function that converts degrees to radians, which we’ll call degrad() is very similar: \\[\\text{degrad}(d) \\equiv \\frac{\\pi}{180} d\\] (Incidentally, \\(\\frac{180}{\\pi} = 57.296\\) while \\(\\frac{\\pi}{180} = 0.017453\\).)\nIn traditional notation, the trigonometric functions such as \\(\\sin()\\) and \\(\\tan()\\) can be written with an argument either in degrees or radians. For instance, \\(\\sin(90^\\circ) = \\sin\\left(\\frac{\\pi}{2}\\right)\\). Similarly, for the inverse functions like \\(\\arccos()\\) the units of the output are not specified. This works because there is always a human to intervene between the written expression and the eventual computation.\nIn R, as in many other computer languages like Python or spreadsheet packages, there is no valid expression like sin(90 deg). In these languages, 90 deg is not a valid expression (although it might be good if it were valid!). In these and many other languages, angles are always given in radians. Such consistency is admirable, but people are not always so consistent. It is a common source of computer bugs that angles in degrees are handed off to functions like \\(\\sin()\\) and that the output of \\(\\arccos()\\) is (wrongly) interpreted as degrees rather than radians.\nFunction composition to the rescue!\nConsider this function given in the Wikipedia article on the position of the sun as seen from Earth.1 \\[\\delta_\\odot(n) \\equiv - 23.44^\\circ \\cdot \\cos \\left [ \\frac{360^\\circ}{365\\, \\text{days}} \\cdot \\left ( n + 10 \\right ) \\right ]\\] Where \\(n\\) is zero at the midnight marking New Years and increases by 1 per day. (The \\(n+10\\) has units of days and translates New Years back 10 days, to the day of the winter solstice.) \\(\\delta_\\odot()\\) gives the declination of the sun: the latitude pieced by an imagined line connecting the centers of the earth and the sun.\nThe Wikipedia formula is well written in that it uses some familiar numbers to help the reader see where the formula comes from. 365 is recognizably the length of the year in days. \\(360^\\circ\\) is the angle traversed when making a full cycle around a circle. \\(23.44^\\circ\\) is less familiar, but the student of geography might recognize it as the latitude of the Tropic of Cancer, the latitude farthest north where the sun is directly overhead at noon (on the day of the summer solstice).\nBut there’s a world of trouble for the programmer who implements the formula as\n\ndec_sun <- makeFun(-23.44 * cos((360/365)*(n+10)) ~ n)\n\nFor instance, the equinoxes are around March 21 (n=81) and Sept 21 (n=264). On an equinox, the declination of the sun is zero degrees. But let’s plug \\(n=81\\) and \\(n=264\\) into the formula and see what we get.\n\ndec_sun(81)\n## [1] 5.070321\ndec_sun(264)\n## [1] -23.38324\n\nThe equinoxes aren’t even equal! And they are not close to zero. Does this mean astronomy is wrong?\nThe Wikipedia formula should have been programmed this way, using 2 \\(\\pi\\) radians instead of 360 degrees in the argument to the cosine function:\n\ndec_sun_right <- \n  makeFun(-23.44 * cos(( 2*pi/365)*(n+10)) ~ n)\ndec_sun_right(81)\n## [1] -0.1008749\ndec_sun_right(264)\n## [1] -0.1008749\n\nThe deviation of one-tenth of a degree reflects rounding off the time of the equinox to the nearest day."
  },
  {
    "objectID": "Modeling/08-dimensions.html#exercises",
    "href": "Modeling/08-dimensions.html#exercises",
    "title": "15  Dimensions and units",
    "section": "15.7 Exercises",
    "text": "15.7 Exercises"
  },
  {
    "objectID": "Modeling/08-dimensions.html#drill",
    "href": "Modeling/08-dimensions.html#drill",
    "title": "15  Dimensions and units",
    "section": "15.8 Drill",
    "text": "15.8 Drill\n\n\nPart i Consider these quantities:\\(a = 25\\) ft \\(b = 3\\) hours\\(c = 4\\)\\(d = 1\\) meter\\(e = 2.718\\) Is this combination dimensionally valid? \\[a / c\\] Why or why not?\n\n\\(c\\) is a dimensionless quantity.\nDivision can accommodate any two quantities, regardless of dimension.\nYou can only divide two quantities of the same dimension.\n\n\n\n\n\nPart ii Consider these quantities:\\(a = 25\\) ft \\(b = 3\\) hours\\(c = 4\\)\\(d = 1\\) meter\\(e = 2.718\\) Is this combination dimensionally valid? \\[\\sqrt{a}\\] Why or why not?\n\nValid. It’s simply 5.\nInvalid. You can’t have a non-integer exponent on a dimension.\nInvalid. 25 feet is not a valid quantity.\n\n\n\n\n\nPart iii Consider these quantities:\\(a = 25\\) ft \\(b = 3\\) hours\\(c = 4\\)\\(d = 1\\) meter\\(e = 2.718\\) Is this combination dimensionally valid? \\[b^c\\] Why or why not?\n\nValid. Exponentiation by a dimensionless integer is always allowed.\nInvalid. Exponentiation of a dimensionful quantity isn’t allowed.\nInvalid. I can’t make any sense out of T\\(^4\\) as a dimension.\n\n\n\n\n\nPart iv Consider these quantities:\\(a = 25\\) ft \\(b = 3\\) hours\\(c = 4\\)\\(d = 1\\) meter\\(e = 2.718\\) Is this combination dimensionally valid? \\[c^b\\] Why or why not?\n\nValid. Exponentiation by a dimensionless integer is always allowed.\nInvalid. Exponentiation by a dimensionful quantity isn’t allowed.\nValid. You can do what you want with plain (dimensionless) numbers like 4.\n\n\n\n\n\nPart v Consider these quantities:\\(a = 25\\) ft \\(b = 3\\) hours\\(c = 4\\)\\(d = 1\\) meter\\(e = 2.718\\) Is this combination dimensionally valid? \\[\\sqrt[3]{a^2 d}\\] Why or why not?\n\nValid. \\(a^2 d\\) is a volume: L\\(^3\\). The cube root of L\\(^3\\) is L.\nInvalid. You can’t raise a dimensionful quantity to a non-integer power.\nInvalid. 25 feet squared is 625 square feet. It makes no sense to multiply square feet by meters.\n\n\n\n\n\nPart vi Consider these quantities:\\(a = 25\\) ft \\(b = 3\\) hours\\(c = 4\\)\\(d = 1\\) meter\\(e = 2.718\\) Is this combination dimensionally valid? \\[\\exp(a d)\\] Why or why not?\n\nValid. The \\(a\\) cancels out the dimension of the \\(b\\).\nInvalid. The input to \\(\\exp()\\) must be a dimensionless quantity.\nInvalid. 25 foot-meters doesn’t mean anything.\n\n\n\n\n\nPart vii Consider these quantities:\\(a = 25\\) ft \\(b = 3\\) hours\\(c = 4\\)\\(d = 1\\) meter\\(e = 2.718\\) Is this combination dimensionally valid? \\[\\exp(c d)\\] Why or why not?\n\nValid. The dimension will be L\\(^4\\)\nInvalid. The input to \\(\\exp()\\) must be a dimensionless quantity.\nInvalid. \\(c d\\) has dimension L.\n\n\n\n\n\nPart viii Consider these quantities:\\(a = 25\\) ft \\(b = 3\\) hours\\(c = 4\\)\\(d = 1\\) meter\\(e = 2.718\\) Is this combination dimensionally valid? \\[\\exp(c/d)\\] Why or why not?\n\nValid. The L dimension of \\(c\\) is cancelled out by the L\\(^{-1}\\) dimension of \\(1/d\\)\nInvalid. The input to \\(\\exp()\\) must be a dimensionless quantity.\nInvalid. \\(c / d\\) has dimension L\\(^{-1}\\).\n\n\n\n\n\nPart ix Here are some physical quantities and their dimension:[Force] = MLT\\(^{-2}\\)[Distance] = L[Area] = L\\(^2\\)[Velocity] = L T\\(^{-1}\\)[Acceleration] = L T\\(^{-2}\\)[Momentum] = M L T\\(^{-1}\\) Given that [Force] = [Pressure][Area], what is the dimension of Pressure?\nM L\\(^{1}\\) T\\(^{-2}\\)M L\\(^{-1}\\) T\\(^{-2}\\)M L\\(^{-2}\\) T\\(^{-1}\\)\n\n\n\n\nPart x Here are some physical quantities and their dimension:[Force] = MLT\\(^{-2}\\)[Distance] = L[Area] = L\\(^2\\)[Velocity] = L T\\(^{-1}\\)[Acceleration] = L T\\(^{-2}\\)[Momentum] = M L T\\(^{-1}\\)Which one of the following statements is true?\n\nVelocity = Mass / Momentum\nMomentum = Mass * Velocity\nMomentum = Mass * Acceleration\n\n\n\n\n\nPart xi Here are some physical quantities and their dimension:[Force] = MLT\\(^{-2}\\)[Distance] = L[Area] = L\\(^2\\)[Velocity] = L T\\(^{-1}\\)[Acceleration] = L T\\(^{-2}\\)[Momentum] = M L T\\(^{-1}\\)Which one of the following statements is true?\n\nArea = Distance / Volume\nVolume = Distance * Area\nForce = Momentum / Acceleration\n\n\n\n\n\nPart xii Here are some physical quantities and their dimension:[Force] = MLT\\(^{-2}\\)[Distance] = L[Area] = L\\(^2\\)[Velocity] = L T\\(^{-1}\\)[Acceleration] = L T\\(^{-2}\\)[Momentum] = M L T\\(^{-1}\\)The dimension of Energy is M L\\(^2\\) T\\(^{-2}\\). Which of the following is true?\n\nForce = Energy / Mass\nEnergy = Distance * Force\nEnergy = Momentum * Acceleration\n\n\n\n\n\nPart xiii Here are some physical quantities and their dimension:[Force] = MLT\\(^{-2}\\)[Distance] = L[Area] = L\\(^2\\)[Velocity] = L T\\(^{-1}\\)[Acceleration] = L T\\(^{-2}\\)[Momentum] = M L T\\(^{-1}\\)Which of the following is true?\n\nLength = Force / Momentum\nLength = Velocity / Acceleration\nArea = Velocity * Acceleration\n\n\n\n\n\nPart xiv Here are some physical quantities and their dimension:[Force] = MLT\\(^{-2}\\)[Distance] = L[Area] = L\\(^2\\)[Velocity] = L T\\(^{-1}\\)[Acceleration] = L T\\(^{-2}\\)[Momentum] = M L T\\(^{-1}\\)Which of the following is true?\n\nMass = Force / Momentum\nLength = Force / Momentum\nTime = Force / Momentum\nArea = Force / Momentum\n\n\n\n\n\nPart xv What kind of thing is \\[\\sqrt[3]{(\\text{4in})(\\text{2 ft})(\\text{1 mile})}\\  ?\\]\nIt’s meaninglessAreaLengthVolume\n\n\n\n\nPart xvi What kind of thing is \\[\\sin(\\pi\\ \\text{seconds})\\  ?\\]\nIt’s meaningless1 / LengthLengthThe number 0\n\n\n\n\nPart xvii If \\(t\\) is measured in seconds and \\(A\\) is measured in feet, what will be the dimension of \\(A \\sin(2\\pi t/P)\\) when \\(P\\) is two hours?\nTLL/T\n\n\n\n\nPart xviii Engineers often prefer to describe sinusoids in terms of their frequency \\(\\omega\\), writing the function as \\(\\sin(2 \\pi \\omega t)\\), where \\(t\\) is time. What is the dimension of \\(\\omega\\)?\nTT\\(^{-1}\\)T\\(^2\\)\n\n\n\n\nPart xix Suppose \\(t\\) is measured in hours and \\(x\\) in yards. What will be the dimension of \\(P\\) in \\[\\sin(2\\pi t x/P)\\ ?\\]\n\nT / L\nL T\nThere’s no such \\(P\\) that will make a valid input to \\(\\sin()\\)\nL / T\n\n\n\n\n\nPart xx What is the value of sin(180)?\n-0.8000.801"
  },
  {
    "objectID": "Modeling/09-modeling-cycle.html",
    "href": "Modeling/09-modeling-cycle.html",
    "title": "16  Modeling cycle [DRAFT]",
    "section": "",
    "text": "INCLUDE POLISHING IN THIS CHAPTER"
  },
  {
    "objectID": "Modeling/09-modeling-cycle.html#possible-introduction",
    "href": "Modeling/09-modeling-cycle.html#possible-introduction",
    "title": "16  Modeling cycle [DRAFT]",
    "section": "16.1 Possible introduction",
    "text": "16.1 Possible introduction\nSeen very abstractly, a mathematical model is a set of functions that represent the relationships between inputs and outputs.\nAt the most simple level, building a model can be a short process:\n\nDevelop an understanding of the relationship you want to model. Often, part of this “understanding” is the pattern seen in data.\nChoose a function type—e.g. exponential, sinusoidal, sigmoid—that you think would be a good match to the relationship.\nFind parameters that scales your function to be able to accept real-world inputs and generate real-world outputs.\n\nIt’s important to distinguish between two basic types of model:\n\nEmpirical models which are rooted in observation and data.\nMechanistic models such as those created by applying fundamental laws of physics, chemistry, and such.\n\nWe are going to put off mechanistic models for a while, for two reasons. First, the “fundamental laws of physics, chemistry, and such” are often expressed with the concepts and methods of calculus. We are heading there, but at this point you don’t yet know the core concepts and methods of calculus. Second, most students don’t make a careful study of the “fundamental laws of physics, chemistry, and such” until after they have studied calculus. So examples of mechanistic models will be a bit hollow at this point."
  },
  {
    "objectID": "Modeling/09-modeling-cycle.html#an-example-from-the-01-parameters-chapter",
    "href": "Modeling/09-modeling-cycle.html#an-example-from-the-01-parameters-chapter",
    "title": "16  Modeling cycle [DRAFT]",
    "section": "16.2 AN EXAMPLE FROM THE 01-parameters chapter",
    "text": "16.2 AN EXAMPLE FROM THE 01-parameters chapter\n\nYou can see in Figure @ref(fig:covid-exp2) that the exponential function plotted in \\(\\color{blue}{\\text{blue}}\\) does not align perfectly with the day-by-day data. For instance, during the interval from March 8 to 21, the function output is consistently higher than the data suggest it should be. As part of the modeling cycle it’s important to notice such discrepancies and try to understand them. In this case, it’s likely that during the early days of the pandemic the number of reported deaths understated the actual number of COVID-related deaths. This happens because, in the early days of the pandemic, many deaths from COVID were mis-attributed to other causes.\n\n\nEffective modelers treat models with skepticism. They look for ways in which models fail to capture salient features of the real world. They have an eye out for deviations between what their models show and what they believe they know about the system being modeled. They consider ways in which the models might not serve the purpose for which they were developed.\nWhen modelers spot a failure or deviation or lack of proper utility, they might discard the model but more often they make a series of small adjustments, tuning up the model until is successfully serves the purposes for which it is intended.\nThus, modeling is a cyclic process of creating a model, assessing the model, and revising the model. The process comes to a sort of preliminary end when the model serves its purposes. But even then, models are often revised to check whether the results are sensitive to some factor that was not included or to check whether some component that was deemed essential really is so."
  },
  {
    "objectID": "Modeling/09-modeling-cycle.html#example-cooling-water",
    "href": "Modeling/09-modeling-cycle.html#example-cooling-water",
    "title": "16  Modeling cycle [DRAFT]",
    "section": "16.3 Example: Cooling water",
    "text": "16.3 Example: Cooling water\nLooking back on the exponential fitted to the cooling water data in ?sec-fit-exponential, it looks like our original estimate of the half-life is a bit too small; the data doesn’t seem to decay at the rate implied by \\(k = -0.0277\\). Perhaps we should try a somewhat slower decay, say \\(k = -0.2\\) and see if that improves things.\n\nIn the cooling water example, we’re using only a subset of the data collected by Prof. Wagon. The next commands re-create that subset so that you can work with it. They also plot the data and an exponential model.\n\n# reconstruct the sample\nset.seed(101)\nStans_data <- CoolingWater %>% sample_n(20)\n# Plot the sample and overlay a model\ngf_point(temp ~ time, data=Stans_data) %>%\n  gf_lims(y = c(20, NA)) %>%\n  slice_plot(25 + 83.3*exp(-.0277*time) ~ time, color=\"dodgerblue\")\n\n\n\n\n\n\n\n\nSee if \\(k=-0.02\\) provides a better fit to the model. (You can add another slice_plot() to be able to compare the original and \\(k=-0.02\\) models.)\n\nLater in this course, we’ll study optimization. There are optimization techniques for directing the computer to refine the parameters to best match the data. Just to illustrate, here’s what we get:\n\n\n\n\nrefined_params <-\n  fitModel(temp ~ A + B*exp(k*time), data = Stans_data,\n           start = list(A = 25, B = 83.3, k = -0.0277))\ncoef(refined_params)\n##           A           B           k \n## 25.92628463 60.69255269 -0.01892572\nnew_f <- makeFun(refined_params)\ngf_point(temp ~ time, data = Stans_data) %>%\n  slice_plot(new_f(time) ~ time, color=\"dodgerblue\")\n\n\n\n\nFigure 16.1: Polishing the fit using the rough model as a starting point.\n\n\n\n\nThe refined parameters give a much better fit to the data than our original rough estimates by eye.\nWe had two rounds of the modeling cycle. First, choice of an exponential model and a rough estimate of the parameters A, B, and \\(k\\). Second, refinement of those parameters using the computer.\nLooking at the results of the second round, the experienced modeler can see some disturbing discrepancies. First, the estimated baseline appears to be too high. Related, the initial decay of the model function doesn’t seem to be fast enough and the decay of the model function for large \\(t\\) appears to be too slow. Prof. Stan Wagon noticed this. He used additional data to fill in the gaps for small \\(t\\) and refined his model further by changing the basis functions in the linear combination. He hypothesized that there are at least two different cooling processes. First, the newly poured water raises the temperature of the mug itself. Since the water and mug are in direct contact, this is a fast process. Then, the complete water/mug unit comes slowly into equilibrium with the room temperature.\nThe newly refined model was a even better match to the data. But nothing’s perfect and Prof. Wagon saw an opportunity for additional refinement based on the idea that there is a third physical mechanism of cooling: evaporation from the surface of the hot water. Prof. Wagon’s additional circuits of the modeling cycle were appropriate to his purpose, which was to develop a detailed understanding of the process of cooling. For other purposes, such as demonstrating the appropriateness of an exponential process or interpolating between the data points, earlier cycles might have sufficed.\nHere’s a graph of the model Prof. Wagon constructed to match the data.\n\n\n\n\n\nFigure 16.2: A model that combines three exponentials provides an excellent fit.\n\n\n\n\nThis is an excellent match to the data. But … matching the data isn’t always the only goal of modeling. Prof. Wagon wanted to make sure the model was physically plausible. And looking at the refined parameters, which include two exponential processes with parameters \\(k_1\\) and \\(k_2\\), he saw something wrong:\n\nBut what can we make of \\(k_1\\), whose [positive value] violates the laws of thermodynamics by suggesting that the water gets hotter by virtue of its presence in the cool air? The most likely problem is that our simple model (the proportionality assumption) is not adequate near the boiling point. There are many complicated factors that affect heat transportation, such as air movement, boundary layer dissipation, and diffusion, and our use of a single linear relationship appears to be inadequate. In the next section [of our paper] we suggest some further experiments, but we also hope that our experiments might inspire readers to come up with a better mathematical model.\n\nThe modeling cycle can go round and round!"
  },
  {
    "objectID": "Modeling/09-modeling-cycle.html#example-the-tides",
    "href": "Modeling/09-modeling-cycle.html#example-the-tides",
    "title": "16  Modeling cycle [DRAFT]",
    "section": "16.4 Example: The tides",
    "text": "16.4 Example: The tides\nIn ?sec-fit-periodic we looked at a sinusoid model of tide levels in Rhode Island. We left unresolved how to refine the estimate of the period \\(P\\) and find the time offset \\(t_0\\) in the sinusoidal model \\[\\text{tide}(t) \\equiv A \\sin\\left(\\frac{2\\pi}{P} (t-t_0)\\right) + B\\]\n\\[{\\color{blue}{\\text{tide}(t)} \\equiv 1.05 + 0.55 \\sin(2\\pi (t - t_0)/11)}\\]\nThe new parameter, \\(t_0\\), should be set to be the time of a positive-going crossing of the baseline. Looking at the tide data (black) plotted in Figure @ref(fig:Fun-4-a-3-4) we can pick out such a crossing at about time = 17. Happily, changing the phase does not itself necessitate re-estimating the other parameters: baseline, amplitude, period. This model, incorporating the phase, has been graphed in \\(\\color{blue}{\\text{blue}}\\).\n\n\n\n\n\nFigure 16.3: Shifting the phase of the sinusoid gives the flexibility needed to align the peaks and troughs of the model with the data. Performing this alignment for one peak makes it clear that the period is wrong.\n\n\n\n\n\n\n\nFigure 16.4: Shifting the phase of the sinusoid gives the flexibility needed to align the peaks and troughs of the model with the data. Performing this alignment for one peak makes it clear that the period is wrong.\n\n\n\n\nFor some modeling purposes, such as prediction of future tides, the phase information is essential. For others, say, description of the amplitude of the tides, not. But getting the phase roughly right can help point out other problems. For instance, in the left panel of Figure @ref(fig:Fun-4-a-3-4) the blue model is roughly aligned with the data. Not at all so in the right panel. What leads to the discrepancy is a bad estimate for the period. 13 hours is roughly right, but over a five-day period the error accumulates until, in the right panel, the model has a trough where the data peak, and vice versa.\nAlthough the blue sinusoid is not perfect, having it for comparison suggests that the previously estimated period of 13 hours is too long. We can shorten the period gradually in our model until we find something that better matches the data. For example: Figure @ref(fig:Fun-4-a-3-5) shows that a period of 12.3 hours is a good match to the data.\nWith this refinement the model is \\[{\\color{green}{\\text{tide}(t)} \\equiv 1.05 + 0.55 \\sin(2\\pi (t - 17)/12.3)}\\]\n\n\n\n\n\nFigure 16.5: With the phase about right, a better estimate can be made of the period: 12.3 hours.\n\n\n\n\nWe might call it quits with the model in Figure @ref(fig:Fun-4-a-3-5). But once we have a pretty good model fit, it’s easy to polish the parameter estimates, letting the computer do the tedious work of trying little tweaks to see if it can improve the fit.\nThe R/mosaic fitModel() can do this tweaking for us. As the following commands show, fitModel() takes a tilde expression as input. To the left of the tilde goes the name of the function output in the data frame being used. The right side is a formula for the model, with names used for each parameter and using the names of inputs from the data frame. The second argument is the data frame. The third argument is used to convey an estimate for each parameter; that estimate should be pretty good if fitModel() is to be able to refine it.\nThe output from fitModel() is a function, which we’re naming tide_mod().\n\ntide_mod <-\n  fitModel(level ~ A + B*sin(2*pi*(hour-t0)/P),\n  data = RI_tide,\n  start=list(A=1.05, B=0.55, t0=17, P=12.3))\ncoef(tide_mod)\n##          A          B         t0          P \n##  1.0220540  0.4998367 15.3899905 12.5593556\n\nThe command coef(tide_mod) displays the parameters found by fitModel() which will be an improvement—perhaps a big improvement, perhaps not—on our original estimates.\nThese new parameters differ only slightly from the ones shown in Figure @ref(fig:Fun-4-a-3-5), but the match to the data with the new coefficients is discernably better, even by eye.\n\n\n\n\n\nFigure 16.6: Polishing the parameters of the sinusoid\n\n\n\n\nThis last model seems capable of making reasonable predictions, so if we collected up-to-date data we might be able to fit a new model to predict the tide level pretty accurately a few days ahead of time. Also, the excellent alignment of the model peaks with the data tell us that the cyclic tide has a period that is constant, at least so far as we can tell.\nWith the period estimate \\(P=12.56\\) hours, we can go looking for other phenomena that might account for the tides. The period of the day-night cycle is, of course 24 hours. So the tides in Providence come in and out twice a day. But not exactly. Something else must be going on.\nIsaac Newton was the first to propose that the tides were caused by the gravitational attraction of the Moon. A complete cycle of the Moon—moon rise to moon rise—takes about 50 minutes longer than a full day: the Earth revolves once every 24 hours, but in that time the Moon has moved a bit further on in its orbit of the Earth. So the Moon’s period, seen from a fixed place on Earth is about 24.8 hours. Half of this, 12.4 hours, is awfully close to our estimate of the tidal period: 12.56 hours. The difference in periods, 8 minutes a day, might be hard to observe over only 4 days. Maybe with more data we’d get a better match between the tides and the moon.\nThis is the modeling cycle at work: Propose a model form (a sinusoid), adjust parameters to match what we know (the Providence tide record), compare the model to the data, observe discrepancies, propose a refined model. You can stop the model when it is giving you what you need. The period 12.56 hour model seems good enough to make a prediction of the tide level a few days ahead, and is certainly better than the “two tides a day” model. But our model is not yet able to implicate precisely the Moon’s orbit in tidal oscillations.\nDiscrepancies between a model and data play two roles: they help us decide if the model is fit for the purpose we have in mind and they can point the way to improved models. That the tidal data deviates from the steady amplitude of our model can be a clue for where to look next. It’s not always obvious where this will lead.\nHistorically, careful analysis of tides led to a highly detailed, highly accurate model: a linear combination of sinusoids with periods near a half-day 12.42 , 12.00, 12.66, and 11.97 hours as well components with periods that are about a day long 23.93, 25.82, 24.07, 26.87, and 24.00 hours. A tide-prediction model is constructed by finding the coefficients of the linear combination; these differ from locale to locale. There is no global model of tides, but rather a framework of linear combinations of sinusoids of different periods. What customizes the framework to the tides in a particular locale is the coefficients used in the linear combination.\n\nPolishing the phase (optional)\nLinear combination to represent phase shift. \\(\\sin(x + \\phi) = \\cos(\\phi)\\sin(x) + \\sin(\\phi) \\cos(x)\\)\n\\(C \\sin(x +\\phi) = A \\sin(x) + B \\cos(x)\\) where \\(C^2 = A^2 + B^2\\) and \\(\\phi=\\arctan(A/B)\\)"
  },
  {
    "objectID": "Modeling/09-modeling-cycle.html#modeling-project",
    "href": "Modeling/09-modeling-cycle.html#modeling-project",
    "title": "16  Modeling cycle [DRAFT]",
    "section": "16.5 Modeling project",
    "text": "16.5 Modeling project\nThe data frame SunsetLA records the number of minutes after 4 pm until the sun sets in Los Angeles, CA over a 4-year interval from January 2010 (month 1) through December 2013 (month 48).\n\nOpen a sandbox and make a plot of sunset time versus month.\n\n\ngf_point(Minutes ~ Month, data = SunsetLA) %>%\n  gf_line()\n\n\n\n\n\n\n\n\nWe’re using both gf_point() and gf_line(). With data that oscillates up and down, connecting the data points with lines makes it easier to see the pattern.\n\n\n\nPart A What is the range of the number of minutes until sunset over the whole 4-year period?\n40 to 190 minutes120 minutes40 to 180 minutes0 to 48 months\n\n\n\n\nPart B The data fall nicely on a sine-shaped curve. What is the period of that sine?\n6 months11 months12 months12 minutes\n\n\nThe function \\[\\text{sunset}(\\text{Month}) \\equiv A  \\sin(2 \\pi\\, \\text{Month} / 12) + C\\] is a linear combination of two functions:\n\nThe constant function one(Month)\nThe sine function sin(2*pi*Month/12)\n\nThe two functions are scaled by \\(C\\) and \\(A\\), respectively.\nMake rough but reasonable numeric estimates for \\(C\\) and \\(A\\) from the data. Then, in the sandbox, define the sunset() function using the numerical estimates in the linear combination. Plot your function as a layer on top of the data. (Pipe the gf_point() layer to slice_plot().)\n\n\nsunset <- makeFun(A + C*sin(2*pi*(Month - offset)/12), \n                  A = __your estimate__, \n                  C = __your estimate__,\n                  offset=0)\ngf_point(Minutes ~ Month, data = SunsetLA) %>%\n  gf_line() %>%\n  slice_plot(sunset(Month) ~ Month)\n\nThe domain for slice_plot() is inherited to that implied by the SunsetLA data. Notice that the input name in slice_plot() corresponds to that established in gf_point().\n\n\n\nPart C Your sunset() function should be a pretty good match to the data except for one thing. What is that thing?\n\nThe sunset() function has a completely different range than the data.\nThe period of the sunset() function doesn’t match the data.\nThere is a horizontal time shift between sunset() and the data.\n\n\n\nWe’re going to fix the problem with sunset() by defining a time offset to use as a reference. For a sine function, a suitable time offset is the value along the horizontal axis when the phenomenon being modeled crosses \\(C\\) with a positive slope. There are 4 such points along the horizontal axis readily identifiable in the data. (They may not be at an integer value of Month.)\n\n\nPart D Which of these is a suitable value for the time offset?\n0 months19 months21.5 months15.5 months\n\n\nIn the original scaffolding, the value of offset was zero. Change that to match your answer to the previous question.\nPlot out the modified sunset() function and confirm that it is a much better match to the data than the original (that is, the one without the time offset). You can “tune” your function by tweaking the numerical values of the \\(A\\), \\(C\\), and \\(offset\\) parameters until you get a solid match.\nAlternatively, you can use fitModel() to do the tuning for you. Plug in your estimates (a.k.a. “guesses”) for the parameters in place of the ___ in the following. Then run the code. You’ll see your estimate of the function compared to the result of having the computer refine your estimate. Chances are, the computer does a better job of stringing the function through the data.\n\n\n## rough estimates from graph\nrough_A <- __estimated_A__\nrough_C <- __estimated_C__\nrough_offset <- __estimated_offset___\nguessed_fun <-\n  makeFun(A*sin(2*pi*(Month - offset)/12) + C ~ Month,\n          A  = rough_A, C = rough_C,\n          offset = rough_offset)\n\ntuned_fun <-\n  fitModel(Minutes ~ A*sin(2*pi*(Month - offset)/12) + C,\n         data  = SunsetLA,\n         start = list(A = rough_A,\n                      C = rough_C,\n                      offset = rough_offset) )\n\ngf_point(Minutes ~ Month, data = SunsetLA) %>%\n  gf_line(color  = \"dodgerblue\") %>%\n  slice_plot(tuned_fun(Month) ~ Month) %>%\n  slice_plot(guessed_fun(Month) ~ Month,  color = \"orange3\")\n\n\nPerhaps you were expecting the tuned sine function to match the data exactly. It does not. One reason for this is that the Earth’s orbit around the Sun is not exactly circular. The sine function is only a model of the phenomenon, good for some purposes and not for others. For a more complete explanation, see this article on Wikipedia.\n(Thomas Swalm contributed to this project.)"
  },
  {
    "objectID": "differentiation-part.html",
    "href": "differentiation-part.html",
    "title": "Differentiation",
    "section": "",
    "text": "This is where I’ll explain what the block is about and the overall goals."
  },
  {
    "objectID": "Differentiation/16-rate-of-change.html",
    "href": "Differentiation/16-rate-of-change.html",
    "title": "17  Rate of change",
    "section": "",
    "text": "For our purposes, the definition of calculus is\nThe agenda of this chapter is to give specific mathematical meaning to the word “change.”"
  },
  {
    "objectID": "Differentiation/16-rate-of-change.html#change-and-slope",
    "href": "Differentiation/16-rate-of-change.html#change-and-slope",
    "title": "17  Rate of change",
    "section": "17.1 Change and slope",
    "text": "17.1 Change and slope\nYou have an solid, intuitive sense of what “change” means. In mathematics, and especially the mathematics of functions, change has a very simple meaning that you have already touched on in your previous math education.\nThe word that encapsulates “change” in high-school math is slope. For instance, you’ve undoubtedly had to calculate the slope of a straight line in a graph. You learned about “rise” and “run” and how to read them from a graph or from a formula. The slope is the ratio: rise over run.\nEveryone has a intuitive sense of the slope of a road or of a hillside. You learned to apply this intuition to reading graphs and the slope of a line. We’ll exploit the intuitive ability to read a landscape in order to introduce abstract mathematical ideas in a down-to-earth setting.\nMathematical modelers learn to think of “slope” abstractly, not just as referring to the incline of a road. For instance, the population of a country can change, as can the number of new cases of an epidemic disease, the temperature of a cup of coffee, or the distance from Earth of a spacecraft. A major part of learning calculus is generalizing and abstracting the mathematical concept of which “slope” is an example."
  },
  {
    "objectID": "Differentiation/16-rate-of-change.html#continuous-change",
    "href": "Differentiation/16-rate-of-change.html#continuous-change",
    "title": "17  Rate of change",
    "section": "17.2 Continuous change",
    "text": "17.2 Continuous change\nMost people are comfortable with the ideas of daily changes in temperature or monthly changes in credit-card debt or quarterly changes in the unemployment rate or annual changes in the height of a child. Such things are easy to record in, say, a spreadsheet. For example, as this paragraph is being written, the weather forecast for the next several days (in southeastern Colorado in mid-May) is\nMAYBE SEASONAL CHANGE\n\n\n\nDay\nHigh\nLow\nDescription\n\n\n\n\nThursday\n73\n43\nsunny\n\n\nFriday\n72\n48\nwindy\n\n\nSaturday\n66\n48\nthunderstorms\n\n\nSunday\n68\n43\nwindy\n\n\nMonday\n70\n39\nsunny\n\n\nTuesday\n70\n43\nsunny\n\n\nWednesday\n66\n45\npartly cloudy\n\n\n\nSuch data is said to be discrete. The day is listed, but not the time of day. The high temperature is forecast, but not the time of that high. The “description” is also discrete, one of the several words that are used to summarize the quality of the weather, as opposed to the quantity of rain.\nCalculus is about continuous change. For instance, if the weather bureau provide a web interface that let me enter the date and time to the nearest fraction of a second, they would be giving a way to track the change continuously. Many physical processes are intrinsically continuous, for instance the motion (change in position) of a spacecraft or the height of the tide or the stress on a tree as a function of wind velocity.\nFinding a language to describe continuous change—famously, the position of the moon or planets in their orbit, or the speed of a ball rolling down a ramp—was central to the emergence of what historians call the “Age of Enlightenment” or “modern scientific method.” The first complete presentation of that language was published by Isaac Newton based on his work in the 1660s. As you might guess, the name of the language is “calculus.”"
  },
  {
    "objectID": "Differentiation/16-rate-of-change.html#slope",
    "href": "Differentiation/16-rate-of-change.html#slope",
    "title": "17  Rate of change",
    "section": "17.3 Slope",
    "text": "17.3 Slope\nYou already know pretty much everything there is to know about the straight-line function,\n\nFormula: \\(f(x) \\equiv a x + b\\). The parameters \\(a\\) and \\(b\\) are the “slope” and “intercept” respectively. (More precisely, \\(b\\) is the “y-intercept.” But in statistics and modeling, it’s just the “intercept.”)\nReading parameters from a graph: You learned several ways to do this which are all equivalent. Maybe the easiest is to read the y-intercept off the graph. That’s \\(b\\). Then choose some non-zero \\(x_0\\) and read off from the graph the value of \\(f(x_1)\\). The slope is simply \\[\\frac{f(x_0) - b}{x_0}\\]\nThe y-intercept method is a special case of a more general method, the two-point method, that you can use even if the y-intercept isn’t shown on the graph. Pick two specific values of \\(x\\), which we’ll call \\(x_0\\) and \\(x_1\\). Evalate the function at these input values and compute the rise over run: \\[\\text{rise over run} \\equiv \\frac{f(x_1) - f(x_0)}{x_1 - x_0}\\] The rise over run is the slope of the straight line.\nThe y-intercept method is the same as the two-point method with \\(x_1 = 0\\).\nMatching a straight-line function to data: You might not have been taught this formally, but the basic process is easy to imitate. The process is called line fitting or, in statistics and other fields, linear regression."
  },
  {
    "objectID": "Differentiation/16-rate-of-change.html#average-rate-of-change",
    "href": "Differentiation/16-rate-of-change.html#average-rate-of-change",
    "title": "17  Rate of change",
    "section": "17.4 Average rate of change",
    "text": "17.4 Average rate of change\nSince the slope is our standard way of representing a relationship of change, we will often use it as a way of summarizing a function. To illustrate, consider the exponential model we constructed to match the cooling-water data in ?sec-fit-exponential:\n\nwater <- makeFun(60*exp(-0.0173*t) + 25 ~ t)\n\n\n\n\n\n\nFigure 17.1: The exponential function that was previously matched to the cooling-water data. The slope of the straight line connecting two points on the function graph is the average rate of change during the interval.\n\n\n\n\nDuring the interval \\([t_0, t_1]\\) the rate at which the water cools is higher at first and lower at the end. The average rate of change is a single number that summarizes the whole interval.\nFor all except straight-line models, the average rate of change depends on the interval chosen.\n\n“Slope” is a natural metaphor when thinking of a function as a graph. But a more general way to describe the concept is the rate of change of the output with respect to the input. The change in the output from one end of the interval is \\(f(x_1) - f(x_0)\\), the change in the input is \\(x_1 - x_0\\). If the input is time (in hours), and the output is the position of a car (in miles), then the rate of change is miles-per-hour: the car’s velocity.\nFor a straight-line function—think of a car driving at constant speed on a highway—it doesn’t matter what you choose for \\(x_1\\) and \\(x_0\\) (so long as they are not identical). But for other functions, the choice does matter.\nImagine a graph of the position of a car along a road as in Figure 17.2. Over the course of an hour, the car travelled about 25 miles. In other words, the average speed is 25 miles/hour: the slope of the tan line segment. Given the traffic, sometimes the car was stopped (time C), sometimes crawling (time D) and sometimes much faster than average (time B).\n\n\n\n\n\nFigure 17.2: The position of an imaginary car over time (black curve). The average rate of change over various intervals is the slope of the straight-line segment connecting the start and end of the black curve in that interval.\n\n\n\n\nDuring the interval from B to C, the car was travelling relatively fast. The slope of the \\(\\color{magenta}{\\text{magenta}}\\) segment connecting the position at times B and C is the average rate of change between times B and C. It’s easy to see that the average rate of change from B to C is larger than the overall average from \\(t=0\\) to \\(t=1\\). Calculating that slope is a matter of evaluating the position at the endpoints and dividing by the length of the interval.\n\n\nWhat is the average rate of change in the car’s position during the interval \\(t_B = 0.40\\) to \\(t_C=0.54\\)?\nThe length of the interval is \\(t_C - t_B = 0.54-0.40=0.14\\).\nEvaluating the function gives \\(x(t_C) = 18\\) and \\(x(t_B) = 12.6\\).\nRise is \\(x(t_C) - x(t_B) = 18 - 12.6 = 5.4\\).\nRun is \\(t_C - t_B = 0.54-0.40=0.14\\).\nThe average rate of change during the interval is \\(5.4/0.14 = 38.6\\) miles/hour.\n\n\nFigure 17.3 shows a simplified model of the amount of usable wood that can be harvested from a typical tree in a managed forest of Ponderosa Pine. (You can see some actual forestry research models here.)\n\n\n\n\n\nFigure 17.3: A model, somewhat realistic, of the amount of wood that can be harvested from a Ponderosa Pine as a function of years since planting to harvest.\n\n\n\n\nYou are writing a business plan for a proposed pine forest. Among other things, you have to forecast the revenue that will be generated and when you will have saleable product.\nThey say that “time is money.” Every year you wait before harvest is another year that you don’t have the money. On the other hand, every year that you wait means more wood at the end. How to decide when to harvest?\nThe tree continues to grow until year 50, when it seems to have reached an equilibrium: perhaps growth goes to zero, or rot balances what growth there is. There’s no point waiting until after year 50.\nAt year 25, the tree is growing as fast as it ever will. You’ll get about 600 board-feet of lumber. Should you harvest at year 25? No! That the tree is growing so fast means that you will have a lot more wood at year 26, 27, and so on. The time to harvest is when the growth is getting smaller, so that it’s not worth waiting an extra year.\nThe quantity of interest is the average rate of growth from seedling to harvest. Harvesting at year 25 will give a total change of 600 board feet over 25 years, giving an average rate of change of \\(600 \\div 25 = 24\\ \\text{board-feet-per-year}\\). But if you wait until year 35, you’ll have about 900 board feet, giving an average rate of change of \\(900 \\div 35 = 25.7\\  \\text{board-feet-per-year}\\).\nIt’s easy to construct a diagram that shows whether year 35 is best for the harvest. Recall that our fundamental model of change is the straight-line function. So we’re going to model the model of tree growth as a straight line function. Like the more realistic model, our straight-line model will start out with zero wood at the time of planting. And to be faithful to the realistic model, we’ll insist that the straight-line intersect or touch the realistic model at some point in the future.\nFigure 17.4 reiterates the realistic model of the tree but adds on to it several straight-line models that all give zero harvest-able wood at planting time. Each of the green lines captures a scenario where the tree is harvested at the indicated time: \\(t_1\\), \\(t_2\\), and so on. For the perspective of representing the rate of growth per year from planting to harvest, the straight-line green models do not need to replicate the actual growth curve. The complexities of the curve are not relevant to the growth rate, which can be simplified down to a straight-line model connecting the output at planting time to the output at harvest time. In contrast, the \\(\\color{magenta}{\\text{magenta}}\\) curve is not a suitable model because it doesn’t match the situation at any harvest time; it doesn’t touch the curve anywhere after planting!\n\n\n\n\n\nFigure 17.4: Modeling the tree-growth model with straight lines connecting planting time to various harvest times. The slope of each line is the average rate of growth for that planting time.\n\n\n\n\nTo maximize average lumber volume per year, choose a harvest time that produces the steepest possible green segment. From Figure 17.4 this is the model that glances the growth curve near year 31 (shown as \\(t_3\\) in the diagram).\nIt’s best to find the argmax by creating a function that shows explicitly what one is trying to optimize. (In Chapter 24 we’ll use the name objective function to identify such function.) Here, the objective function is \\(\\text{ave.growth(year)} \\equiv \\text{volume(year)} / \\text{year}\\). See Figure 17.5.\n\n\n\n\n\nFigure 17.5: Graph of the average-growth function ave_growth(year), constructed by dividing volume(year) by year.\n\n\n\n\nThe graph of ave_growth(year) makes it clear that the maximum average growth from planting to harvest will occur at about year 32."
  },
  {
    "objectID": "Differentiation/16-rate-of-change.html#sec-instantaneous-rate-of-change",
    "href": "Differentiation/16-rate-of-change.html#sec-instantaneous-rate-of-change",
    "title": "17  Rate of change",
    "section": "17.5 Instantaneous rate of change",
    "text": "17.5 Instantaneous rate of change\nThe average rate of change is the slope of a line segment connecting two points on the graph of a function, the points \\(\\left(\\strut t_0, f(t_0)\\right)\\) and \\(\\left(\\strut t_1, f(t_1)\\right)\\). It reflects all the point-to-point changes in the value of the function over the interval \\(t_0\\) to \\(t_1\\) in the function’s domain.\nIt turns out to be helpful to consider the rate of change of a function at an individual point \\(t_0\\) in the domain, rather than the interval between two points. This rate of change at a point is called the instantaneous rate of change. In Block 2, we’ll see that a good way to define an instantaneous rate of change at \\(t_0\\) is as the average rate of change over the interval \\(t_0 \\leq t \\leq t_0 + h\\) with the proviso that the interval length \\(h\\) goes as closely as it can to zero. Visually, this is the line that’s tangent to the function’s graph at the input value \\(t_0\\) as in Figure 17.6.\n\n\n\n\n\nFigure 17.6: A line tangent to a the curve at a single point. The slope of this line is the instantaneous rate of change.\n\n\n\n\nIt’s convenient to be able to find the slope of such a tangent line using just the definition \\(f(t)\\), rather than having to draw a graph and eyeball the tangent. For now, let’s approximate the slope of tangent line by the average rate of change over a small run from \\(t_0\\) to \\(t_0 + 0.1\\): \\[\\text{slope of}\\ f(t) \\ \\text{at}\\ t_0 \\approx\\frac{f(t_0 + 0.1) - f(t_0)}{0.1} = \\frac{\\text{amount of rise}}{\\text{length of run}}\\] The \\(\\approx\\) symbol means “is approximately.” For now, I want to put off the question of what “approximately” means. In modeling, whether the 0.1 gives a good enough approximation will depend on the function \\(f()\\) and the context in which the slope is needed. For instance, in drawing Figure 17.6 I needed to find the tangent line. Using 0.1 is entirely satisfactory in this setting but it might not be in other settings.\nThe notation “slope of \\(f(t)\\) at \\(t=t_0\\)” is long-winded and awkward. If we were looking at the “value of \\(f(t)\\) at \\(t_0\\) we have at hand a much more concise notation: \\(f(t_0)\\). But it doesn’t work to write”slope of \\(f(t_0)\\)” because \\(f(t_0)\\) is a quantity and not a function. Instead, let’s make a concise notation for “slope of \\(f(t)\\).” Following tradition, we’ll write \\({\\cal D}f(t)\\). The name of this “slope of \\(f(t)\\)” function is \\({\\cal D}f()\\): a two-letter name. When we want to say, “the (approximate) slope of the tangent line to \\(f(t)\\) at \\(t_0\\), we can write simply: \\[{\\cal D}f(t_0)\\] meaning, evaluate the”slope function of f()” at \\(t_0\\).\nTo formalize this, we’ll define the slope function of f() as \\[{\\cal D}f(t) \\equiv \\frac{f(t + 0.1) - f(t)}{0.1}\\] Let’s look at the slope functions that correspond to some of pattern-book functions: \\(e^x\\), \\(\\sin(x)\\), \\(x^{-1}\\) and \\(\\ln(x)\\). We can define them easily enough in R:\n\nDexp <- makeFun((exp(t+0.1) - exp(t))/0.1 ~ t)\nDsin <- makeFun((sin(t+0.1) - sin(t))/0.1 ~ t)\nDxm1 <- makeFun(((1/(t+0.1)) - (1/t))/0.1 ~ t)\nDlog <- makeFun((log(t+0.1) - log(t))/0.1 ~ t)\n\n\n\n\n\n\nFigure 17.7: Comparing the pattern-book function (blue) to its slope function (tan)\n\n\n\n\n\n\n\nFigure 17.8: Comparing the pattern-book function (blue) to its slope function (tan)\n\n\n\n\n\n\n\nFigure 17.9: Comparing the pattern-book function (blue) to its slope function (tan)\n\n\n\n\n\n\n\nFigure 17.10: Comparing the pattern-book function (blue) to its slope function (tan)\n\n\n\n\n\nWhy did you plot both the function and the slope function in the same graphics frame?\nExcellent question! In general, it is illegitimate to plot a function and it’s slope function on the same vertical axis. The reason is the units of the two functions will be different. For instance, the output of a function position(t) might have units of “miles,” while the output of the slope function of position (that is, \\({\\cal D}\\)position(t) would have units such as miles-per-hour.) So, as a general rule, never plot a function and its corresponding slope function on the same scale.\nAn exception is for the pattern-book functions. These always take a number as input and produce a number as output. The slope function of a pattern-book function also produces a number as output.\nThis exception is not a good excuse for indulging a bad practice. Perhaps you’ll forgive the authors if they claim they wanted to emphasize the point by demonstrating it.\n\n\nHere, we write the slope function of \\(f(t)\\) as \\({\\cal D}f(t)\\). That works for this chapter, which deals with functions with only one input. But in general modeling functions have more than one input, for instance \\(g(x, t)\\). To work with slope functions with more than one input, we need to extend the notation a little. We will place a small subscript after \\({\\cal D}\\) to indicate which input we are changing. Thus, there will be two slope functions for \\(g(x,t)\\): \\[{\\cal D}_{\\color{blue}x} g(x, t) \\equiv \\frac{g({\\color{blue}x + 0.1}, t) - g(x, t)}{0.1}\\] and\n\\[{\\cal D}_{\\color{magenta}t} g(x, t) \\equiv \\frac{g(x, {\\color{magenta}t + 0.1}) - g(x, t)}{0.1}\\] The input referred to in the subscript following \\({\\cal D}\\) is called the with-respect-to input.\n\nThe idealization of the slope function involves replacing \\(h=0.1\\) with something much smaller. What “much smaller” means has been a complicated issue in the history of calculus. Today, we write \\(h \\rightarrow 0\\) to signify the process of making \\(h\\) smaller and smaller, but never zero. When \\(h\\) has this “much smaller” value, the rate of change over the interval \\(x\\) to \\(x+h\\) becomes a rate of change “at \\(x\\)”, also called the instantaneous rate of change at \\(x\\). For the pattern-book functions, \\(h=0.1\\) or smaller gives a pretty good approximation to the instantaneous rate of change. Later, in Block 2, we’ll see how to arrange \\(h\\) so that it’s “much smaller” for functions in general.\n\nIn the previous section we looked at the optimal time to harvest a tree so that the average rate of growth in usable lumber over the tree’s life is maximized. Using a model of tree growth of a ponderosa pine we found the best harvest time to be 32 years.\nLet’s return to the modeling phase of the wood-harvest problem with a new perspective. The real objective of tree farming is to maximize the economic value of the wood. This depends on the market price of the wood which itself may be changing in time. A market-savvy modeler will want to exploit any information about the possibility of rising or falling prices in selecting the best harvest time. Companies often hire economists to forecast market trends, but this requires a deep knowledge of trends in supply and demand which is out of the scope of what we can cover in this book.\nHowever, there is one economic principle that we can incorporate into the model without such detailed, industry specific expertise. This is the economic principle of opportunity cost.\nOpportunity cost takes into account when valuing an asset the other possible uses of that asset. For example, lumber companies constantly invest in planting new trees for future harvest. In order to do this, they borrow money and they pay interest on the borrowed money. They need to borrow because their existing assets are tied up in the form of wood. The opportunity cost of not harvesting a tree is the interest on the loan the company needs to take out in order to invest for the future.\nBetween year 30 and 32, there is hardly any change in the value of the average-rate-of-change function. It’s increasing a little, but is it really worthwhile to wait? One argument is that at year 30 you already have a valuable resource: wood that could be money in the bank. If the money were in the bank, you could invest it and earn more money and at the same time get a new seedling in the ground to start its growth. You’re doing two things at once. Efficient!\nTo know what is the best year for harvest from this point of view, you want to calculate the effective “interest rate” on the present amount of wood that you earn in the form of new wood. That interest rate is the ratio of the instantaneous rate of growth of new wood divided by the amount of existing wood. Figure 17.11 shows this function.\n\n\n\n\n\nFigure 17.11: The instantaneous investment return from the tree is the instantaneous rate of change in wood volume divided by the wood volume itself. This falls over the age of the tree as the harvestable wood volume increases.\n\n\n\n\nEarly in the tree’s life, the growth is high compared to the volume of the tree. That’s because the tree is small. As the years pass, the tree gets bigger. Even though the rate of growth increases through year 23, the accumulated volume increases even faster, so there is a fall in the rate of return.\nThe best time to harvest is when the annual “interest rate” paid by the growing tree falls to the level of the next best available investment. Suppose that investment would pay 10% per year. Then harvest the tree when the function values falls below 10%. That happens at year 24. If the next best investment paid only 5% (blue horizontal line), the harvest should be made at about year 29. If money could be borrowed at 2%, it would be worthwhile to harvest the tree still later."
  },
  {
    "objectID": "Differentiation/16-rate-of-change.html#other-old-stuff",
    "href": "Differentiation/16-rate-of-change.html#other-old-stuff",
    "title": "17  Rate of change",
    "section": "17.6 Other old stuff",
    "text": "17.6 Other old stuff\nThe rate of change is based on a simple question: If the input changes from \\(x = A\\) to \\(x = B\\), how much does the output change? Of course, the output from function \\(f(x)\\) will be \\(f(x=A)\\) and \\(f(x=B)\\) respectively. The rate-of-change relationship is the ratio \\[\\frac{\\color{red}{f(x=B) - f(x=A)}}{\\color{blue}{B-A}}\\ \\ \\text{also written}\\ \\ \\frac{\\color{red}{\\text{rise}}}{\\color{blue}{\\text{run}}}\\]\nWhy do we focus on the rate of change rather than something simpler, for example the net change \\(\\color{red}{f(x=B) - f(x=A)}\\)? The reason goes back to a scientific breakthrough in the 1600s: the writing down of the Newton’s laws of motion. The language in which these laws were first successfully expressed is the language of rates of change. In the intervening 400 years, the laws have been updated with the theory of relativity and quantum mechanics. These laws too are expressed as rates of change. In undertaking to study just about any quantitative field from engineering to economics you’ll find that theory is expressed using functions and rates of change.\nYou may recognize in the formula for the rate of change a familiar quantity: the slope of a line. Everyone understands what a line is, but the geometry is not our primary concern here. We describe relationships using functions and for us the straight-line function will be a fundamental way of expressing a relationship. Straight-line functions can be written in several ways, but we’ll tend to use two predominant forms: \\[\\line(x) \\equiv a x + b\\ \\ \\ \\text{or}\\ \\ \\ \\ \\line(x) \\equiv a [x - x_0]\\] The two forms are interchangeable, but as you’ll see in upcoming chapters, sometimes it’s more convenient to use one form or the other. In either case, the rate of change is, quantitatively, the value of the parameter \\(a\\).\nThe simple function \\(\\line(x)\\), whose change relationship we understand intuitively, will be used to approximate more complicated change relationships. With the approximation in place, we can do calculations about the change relationships much more easily. Collectively, the set of mathematical concepts and techniques that support describing and calculating on change relationships has the name Calculus."
  },
  {
    "objectID": "Differentiation/16-rate-of-change.html#exercises",
    "href": "Differentiation/16-rate-of-change.html#exercises",
    "title": "17  Rate of change",
    "section": "17.7 Exercises",
    "text": "17.7 Exercises"
  },
  {
    "objectID": "Differentiation/16-rate-of-change.html#drill",
    "href": "Differentiation/16-rate-of-change.html#drill",
    "title": "17  Rate of change",
    "section": "17.8 Drill",
    "text": "17.8 Drill\n\n\nPart i What is the instantaneous rate of change of this function when the input is \\(t=-3\\)?(Choose the closest answer.)\n0.511.52\n\n\n\n\nPart ii What is the average rate of change of this function on the interval $-5 < t < 5?(Choose the closest answer.)\n0.511.52\n\n\n\n\nPart iii Consider the average rate of change of this function on the interval \\(-2 < t < 2\\). Which of these statements best describes that average rate of change.\n\nvery close to 1/2\nslightly less than 1\nslightly greater than 1\nvery close to 2"
  },
  {
    "objectID": "Differentiation/17-intro.html",
    "href": "Differentiation/17-intro.html",
    "title": "18  Change relationships",
    "section": "",
    "text": "As you know, a function is a mathematical idea used to represent a relationship between quantities. For instance, the water volume of a reservoir behind a dam varies with the seasons and over the years; there is a relationship between water volume (one quantity) and time (another quantity). Similarly, the flow in a river feeding the reservoir has a relationship with time. In spring the river may be rushing with snow-melt, in late summer the river may be dry, but after a summer downpour the river flow again rises briefly. In other words, river flow is a function of time.\nDifferentiation is a way of describing a relationship between relationships. The water volume in the reservoir has a relationship with time. The river flow has a relationship with time. Those two relationships are themselves related: the river flow feeds the reservoir and thereby influences the water volume.\nIt’s not so easy to keep straight what’s going on in a “relationship between relationships.” When you can describe such a thing, it often gives great insight to the mechanisms that drive the world. For instance, Johannes Kepler (1572-1630) spent years analyzing the data collected by astronomer Tycho Brahe (1546-1601). The data showed clearly a relationship between time and the speed of a planet across the sky. Long-standing wisdom claimed that there is also a specific relationship between a planet’s position and time. From antiquity it had been certain that planets moved in circular orbits. Kepler worked hard to find the relationship between the two relationships: speed vs time and position vs time. He was unsuccessful until he dropped the assumption that planet orbits are circular. Positing orbits as elliptical, Kepler was able to find a simple relationship between speed vs time and position vs time.\nBuilding on Kepler’s work, Newton hypothesized that planets might be influenced by the same gravity that pulls an apple to the ground. It was evident from human experience that gravity has the most trivial relationship with time: gravity is constant! But Newton could not find a link between this constant notion of gravity and Kepler’s planetary motion as a function of time. Success came when Newton hypothesized—without any direct evidence from experience—that gravity is a function of distance. Newton’s formulation of the relationship between relationships— gravity-as-a-function-of-distance and orbital-position-as-a-function-of time—became the a foundation of model science. Newton’s theories of gravity, force, and motion created an extremely complicated chain or reasoning that is still almost impossible to grasp. Or, more precisely, it is almost impossible to grasp until you have the language for describing relationships between relationships. Newton invented this language, the language of differentiation. As you learn to understand this language, you will find it easier to express and understand relationships between relationships, that is, the mechanisms that account for the ever changing quantities around us."
  },
  {
    "objectID": "Differentiation/17-intro.html#mathematics-in-motion",
    "href": "Differentiation/17-intro.html#mathematics-in-motion",
    "title": "18  Change relationships",
    "section": "18.1 Mathematics in motion",
    "text": "18.1 Mathematics in motion\nThe questions that started it all had to do with motion. There were words to describe speed: fast and slow. There were words to describe force: strong and weak, heavy and light. And there were words to describe location and distance: far and near, long and short, here and there. But what were the relationships among these things? And how did time fit in, an intangible quantity that had aspects of location (long and short) and of speed (quick and slow)?\nGalileo (1564-1642) started the ball rolling.1 As the son of a musician and music theorist, he had a sense of musical time, a steady beat of intervals. When a student of medicine in Pisa, he noted that swinging pendulums kept reliable time, regardless of the amplitude of their swing. After accidentally attending a lecture on geometry, he turned to mathematics and natural philosophy. Inventing the telescope, his observations put him on a collision course with the accepted classical truth about the nature of the planets. Seeking to understand gravity, he built an apparatus that enabled him to measure accurately the position in time of a ball rolling down a straight ramp. The belled gates he set up to mark the ball’s passage were spaced arithmetically in musical time: 1, 2, 3, 4, …. But the distance between the gates was geometric: 1, 4, 9, 16, …. Thus he established a mathematical relationship between increments in time and increments in position. Time advanced as 1, 1, 1, 1, … and position as 1, 3, 5, 7, …. He observed that the second increments of position, the increments of the increments 1, 3, 5, 7, …, were themselves evenly spaced: 2, 2, 2, ….\nPutting these observations in tabular form, and adding columns for the\n\nfirst increment \\(y(t) \\equiv x(t+1) - x(t)\\) and the\nsecond increment \\(y(t+1) - y(t)\\)\n\n\n\n\n\n\n\\(t\\)\n\\(x(t)\\)\nfirst increment\nsecond increment\n\n\n\n\n0\n0\n1\n2\n\n\n1\n1\n3\n2\n\n\n2\n4\n5\n2\n\n\n3\n9\n7\n\n\n\n4\n16\n\n\n\n\n\nGalileo had neither the mathematics nor the equipment to measure motion continuously in time. So what might be obvious to us now, that position is a function of time \\(x(t)\\), would have had little practical significance to him. But we discover in his first increments of \\(x\\) something very much like the slope function in ?sec-fun-slopes.\n\\[{\\cal D}_t\\, x(t) \\equiv \\frac{x(t + 1) - x(t)}{1}\\] From his data, he observed that \\({\\cal D}_t\\, x(t)\\) increases linearly in \\(t\\): \\[{\\cal D}_t x(t) = 2 t + 1\\]\nCalculating the second increments of \\(x\\) is done by the “slope function of the slope function,” which we can call \\({\\cal D}_{tt}\\): \\[{\\cal D}_{tt} x(t) \\equiv {\\cal D}_t \\left[{\\cal D}_t x(t)\\right] = {\\cal D_t} \\left[\\strut 2t+1\\right] = \\frac{\\left[\\strut2(t+1) + 1\\right] - \\left[\\strut 2 t + 1\\right]}{1} = 2\\]"
  },
  {
    "objectID": "Differentiation/17-intro.html#continuous-time",
    "href": "Differentiation/17-intro.html#continuous-time",
    "title": "18  Change relationships",
    "section": "18.2 Continuous time",
    "text": "18.2 Continuous time\nNewton placed the motion in continuous time rather than Galileo’s discrete time. He reframed the slope function from the big increments of the slope operator \\({\\cal D}_t\\) to imagined vanishingly small increments of a operator that we shall denote \\(\\partial_t\\) and call differentiation.\nThe kind of question for which Newton wanted to be able to calculate the answer was, “How to find the function \\(x(t)\\) whose second increment, \\(\\partial_{tt} x(t) = 2\\)?” His approach, which he called the “method of fluxions,” became so important that its name became, simply, “Calculus.”\nOver the next three centuries, calculus evolved from a set of techniques for describing motion into the general-purpose mathematics of change. Applying calculus in the real world involves understanding change relationships between quantities. To give some examples:\n\nElectrical power is the change with respect to time of electrical energy.\nBirth rate is one component of the change with respect to time of population.\nInterest, as in bank interest or credit card interest, is the change with respect to time of assets.\nInflation is the change with respect to time of prices.\nDisease incidence is one component of the change with respect to time of disease prevalence.\nForce is the change with respect to position of energy."
  },
  {
    "objectID": "Differentiation/17-intro.html#instantaneous-rate-of-change",
    "href": "Differentiation/17-intro.html#instantaneous-rate-of-change",
    "title": "18  Change relationships",
    "section": "18.3 Instantaneous rate of change",
    "text": "18.3 Instantaneous rate of change\nOn the radio once, I heard a baseball fanatic describing the path of a home run slammed just inside the left-field post. “Coming off the bat, the ball screamed upwards, passing five stories over the head of the first baseman and still gaining altitude. Then, somewhere over mid left-field, gravity caught up with the ball, forcing it down faster and faster until it crashed into the cheap seats.” A nice image, perhaps, but wrong about the physics. Gravity doesn’t suddenly catch hold of the ball; even when upward bound, gravity influences the ball to the same extent as it does at the peak of the flight and as the ball falls back down. The vertical velocity of the ball is positive while climbing and negative on descent, but that velocity is steadily changing all through the flight: a smooth, almost linear numerical decrease in velocity from the time the ball leaves the bat to when it lands in the bleachers.\nAt each instant of time, the vertical velocity of the ball has a numerical value in feet-per-second. That value changes continuously and is never the same at any two points in the ball’s flight. If \\(Z(t)\\) is the height of the ball at time \\(t\\), and \\(v_Z(t)\\) is the vertical velocity at time \\(t\\), then the slope function \\[{\\cal D}_t Z(t) \\equiv \\frac{Z(t+h) - Z(t)}{h}\\] tells us the average velocity of the ball over a time interval of \\(h\\).\nThe “average velocity” is a human construction. At each instant in time the ball has a velocity that is constantly changing. The reality of the ball is that it has only an instantaneous velocity. The average velocity is merely a concession to the way we might measure the velocity, by recording the height at two different times and computing the difference in height divided by the difference in time.\nOur measurement of the average velocity gets closer to the instantaneous velocity when we make the time interval \\(h\\) smaller. Ideally, to genuinely reflect the state of the ball at a instant, we would make the interval of time infinitely small, that is, we would make \\(h = 0\\).\nOne thing that happens when we make \\(h = 0\\) is that the formula for \\({\\cal D}_t Z(t)\\) suffers from a divide by zero; a meaningless arithmetic operation. So it would seem that “instantaneous velocity” is a mathematical non-starter, even if it is a physical reality. But there’s something else that happens when \\(h = 0\\), the two heights \\(Z(t + h)\\) and \\(Z(t)\\) become equal, so \\(Z(t + h) - Z(t) = 0\\). Not only are we dividing by zero when calculating \\({\\cal D}_t Z(t)\\), the quantity that we are dividing zero into is itself zero. We have 0/0. That’s a doubly mysterious quantity, an arithmetic non-entity.\nThe mystery of 0/0 baffled mathematicians and philosophers for thousands of years. It was Newton who turned it into a computational reality, although his reasoning was regarded with suspicion for two hundred years.\nThe world’s best mathematicians struggled for centuries with the logic of finding a mathematical framework for making sense of what a baseball and gravity do naturally. Rather than ourselves dealing with the intricacies of mathematical logic, we can gain an adequate understanding of the situation by avoiding \\(h=0\\) in favor of a gentler, gradual, evanescent h.\nThe type of slope function calculated with this (as yet undefined) evanescent h is called a derivative and corresponds to the instantaneous rate-of-change function. The process of constructing the derivative of a function \\(f(t)\\) is called differentiation. And to help us keep track of things, whenever we construct a derivative of \\(f(t)\\), we will name the constructed function \\(\\partial_t f(t)\\). Similarly, the name of the function that is the derivative of \\(g(x)\\) will be \\(\\partial_x g(x)\\)"
  },
  {
    "objectID": "Differentiation/17-intro.html#slopes-and-motion",
    "href": "Differentiation/17-intro.html#slopes-and-motion",
    "title": "18  Change relationships",
    "section": "18.4 Slopes and motion",
    "text": "18.4 Slopes and motion\nConsider a graph of the position of a car along a road as in Figure 17.2. Over the course of an hour, the car traveled about 25 miles. In other words, the average speed is 25 miles/hour: the slope of the tan-colored line segment. Given the traffic, sometimes the car was stopped (time C), sometimes crawling (time D) and sometimes much faster than average (time B)."
  },
  {
    "objectID": "Differentiation/18-evanescent-h.html",
    "href": "Differentiation/18-evanescent-h.html",
    "title": "19  Evanescent h",
    "section": "",
    "text": "In the very early days of calculus, the vanishing \\(h\\) was described as “evanescent.” (Dictionary definition: “tending to vanish like vapor.”1) Another good image of \\(h\\) becoming as small as possible comes from the same University of Oxford mathematician whose poem The Jabberwocky we introduced in ?sec-fun-notation. In Alice in Wonderland, Dodgson introduced the character of the Cheshire Cat.\n\n\n\n\n\nFigure 19.1: Vanishing \\(h\\) in the form of the Chesire Cat from Alice in Wonderland.\n\n\n\n\n\n\n\n\n\n\nFigure 19.2: The pattern-book sigmoidal function. A vertical blue line marks the input \\(t=0\\).\n\n\n\nStart our story with two of the basic modeling functions that, like the characters from Alice in Wonderland, have considerable “personality”: the sinusoid (sin()) and the sigmoid (pnorm()).\nThe computer can easily construct the slope functions for the sinusoid and sigmoid, which we’ll call Dsin() and Dsigma() respectively.\n\nDsin   <- makeFun((  sin(t+h) -   sin(t))/h ~ t, h=0.1)\nDsigma <- makeFun((pnorm(t+h) - pnorm(t))/h ~ t, h=0.1)\n\n\n\n\n\n\n\nFigure 19.3: The pattern-book sinusoid.\n\n\n\nIn the tilde expression handed to makeFun(), we’ve identified t as the name of the input and given a “small” default value to the h parameter. But R recognizes that both Dsin() and Dsigma() are functions with two inputs, t and h, as you can see in the parenthesized argument list for the functions.\n\nDsin\n## function (t, h = 0.1) \n## (sin(t + h) - sin(t))/h\nDsigma\n## function (t, h = 0.1) \n## (pnorm(t + h) - pnorm(t))/h\n\nThis is a nuisance, since when using the slope functions we will always need to think about h, a number that we’d like to describe simply as “small,” but for which we always need to provide a numerical value. A surprisingly important question in the development of calculus is, “What can we do to avoid this nuisance?” To find out, let’s look at Dsin() and Dsigma() for a range of values of h, as in Figure 19.4.\n\n\n\n\n\nFigure 19.4: The slope functions of the sinusoid and sigmoid. Each curve shows the slope function for a particular numerical choice of h. Both panels show \\(h=2, 1, 0.5, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001\\).\n\n\n\n\nSome observations from this numerical experiment:\n\nAs \\(h\\) gets very small, the slope function doesn’t depend on the exact value of \\(h\\). As you can see in Figure 19.4, the graphs of the functions with the smallest \\(h\\) (blue), with labels near the top of the graph) lie on top of one another.\nThis will provide a way for us, eventually, to discard \\(h\\) so that the slope function will not need an \\(h\\) argument.\nFor small \\(h\\), we have \\(\\partial_t \\sin(t) = \\sin(t + \\pi/2) = \\cos(t)\\). That is, taking the slope function of a sinusoid gives another sinusoid, shifted left by \\(\\pi/2\\) from the original. Or, in plain words, for small \\(h\\) the cosine is the slope function of the sine.\nFor small \\(h\\), we have \\(\\partial_t \\pnorm(t) = \\dnorm(t)\\). That is, for small \\(h\\) the gaussian function is the slope function of the sigmoid \\(\\dnorm()\\) function.\n\nYou can confirm these last two statements by comparison with the original functions, especially the alignment of the peaks of the slope functions with respect to the peak of the sinusoid and the half-way point of the sigmoid.\n\nHere you use \\(t\\) as the name of the input and \\(\\partial_t\\) as the notation for differentiation. Previously in this block you used \\(x\\) as the input name and \\(\\partial_x\\) for differentiation. Are they the same?\nMathematically, the name of the input makes no difference whatsoever. We could call it \\(x\\) or \\(t\\) or \\(y\\) or Josephina. What’s important is that the name be used consistently on the left and right sides of \\(\\equiv\\), and that the derivative symbol \\(\\partial\\) has a subscript that identifies the with-respect-to input. All these are the same statement mathematically:\n\\[\\partial_x\\, x = 1\\ \\ \\ \\ \\partial_t\\, t = 1\\ \\ \\ \\ \\partial_y\\, y = 1\\ \\ \\ \\ \\partial_\\text{Josephina} \\text{Josephina} = 1\\] Admittedly, the last one is hard to read.\nWhen we look at derivatives of functions with multiple inputs we will need to be thoughtful about our choice of the with-respect-to input. But we want you to get used to seeing different input names used for differentiation.\n\nNow consider the slope functions of the logarithm and exponential functions.\n\n\n\n\n\nFigure 19.5: The slope functions of the logarithm and exponential.\n\n\n\n\nThese numerical experiments with the logarithm and exponential functions are more evidence that, as \\(h\\) gets small, the slope function doesn’t depend strongly on \\(h\\). And, we find that:\n\nFor small \\(h\\), the slope function of the logarithm is a power-law function: \\(\\partial_t \\ln(t) = \\frac{1}{t}\\).\nFor small \\(h\\), the slope function of the exponential is the exponential itself: \\(\\partial_t e^x = e^x\\).\n\nYou can confirm these by evaluating the slope function of the exponential at \\(t=0\\) and \\(t=1\\), and the slope function of the logarithm at \\(t= 2, 1, 1/2, 1/4, 1/8.\\)\n“Small” and “zero,” although related, are different. In constructing a derivative, we use smaller and smaller \\(h\\), but never zero. Let’s see what happens if instead of evanescent h, we use zero h. For example, we can use the slope function Dsin() and Dsigma() that we created earlier. Setting \\(h\\) to zero does not give a result that is the instantaneous rate of change of anything:\n\nDsin(t=1, h=0)\n## [1] NaN\nDsigma(t=0, h=0)\n## [1] NaN\n\nIn NaN, you can hear the echo of your fourth-grade teacher reminding you that it is illegal to divide by zero.\nThink of evanescent \\(h\\) as the vapor in the definition of “evanescent”: “tending to vanish like vapor.” This vapor is like the solvent in paint. You don’t want the solvent once the paint is on the wall; wet paint is a nuisance. But getting the paint from the container to the wall absolutely needs the solvent.\nWe used the solvent \\(h\\) earlier in the chapter in the numerical experiments that led us to the derivatives of the pattern-book functions, for instance \\(\\partial_x e^x = e^x\\) or \\(\\partial_x \\sin(x) = \\cos(x)\\). Eventually, we’ll construct an \\(h\\)-free theory of differentiation, reducing the process to a set of algebraic rules in which \\(h\\) never appears. With this as our goal, let’s continue using \\(h\\) for a while to find some additional useful facts about derivatives.\n\n\n\n\n\nSource↩︎"
  },
  {
    "objectID": "Differentiation/19-computing.html",
    "href": "Differentiation/19-computing.html",
    "title": "20  Computing derivatives",
    "section": "",
    "text": "To differentiate a function \\(g(x)\\) means simply to produce the corresponding function \\(\\partial_t g(x)\\). This is often called “finding the derivative,” language that resonates with the high-school algebra task of “finding \\(x\\).” Rather than conjuring an image of searching high and low for a missing function, it’s more accurate to say, “compute the derivative.”\nIn this chapter we’ll introduce two ways of computing a derivative. For simplicity we will write \\(x\\) for the with-respect-to input, although in practice you might be using \\(t\\) or \\(z\\) or something else.\nIn the days when functions were always presented using formulas, symbolic differentiation was usually the only method taught. Nowadays, when functions are just as likely to be described using data and an algorithm, finite-differencing provides the practical approach."
  },
  {
    "objectID": "Differentiation/19-computing.html#a-function-from-a-function",
    "href": "Differentiation/19-computing.html#a-function-from-a-function",
    "title": "20  Computing derivatives",
    "section": "20.1 A function from a function",
    "text": "20.1 A function from a function\nRecall that the goal of differentiation is to make a function out of an already known function. We’ll call the already known function \\(g(x)\\). In ?sec-change-relationships we’ve outlined the properties that the new function should have and gave a nice naming convention, \\(\\partial_x g(x)\\) that shows where the new function comes from. In this section we’ll put that aside and focus on the question of what it means to “make a function.”\nWhen mathematics is done with paper and pencil, “making a function” is a matter of writing a formula, such as \\(x^2 \\sin(x) + 3\\) and sometimes giving a name to the formula, e.g. \\(h(x) \\equiv x^2 \\sin(x) + 3\\). We are essentially writing something down that will make sense when viewed by another person trained in the conventions of mathematical notation.\nFor a computer, on the other hand, a function is a definite kind of thing. We “make a function” by creating that kind of thing and, usually, giving it a name. We evaluate a function—that is, apply the function to inputs to produce an output—by using specific punctuation, which in R involves the use of parentheses, for instance name(input).\nThe computer language itself provides specific means to define a new function. In R/mosaic, you first construct a tilde expression naming the function inputs (right side of the tilde) and specifying the algorithm of that function (left side of the tilde), as with this formula:\n\nf_description <- x^2 * sin(x) + 3 ~ x   # a tilde expression\n\nOn its own, f_description cannot be used like a function because it was constructed as something else: a tilde expression. Trying to use f_description in the way one uses a function produces an error.\nf_description(2)\n Error in f_description(2): could not find function \"f_description\"\nIn between the tilde expression and the final result—a function—is software that translates from tilde-expressions into functions:\n\nf <- makeFun(f_description)\n\nThe new creation, f() can now be used like any other function, e.g.\n\nf(2)\n## [1] 6.63719\n\nDown deep inside, makeFun() uses a more basic function-creation syntax which looks like this\n\nfunction(x) {x^2 * sin(x) + 3}\n## function(x) {x^2 * sin(x) + 3}\n\nYou can see all the same information that was in the tilde description, just arranged differently.\nAlmost every computer language provides something like function. The internal workings of function are elaborate and detailed … only advanced programmers need to be aware of them. This, in much the same way as it’s unnecessary to understand the workings of a transistor in order to use a computer, or comprehend the biochemistry of a COVID vaccine in order to benefit from it.\nIn the same spirit as makeFun(), which translates a tilde-expression into the corresponding function, in R/mosaic you have D() which takes a tilde expression and translates it into the derivative of the function described.1 For example:\n\nD(f_description)\n## function (x) \n## x * (2 * sin(x) + x * cos(x))"
  },
  {
    "objectID": "Differentiation/19-computing.html#finite-differencing",
    "href": "Differentiation/19-computing.html#finite-differencing",
    "title": "20  Computing derivatives",
    "section": "20.2 Finite differencing",
    "text": "20.2 Finite differencing\nYou can use the definition of the slope function \\[{\\cal D}_x f(x) = \\frac{f(x+0.1) - f(x)}{0.1}\\] to create an approximation to the derivative of any function. Like this:\n\ng <- makeFun(sin(2*x)*(pnorm(x/3)-0.5) ~ x)\ndg <- makeFun((g(x+0.1) - g(x))/0.1 ~ x)\n\nWhenever you calculate a derivative function, you should check against mistakes or other sources of error. For instance, whenever the derivative is zero, the original function should have an instantaneous slope of zero. Figure 20.1 shows a suitable plot for supporting this sort of check.\n\nzeros_of_dg <- Zeros(dg(x) ~ x, domain(x=-5:5))\nslice_plot(g(x) ~ x, domain(x=-5:5), npts=500) %>%\n  slice_plot(dg(x) ~ x, color=\"magenta\", npts=500) %>%\n  gf_hline(yintercept = ~ 0, color = \"orange\", size=2, alpha=0.2) %>%\n  gf_vline(xintercept = ~ x, data=zeros_of_dg, color=\"blue\")\n\n\n\n\nFigure 20.1: The x-position of zero crossings of the derivative function (magenta) are marked with blue lines. The zero crossings correspond to local maxima or minima in the original function (black). This is because the original function has slope zero at maxima and minima.\n\n\n\n\nLook very closely at Figure 20.1, particularly at the places where the blue vertical markers cross the function \\(g(x)\\) (black). They should cross exactly at the flat zone, but they are a little shifted to the left. (You might have to zoom in on the plot to see the offset between the vertical blue marker and the local maximum of the function.) That’s the sense in which the finite-difference approach gives an approximation. The small left-shift stems from the use of 0.1 in the definition of the zero function. Use a smaller value, say 0.01 or 0.001, and you won’t be able to see the shift at all.\n\nIn modeling work, there’s nothing wrong with an approximation so long as it is good enough for your purposes. We picked the value 0.1 for our definition of the slope function because it works very well with the pattern-book functions. Here, “very well” means you can’t easily see in the graph any deviation compared to the exact derivative.\nWhen a calculation can be done exactly (without outrageous effort) it certainly makes sense to use the exact method. However:\n\nIt’s useful to have an easy, approximate method always at hand. This lets you check the results of other methods for the possibility of some blunder or mis-conception. The slope function approach to differentiation is certainly easy, and if you think the approximation isn’t good enough, then instead of 0.1 use something smaller. (Section 19 discusses how small is too small.)\nThe computer makes it practical to employ the slope function as a useful approximation to the derivative. There are many other mathematical methods that the computer has made feasible, for instance the methods of machine learning. These methods create functions that sometimes cannot be handled by the traditional (“exact”) methods of differentiation."
  },
  {
    "objectID": "Differentiation/19-computing.html#the-slope-function-operator",
    "href": "Differentiation/19-computing.html#the-slope-function-operator",
    "title": "20  Computing derivatives",
    "section": "20.3 The slope-function operator",
    "text": "20.3 The slope-function operator\nTake a look at the statement we used to construct the slope function of g():\ndg <- makeFun((g(x+0.1) - g(x))/0.1 ~ x)\nThere is almost nothing about this statement that has anything to do with the specifics of how we defined g(); we could have used any \\(g()\\). The “almost” in the previous sentence is about the choice of 0.1, which isn’t guaranteed to be small enough.\n\nIn today’s world, considerable mathematical content is conveyed to users not directly with formulas but with software that implements the formulas. And in software, it’s a good idea to have a name for each operation so that the readers and authors of software have a completely explicit indication that a particular operation is being used.\nWhen you have many slope functions to compute in some application, it can be error prone to write many statements that are versions of\ndg <- makeFun((g(x+0.1) - g(x))/0.1 ~ x)\nIt’s too easy to make a mistake in copying this over, producing a wrong computation that can be hard to detect in the code. For instance, each of these statements looks a lot like the above, but all of them are different and none is constructing a slope function:\ndf <- makeFun((f(x-0.1) - f(x))/0.1)\ndh <- makeFun((h(x+0.1) - f(x))/0.1)\ndu <- makeFun((u(x+0.1) - u(x))/1.0)\ndg <- makeFun(g(x+0.1) - g(x)/0.1)\nMuch easier to read and more reliable would be something like this:\ndf <- slopeFun(f(x) ~ x)\ndh <- slopeFun(h(x) ~ x)\ndu <- slopeFun(u(x) ~ x)\nCreating such an R operator is a programming task and in that sense beyond the scope of this course. Still, it’s a good idea to get in the habit of reading programming code. So here goes …\nCreating a slopeFun() operator:\n\nInstead of makeFun() which is really just maknig for mathematical functions, R programmers use a construction named function(). The name of the arguments goes inside the parentheses. The function algorithm goes between curly braces: { }\nWe’re going to use a tilde expression as the input to slopeFun(). This is how the other R/mosaic operators work. That will be easier for the user and will also give us access to those other operators if we need them in writing slopeFun().\nThe object returned by the slopeFun() operator will be, of course, a function. We’ve been using makeFun() to make our mathematical functions, so expect to see that in the code for slopeFun().\nThere’s the matter of whether 0.1 is small enough. So let’s use an h argument in place of 0.1 that we can change when needed.\n\nPutting this together, here is a slopeFun() operator that takes a tilde expression (as do makeFun() and slice_plot()) and produces a new mathematical function that is the slope function for the mathematical function described in the tilde expression. (There are a couple of R programming elements in slopeFun() that you aren’t expected to understand completely. But do try reading the code to see what sense you can make of it.)\n\n# two arguments, a tilde expression and a choice for h\n# with a default value\nslopeFun <- function(tilde, h=0.1) { \n   # Turn the tilde expression into a function\n   g <- makeFun(tilde) \n   # just like before, with h instead of 0.1\n   makeFun((g(x + h) - g(x))/h ~ x, h=h) \n}\n\nAnother important advantage of centralizing computations in a single operator is that the operator can be made more sophisticated without being harder to use. For instance, R/mosaic provides the D() operator for computing derivatives. This knows the rules for symbolic differentiation that will be introduced in Section 23 and switches to a finite-difference method (like slopeFun(), but more sophisticated) when symbolic differentiation isn’t applicable.\n\nIn practice, instead of home-brewed functions like slopeFun(), you can use the R/mosaic D() instead, which takes a more careful approach to the computer numerics and uses symbolic differentiation whenever possible to give results without numerical error."
  },
  {
    "objectID": "Differentiation/19-computing.html#sec-symbolic-differentiation",
    "href": "Differentiation/19-computing.html#sec-symbolic-differentiation",
    "title": "20  Computing derivatives",
    "section": "20.4 Symbolic differentiation",
    "text": "20.4 Symbolic differentiation\nSymbolic differentiation is the process of taking a formula and translating it to a new formula according to certain patterns or rules. Each rule is ultimately derived from the definition of the slope function and the differencing operator.\nAs you recall, the differencing operator \\(\\diff{x}\\) turns a function into its slope function \\[\\diff{x} f(x) \\equiv \\frac{f(x+h) - f(x)}{h}\\]\n\n20.4.1 The line rule\nLet’s look at one where we already know the result: The straight line function \\(\\line(x) \\equiv a x + b\\) has a slope function that is constant: \\(\\diff{x}\\line(x) = a\\)\n\\[\\diff{x}\\line(x) = \\frac{\\line(x+h) - \\line(x)}{h} = \\frac{\\left[\\strut a (x+h) + b\\right] - \\left[\\strut a x + b\\right]}{h} = \\frac{ah}{h} = a\\] The derivative is the slope function with \\(h\\) made as small as possible. It’s tempting to think of this as \\(h = 0\\), but that would imply dividing by zero in the differencing operator.\nBeing wary about the possibility of dividing by zero, mathematicians adopt a convention which indicates clearly that \\(h\\) is to be small, but not zero. This convention is marked with the notation \\(\\lim_{h \\rightarrow 0}\\), which means “as close as you can get to zero, but not zero exactly”.\n\\[\\partial_x \\line(x) \\equiv \\lim_{h\\rightarrow 0} \\frac{\\line(x+h) - \\line(x)}{h} =\\\\\n\\ \\\\\n= \\lim_{h\\rightarrow 0} \\frac{a h}{h} = a\\]\nThis derivation is unarguably correct for any non-zero \\(h\\).\nThis short derivation gives us a basic differentiation rule which we can divide into 3 special cases.\n\nLine rule: \\(\\partial_x ax + b = a\\)\n\n\\(\\partial_x ax = a\\). The function \\(ax\\) is \\(\\line(x)\\) with \\(b=0\\).\n\\(\\partial_x b = 0\\). The function \\(b\\) is \\(\\line(x)\\) with \\(a=0\\) and thus is the constant function.\n\\(\\partial_x x = 1\\). The function \\(x\\) is \\(\\line(x)\\) with \\(a=1\\) and \\(b=0\\).\n\n\nRemember that \\(\\partial_x f(x)\\) is always a function no matter what kind of function \\(f(x)\\) is. The functions associated with the line rule are all constant functions, meaning the output doesn’t depend on the input.\n\n\n20.4.2 The square rule\nOnly for the \\(\\line()\\) function and its three special cases is the derivative a constant function. And \\(\\line()\\) is the only function for which the \\(h\\) in the differencing operator disappears on its own. For instance, consider the square function: \\(g(x) \\equiv x^2\\).\n\\[\\begin{eqnarray}\n\\partial_x [x^2] & = & \\lim_{h\\rightarrow 0}\\frac{(x+h)^2 - x^2}{h}\\\\\n& = & \\lim_{h\\rightarrow 0}\\frac{(x^2 + 2 x h + h^2) - x^2}{h}\\\\  \n& = & \\lim_{h\\rightarrow 0}\\frac{2 x h + h^2}{h} \\\\\n& = &\\lim_{h\\rightarrow 0} [2x + h]\\\\\n\\end{eqnarray}\\]\nIt’s accepted that the limit of a sum is the sum of the limits, so … \\[\\lim_{h\\rightarrow 0} \\left[\\strut 2 x + h\\right]= \\lim_{h\\rightarrow 0} 2x + \\lim_{h\\rightarrow 0}h\\] The limit of something not involving \\(h\\) is just that thing, giving \\[\\lim_{h\\rightarrow 0}2x = 2x\\ .\\] Finally, when an expression including \\(h\\) doesn’t involve division by \\(h\\), we can remove the \\(\\lim_{h\\rightarrow 0}\\) and just use \\(h=0\\) instead, so \\[\\lim_{h\\rightarrow 0} h = 0\\ .\\]\nPutting these together gives\n\\[\\partial_x [x^2] = 2x + \\lim_{h\\rightarrow 0}h = 2x\\]\nWe’ll write this as another differentiation rule.\n\nQuadratic rule: \\(\\partial_x [x^2] = 2x\\)\n\n\n\n20.4.3 The exponential rule\nLet’s take on the exponential function \\(h(x) \\equiv e^x\\):\n\\[\\partial_x e^x = \\lim_{h\\rightarrow 0}\\frac{e^{x+h} - e^x}{h} = e^x \\lim_{h\\rightarrow 0}\\left[\\frac{e^h - 1}{h}\\right]\\] At a glance, it can be hard to know what to make of \\(\\lim_{h\\rightarrow 0} (e^h-1)/h\\). Setting \\(h=0\\) in the denominator is perfectly legitimate and gives \\(e^0 - 1 = 0\\). But that still leaves the \\(h\\) in the numerator. Still, for any non-zero \\(h\\), the division is legitimate, so let’s see what happens as \\(h \\rightarrow 0\\):\n\nf <- makeFun((exp(h) - 1)/h ~ h)\nf(0.1)\n## [1] 1.051709\nf(0.01)\n## [1] 1.005017\nf(0.001)\n## [1] 1.0005\nf(0.0001)\n## [1] 1.00005\nf(0.0000001)\n## [1] 1\nf(0.0000000001)\n## [1] 1\n\nSetting \\(h\\) exactly to zero, however, won’t work: it produces NaN.\n\nf(0)\n## [1] NaN\n\nSince \\(\\lim_{h\\rightarrow 0} (e^h-1)/h = 1\\), we have\n\nExponentiation rule: \\(\\partial_x e^x = e^x\\)\n\n\n\n20.4.4 The reciprocal rule\nStill another example: the reciprocal function, written equivalently as \\(1/x\\) or \\(x^{-1}\\)\n\\[\\begin{eqnarray}\n\\partial x^{-1} & = &\\lim_{h\\rightarrow 0}\\frac{\\frac{1}{x+h} - \\frac{1}{x}}{h} \\\\\n& = & \\lim_{h\\rightarrow 0} \\frac{\\frac{x - (x+h)}{x(x+h)}}{h}\\\\\n& = & \\lim_{h\\rightarrow 0}\\frac{x - x+h}{x(x+h)h} = -\\lim_{h\\rightarrow 0}\\frac{h}{x(x+h)h} \\\\\n& = & -\\lim_{h\\rightarrow 0}\\frac{1}{x^2 + hx}\n\\end{eqnarray}\\] So long as \\(x \\neq 0\\), there is no divide-by-zero problem even when \\(h=0\\), but let’s see what the computer thinks:\n\ng <- makeFun(-1/(x^2 + h*x) ~ h, x=10)\ng(0.1)\n## [1] -0.00990099\ng(0.01)\n## [1] -0.00999001\ng(0.001)\n## [1] -0.009999\ng(0.0001)\n## [1] -0.0099999\ng(0.0000001)\n## [1] -0.01\ng(0.0000000001)\n## [1] -0.01\ng(0)\n## [1] -0.01\n\nSetting \\(h\\) to zero in the last expression gives another differentiation rule:\n\nReciprocal rule: \\(\\partial_x \\frac{1}{x} = -\\frac{1}{x^2}\\)\n\n\n\n20.4.5 Power-law rule\nWe don’t yet have the tools needed to prove a formula for the derivative of power-law functions, but we already have some instances where we know the derivative:\n\n\\(\\partial_x x^2 = 2 x\\)\n\\(\\partial_x x^1 = 1\\)\n\\(\\partial_x x^0 = 0\\)\n\nA rule that fits all these examples is \\[\\partial_x x^p = p\\, x^{p-1}\\ .\\] For instance, when \\(p=2\\) the rule gives \\[\\partial_x x^2 = 2\\, x^1 = 2 x\\] since \\(p-1\\) will be 1$.\nIt’s not too hard to do the algebra to find the derivative of \\(x^3\\). According to the proposed rule, the derivative should be \\[\\partial_x x^3 = 3 x^2\\ .\\] Let’s check this via the definition of the derivative: \\[\\partial_x x^3 = \\lim_{h \\rightarrow 0}\\frac{(x+h)^3 - x^3}{h}\\] Direct multiplication \\((x+h)(x+h)(x+h)\\) gives \\[(x+h)^3 = x^3 + 3\\, x^2 h + 3\\, x h^2 + h^3\\] Consequently, the limit definition amounts to: \\[\\partial_x x^3 = \\lim_{h\\rightarrow 0}\\frac{3\\, x^2 h + 3\\, x h^2 + h^3}{h} = \\lim_{h\\rightarrow 0} \\left[\\strut 3\\, x^2 + 3\\,x h + h^2\\right]\\] But there is no divide-by-\\(h\\) in sight, so we can resolve \\(\\lim_{h\\rightarrow 0}\\) to \\(h=0\\), giving \\[\\partial_x x^3 = 3 x^2\\ ,\\] consistent with the proposed rule.\nThose familiar with the use of Pascal’s Triangle for finding the terms of binomial expansions such as \\((x + h)^p\\) will gain insight into the \\(p x^{p-1}\\) rule.\n\n\n20.4.6 List of pattern-book rules\nWe’ll save for later the derivation of the derivatives of the other pattern-book functions, but note that the gaussian function is defined to be the derivative of the sigmoidal function.\n\n\n\nName\n\\(f(x)\\)\n\\(\\partial_x f(x)\\)\n\n\n\n\nexponential\n\\(e^x\\)\n\\(e^x\\)\n\n\nlogarithm (natural)\n\\(\\ln(x)\\)\n\\(1/x\\)\n\n\nsinusoid\n\\(\\sin(x)\\)\n\\(\\cos(x)\\)\n\n\nsquare\n\\(x^2\\)\n\\(2x\\)\n\n\nproportional\n\\(x\\)\n\\(1\\)\n\n\nconstant\n1\n0\n\n\nreciprocal\n\\(1/x\\) or \\(x^{-1}\\)\n\\(-1/x^2\\)\n\n\ngaussian (hump)\n\\(\\dnorm(x)\\)\n\\(-x\\, \\dnorm(x)\\)\n\n\nsigmoid\n\\(\\pnorm(x)\\)\n\\(\\dnorm(x)\\)\n\n\n\n\n\n\n\n\nFigure 20.2: A diagram showing how differentiation connects the pattern-book functions to one another."
  },
  {
    "objectID": "Differentiation/19-computing.html#exercises",
    "href": "Differentiation/19-computing.html#exercises",
    "title": "20  Computing derivatives",
    "section": "20.5 Exercises",
    "text": "20.5 Exercises"
  },
  {
    "objectID": "Differentiation/19-computing.html#drill",
    "href": "Differentiation/19-computing.html#drill",
    "title": "20  Computing derivatives",
    "section": "20.6 Drill",
    "text": "20.6 Drill\n\n\nPart i Which pattern-book function is the derivative of the sigmoid function pnorm()? That is, \\[{\\large \\text{pnorm}(x)}  \\underset{\\scriptsize \\text{anti-diff}}{{\\stackrel{\\text{diff}}{\\ \\ \\ \\ {\\Huge\\rightleftharpoons}\\ \\ \\ }}}  {\\LARGE ?}\\]\nGaussian dnorm(x)Exponential \\(e^x\\)Sinusoid \\(\\sin(x)\\)Constant \\(1\\)Reciprocal \\(1/x\\)\n\n\n\n\nPart ii Which pattern-book function is the anti-derivative of the reciprocal \\(1/x\\)? That is, \\[{\\LARGE ?}  \\underset{\\scriptsize \\text{anti-diff}}{{\\stackrel{\\text{diff}}{\\ \\ \\ \\ {\\Huge\\rightleftharpoons}\\ \\ \\ }}}  {\\large \\frac{1}{x}}\\]  NOTE: Differentiation produces a “child” function from a “parent” function. The child is the derivative of the parent. Putting the relationship the other way, the parent is the anti-derivative of the child. “Derivative” and “anti-derivative” are two ways of looking at the same relationship between a pair of functions. So, if \\(f(x)\\) is the derivative of \\(F(x)\\), then \\(F(x)\\) is the anti-derivative of \\(f(x)\\).\n\nGaussian \\(\\text{dnorm(x)}\\)\nLogarithm \\(\\ln(x)\\)\nSinusoid \\(\\sin(x)\\)\nConstant \\(1\\)\nReciprocal \\(1/x\\)\n\n\n\n\n\nPart iii Which pattern-book function is the anti-derivative of the gaussian \\(\\text{dnorm()}\\)? That is, \\[{\\LARGE ?}  \\underset{\\scriptsize \\text{anti-diff}}{{\\stackrel{\\text{diff}}{\\ \\ \\ \\ {\\Huge\\rightleftharpoons}\\ \\ \\ }}}  {\\large \\text{dnorm}(x)}\\]  NOTE: Differentiation produces a “child” function from a “parent” function. The child is the derivative of the parent. Putting the relationship the other way, the parent is the anti-derivative of the child. “Derivative” and “anti-derivative” are two ways of looking at the same relationship between a pair of functions. So, if \\(f(x)\\) is the derivative of \\(F(x)\\), then \\(F(x)\\) is the anti-derivative of \\(f(x)\\). In other words: \\[{\\large F(x)}  \\underset{\\scriptsize \\text{anti-diff}}{{\\stackrel{\\text{diff}}{\\ \\ \\ \\ {\\Huge\\rightleftharpoons}\\ \\ \\ }}}  {\\Large f(x)}\\]\n\nGaussian \\(\\text{dnorm(x)}\\)\nLogarithm \\(\\ln(x)\\)\nSigmoid \\(\\text{pnorm(x)}\\)\nConstant \\(1\\)\nReciprocal \\(1/x\\)\n\n\n\n\n\nPart iv What is the derivative of the power-law function \\(x^p\\)?i That is, \\[{\\Large x^p}  \\underset{\\scriptsize \\text{anti-diff}}{{\\stackrel{\\text{diff}}{\\ \\ \\ \\ {\\Huge\\rightleftharpoons}\\ \\ \\ }}}  {\\LARGE ?}\\]\n\n\\((p-1)\\, x^{p-1}\\)\n\\((p-1)\\, x^p\\)\n\\(p\\, x^{p-1}\\)\n\\(\\frac{1}{p} x^{p+1}\\)\n\\(p\\, x^p\\)\n\n\n\n\n\nPart v There are two pattern-book functions whose second derivative is proportional to the function itself. Which are they?\n\nSinusoid and gaussian\nExponential and sigmoid\nExponential and sinusoid\nExponential and logarithm\n\n\n\n\n\nPart vi What is the derivative of \\(t^5\\) with respect to \\(t\\)? That is, \\[{\\Large t^5}  \\underset{\\scriptsize \\text{anti-diff}}{{\\stackrel{\\text{diff}}{\\ \\ \\ \\ {\\Huge\\rightleftharpoons}\\ \\ \\ }}}  {\\Large ?}\\]\n\\(\\frac{1}{4} t^5\\)\\(4 t^5\\)\\(5 t^4\\)\\(\\frac{1}{5} t^4\\)\n\n\n\n\nPart vii What is \\(\\partial_x x^2\\)?\n\\(2/x\\)\\(2\\)\\(2 x\\)\\(2 x^2\\)\n\n\n\n\nPart viii What is \\(\\partial_t \\sin(x)\\)\n\\(-\\cos(x)\\)0\\(\\cos(x)\\)\\(-\\sin(x)\\)\n\n\n\n\nPart ix Suppose you know only this one fact about \\(f(x)\\), that \\(\\partial_{xx}\\, f(7.3) = 1.6\\). Which of these statements must be true?\n\n\\(f(x)\\) is concave up at \\(x=7.3\\), but eventually it will become concave down.\n\\(f(x)\\) is concave up and decreasing at \\(x=7.3\\)\n\\(f(x)\\) is increasing at \\(x=7.3\\).\n\\(f(x)\\) is concave up at \\(x=7.3\\)\n\n\n\n\n\nPart x If \\(f(x)\\) is discontinuous at \\(x=5\\), can it possibly be smooth at \\(x=6\\)?\nNoYes\n\n\n\n\nPart xi If \\(g(x)\\) is discontinuous at \\(x=1\\), what will be the value of \\(\\partial_x g(x)\\) at \\(x=1\\)?\n\nDepends on how big the gap is at the discontinuity.\n0\n\\(1/x\\)\nThe derivative isn’t defined at a discontinuity.\n\n\n\n\n\nPart xii Which of the following is the correct construction for \\(\\partial_t g(t)\\)?\n\n\\(\\lim_{x \\rightarrow 0} \\frac{g(t + h) - g(t)}{h}\\)\n\\(\\lim_{h \\rightarrow 0} \\frac{g(t + h) - g(t)}{t}\\)\n\\(\\lim_{h \\rightarrow 0} \\frac{g(t + h) - g(t)}{h}\\)\n\\(\\lim_{h \\rightarrow 0} \\frac{g(t) - g(t+h)}{h}\\)\n\n\n\n\n\nPart xiii Which of these is a reasonable definition of a derivative?\n\nA derivative is the slope of a function.\nA derivative is a function whose value tells, for any input, the instantaneous rate of change of the function from which it was derived.\nA derivative is a function whose value tells, for any input, the instantaneous change of the function from which it was derived.\n\n\n\n\n\nPart xiv Which one of these is not the derivative of a pattern-book function?\nSigmoidZeroReciprocalOne\n\n\n\n\nPart xv Which of the following shapes of functions is not allowed? You are strongly advised to try to draw each shape.\n\nIncreasing and concave up.\nDecreasing and concave up.\nIncreasing and concave down.\nDecreasing and concave down.\nNone of them are allowed.\nAll of them are allowed."
  },
  {
    "objectID": "Differentiation/20-concavity.html",
    "href": "Differentiation/20-concavity.html",
    "title": "21  Concavity and curvature",
    "section": "",
    "text": "Looking at the graph of a function, our eyes immediately register the slope at any point we focus on. A glance shows whether the slope at that point is positive or negative. Comparing the slopes at two locales is also an automatic visual task: most people have little difficulty saying which slope is steeper.\nOne consequence of this visual ability is that it’s easy to recognize whether a line that touches the graph at a point is tangent to the graph.\nThere are other aspects of functions, introduced in Section @ref(word-descriptions), that are also readily discerned from a glance at the function graph.\nThe following exercises are simply meant to test your visual acuity in spotting concavity, tangency, and smoothness. Then we’ll move on to the calculations involved."
  },
  {
    "objectID": "Differentiation/20-concavity.html#quantifing-concavity-and-curvature",
    "href": "Differentiation/20-concavity.html#quantifing-concavity-and-curvature",
    "title": "21  Concavity and curvature",
    "section": "21.1 Quantifing concavity and curvature",
    "text": "21.1 Quantifing concavity and curvature\nIt often happens in building models that the modeler (you!) knows something about the concavity and/or curvature of a function. For example, concavity is important in classical economics; the curve for supply is concave down while the curve for demand is concave up. For a train, car, or plane, there are forces that depend on the curvature of the track, road, or trajectory. If you are designing a road, you’ll need to calculate the curvature in order to know if the road is safe at the indicated speed.\nIt turns out that quantifying these properties of functions or shapes is naturally done by calculating derivatives.\n\nImagine designing a highway. Due to the terrain, part of the road is oriented east-west and another part north-south. For vehicles to use the road, those two parts need to be connected together! (In math-speak, we might say that the road has to be continuous, but this is just common sense.)\nFrom your experience with highways, you know the connection will be a smooth curve. If the curve is part of a circle, then the design needs to specify the radius of curvature of the circle. Too tight a radius and the traffic won’t be able to handle the centrifugal force and will drift or skid off the road. A big radius is needed for safety, but making the radius bigger than required adds additional cost to road construction.\nIt’s not as simple as finding the radius of the curve. The radius needs to change at the entry and exit of the curve. Why? Here’s an explanation from the American Association of State Highway and Transportation Officials *Policy on Geometric Design of Highways and Streets (1994):\nAny motor vehicle follows a transition path as it enters or leaves a circular horizontal curve. The steering change and the consequent gain or loss of centrifugal force cannot be effected instantly. For most curves the average driver can effect a suitable transition path within the limits of normal lane width. However, with combinations of high speed and sharp curvature the resultant longer transition can result in crowding and sometimes actual occupation of adjoining lanes. In such instances transition curves would be appropriate because they make it easier for a driver to confine the vehicle to his or her own lane. The employment of transition curves between tangents and sharp circular curves and between circular curves of substantially different radii warrants consideration.\nLater in this chapter, you’ll see the calculus concepts that relate to designing a road with a gently changing curvature. (Hint, but don’t get scared: It’s the third derivative, not the first or the second.)\n\nLet’s frame the calculations in terms of a function \\(f(x)\\). Depending on the setting, \\(x\\) might be the price of a product and \\(f(x)\\) the demand for that product. Or the graph of \\(f(x)\\) might be the path of a road drawn in \\((x,y)\\) coordinates or the reach of a robot arm as a function of time. Remember that \\(f()\\) is just a pronoun that I’m using instead of a proper descriptive name. I use such pronouns (also, \\(g()\\), \\(h()\\), the “she” and “he” of mathematical language) when writing about the general properties of functions."
  },
  {
    "objectID": "Differentiation/20-concavity.html#sec-concavity-deriv",
    "href": "Differentiation/20-concavity.html#sec-concavity-deriv",
    "title": "21  Concavity and curvature",
    "section": "21.2 Concavity",
    "text": "21.2 Concavity\nRecall that to find the slope of a function \\(f(x)\\) at any input \\(x\\), you compute the derivative of that function, which we’ve been writing \\(\\partial_x\\,f(x)\\). Plug in some value for the input \\(x\\) and the output of \\(\\partial_x\\, f(x)\\) will be the slope of \\(f(x)\\) at that input. (Section 20 introduced some techniques for computing the derivative of any given function.)\nNow we want to show how differentiation can be used to quantify the concavity of a function. It will help if we augment our nomenclature a bit. When we speak of the “derivative” of a function, we mean something that might be more completely expressed as the first derivative of the function. Just that name naturally suggests that there will be a second derivative, a third derivative, and so on.\nFigure 21.1 shows a simple function that is concave down.\n\n\n\n\n\n\nFigure 21.1: A function that is concave down.\n\n\n\nNotice that the concavity is not about the slope. The curve in Figure 21.1 is concave down everywhere in the domain \\(0 \\leq x \\leq 4\\), but the slope is positive for \\(0 \\leq x \\leq 1\\) and negative for larger \\(x\\). Slope and concavity are two different aspects of a function.\nAs introduced in ?sec-fun-describing, the concavity of a function depends not on the slope, but on the change in the slope. Figure 21.2 adds some annotations on top of the graph in Figure 21.1. In the subdomain marked A, the function slope is positive while in the subdomain B, the function slope is negative. It is this transition from the slope in A to the slope in B that corresponds to the concavity of the function between A and B.\n\n\n\n\n\n\nFigure 21.2: Concavity is about how the slope changes from one place in the domain to another.\n\n\n\nSimilarly, the concavity of the function between B and C, reflects the transition in the slope from B to C. Even though the slope is negative in both B and C, the change in slope tells us about the concavity.\nLet’s look at this using symbolic notation. Keep in mind that the function graphed is \\(f(x)\\) while the slope is the function \\(\\partial_x\\,f(x)\\). We’ve seen that the concavity is indicated by the change in slope of \\(f()\\), that is, the change in \\(\\partial_x\\, f(x)\\). We’ll go back to our standard way of describing the rate of change near an input \\(x\\):\n\\[\\text{concavity.of.f}(x) \\equiv\\ \\text{rate of change in}\\ \\partial_x\\, f(x) = \\partial_x [\\partial_x f(x)] \\\\\n\\\\\n= \\lim_{h\\rightarrow 0}\\frac{\\partial_x f(x+h) - \\partial_x f(x)}{h}\\] We’re defining the concavity of a function \\(f()\\) at any input \\(x\\) to be \\(\\partial_x [\\partial_x f(x)]\\). We create the concavity_of_f(x) function by applying differentiation twice to the function \\(f()\\).\nSuch a double differentiation of a function \\(f(x)\\) is called the second derivative of \\(f(x)\\). The second derivative is so important in applications that it has it’s own compact notation: \\[\\text{second derivative of}\\ f()\\ \\text{is written}\\ \\partial_{xx} f(x)\\] Look carefully to see the difference between the first derivative \\(\\partial_x f(x)\\) and the second derivative \\(\\partial_{xx} f(x)\\): it’s all in the double subscript \\(_{xx}\\).\nComputing the second derivative is merely a matter of computing the first derivative \\(\\partial_x f(x)\\) and then computing the (first) derivative of \\(\\partial_x f(x)\\). In R this process looks like:\n\ndx_f  <- D(   f(x) ~ x)   # First deriv. of f()\ndxx_f <- D(dx_f(x) ~ x)   # Second deriv. of f()\n\n\nAs a shortcut for the two-step process above, for the second derivative you can use a notation which doubles up on the x on the right-hand side of the tilde: dxx_f <- D(f(x) ~ x & x)"
  },
  {
    "objectID": "Differentiation/20-concavity.html#sec-curvature-definition",
    "href": "Differentiation/20-concavity.html#sec-curvature-definition",
    "title": "21  Concavity and curvature",
    "section": "21.3 Curvature",
    "text": "21.3 Curvature\nAs you see from Section 21.2, it’s easy to quantify the concavity of a function \\(f(x)\\): just evaluate the second derivative \\(\\partial_{xx} f(x)\\). But it turns out that people are very poor at estimating the quantitative value of concavity by eye.\nTo illustrate, consider the square function, \\(f(x) \\equiv x^2\\). (See Figure 21.3.)\n\n\n\n\n\n\nFigure 21.3: Does the concavity of the square function vary with \\(x\\)?\n\n\n\nClearly, the square function is concave up. Now a test: Looking at the graph of the square function, where is the concavity the largest? Don’t read on until you’ve pointed where you think the concavity is largest.\nWith your answer to the test question in mind, let’s calculate the concavity of the square function using derivatives.\n\\[f(x) \\equiv x^2\\ \\text{      so     }\\\n\\partial_x f(x) = 2 x\\ \\text{     and therefore     }\\ \\partial_{xx} f(x) = 2\\]\nThe second derivative of \\(f(x)\\) is positive, as you would expect for a function that is concave up. What you might not expect, however, is that the second derivative is constant.\nThe concavity-related property that the human eye reads from the graph of a function is not the concavity itself, but the curvature of the function. The curvature of \\(f(x)\\) at a point \\(x_0\\) is defined to be the radius of the circle that is tangent to the function at \\(x_0\\).\nFigure 21.4 illustrates the changing curvature of \\(f(x) \\equiv x^2\\) by inscribing tangent circles at several points on the function graph, marked with dots. You can see the tangency of the circle to the function graph; the function’s thin black line goes right down the middle of the broader lines used to draw the circles.\n\n\n\n\n\nFigure 21.4: At any point on the graph of a smooth function, a circle tangent to the graph can be drawn. The radius of this circle is \\(1/{\\cal K}\\).\n\n\n\n\nBlack dots have been put along the graph at the points where the graph of the function is tangent to the inscribed circle. The visual sign of tangency is that the graph of the function goes right down the center of the circle.\nThe inscribed circle at \\(x=0\\) is tightest. The circle at \\(x=1\\) has a somewhat larger radius. The radius of the circle at \\(x=-1.5\\) is the largest of all. Whereas the concavity is the same at all points on the graph, the visual impression that the function is most highly curved near \\(x=0\\) is better captured by the radius of the inscribed circle. The radius of the inscribed circle at any point is the reciprocal of a quantity \\({\\cal K}\\) called the curvature.\nThe curvature \\({\\cal K}\\) of a function \\(f(x)\\) depends on both the first and second derivative. The formula for curvature \\(K\\) is somewhat off-putting; you are not expected to memorize it. But you can see where \\(\\partial x f()\\) and \\(\\partial_{xx}f()\\) come into play.\n\\[{\\cal K}_f  \\equiv \\frac{\\left|\\partial_{xx} f(x)\\right|}{\\ \\ \\ \\ \\left|1 + \\left[\\strut\\partial_x f(x)\\right]^2\\right|^{3/2}}\\]\nMathematically, the curvature \\(\\cal K\\) corresponds to the reciprocal of the radius of the tangent circle. When the tangent circle is tight, \\(\\cal K\\) is large. When the tangent circle has a very large radius, that is, the function is very close to approximating a straight line, \\(\\cal K\\) is very small.\n\nReturning to the highway design example earlier in the chapter … The Policy on geometric design of highways and streets called for the curvature of a road to change gently, giving the driver time to adjust the steering and accomodate the centrifugal force of the car going around the curve.\nChanging curvature implies that \\(\\partial_x {\\cal K}\\) is non-zero. Since \\({\\cal K}\\) depends on the first and second derivatives of \\(f(x)\\), the Policy on gradual change means that the third derivative of \\(f(x)\\) is non-zero."
  },
  {
    "objectID": "Differentiation/20-concavity.html#exercises",
    "href": "Differentiation/20-concavity.html#exercises",
    "title": "21  Concavity and curvature",
    "section": "21.4 Exercises",
    "text": "21.4 Exercises"
  },
  {
    "objectID": "Differentiation/21-cont-and-smooth.html",
    "href": "Differentiation/21-cont-and-smooth.html",
    "title": "22  Continuity and smoothness",
    "section": "",
    "text": "You’ve seen how various properties of a function—whether it is monotonic, how it slopes, whether it is concave up or down (or not at all), curvature, etc.—can be related to the first and second derivatives of the function.\nIn this chapter, we’ll elaborate on continuity, one of the ideas introduced in ?sec-word-descriptions, and use the concept of continuity to characterize functions in a new way: their smoothness."
  },
  {
    "objectID": "Differentiation/21-cont-and-smooth.html#continuity",
    "href": "Differentiation/21-cont-and-smooth.html#continuity",
    "title": "22  Continuity and smoothness",
    "section": "22.1 Continuity",
    "text": "22.1 Continuity\nThe intuition behind continuity is simple: If you can draw the graph of a function without lifting the pencil from the paper, the function is continuous.\nContinuity can be an important attribute of a modeling function. Often, we are modeling phenomena where a small change in input is expected to produce a small change in output. For instance, if your income changes by one penny, you would expect your lifestyle not to change by much. If the temperature of an oven changes by 1 degree, you don’t expect the quality of the cake you are baking to change in any noticeable way.\n\n\n\n\n\n\nFigure 22.1: The Heaviside function is piecewise constant with a discontiuity at \\(x=0\\).\n\n\n\nAll of our basic modeling functions are continuous over their entire input domain.1 To illustrate discontinuity we’ll consider piecewise functions, as introduced in ?sec-fun-piecewise. The Heaviside function, graphed in Figure 22.1 is discontinuous.\nDrawing the graph of the Heaviside function \\(H(x)\\) involves lifting the pencil at \\(x=0\\).\nIn contrast, the piecewise ramp function (Figure 22.2) is continuous; you don’t need to lift the pencil from the paper in order to draw the ramp function.\n\n\n\n\n\n\nFigure 22.2: The ramp function is a continuous piecewise function.\n\n\n\nImagine that you were constructing a model of plant growth as a function of the amount of water (in cc) provided each day. The plant needs about 20 cc of water to thrive. Let’s say that you use the Heaviside function for the model, say \\(H(W-20)\\), where an output of 1 means the plant thrives and a output 0 means the plant does not. The model implies that if you provide 20.001 cc of water, the plant will thrive. But if you are stingy, and provide only 19.999 cc of water, the plant will die. In other words, a very small change in the input can lead to a large change in the output.\nCommon sense suggests that a change of 0.002 cc in the amount of water—that’s a small fraction of a drop, 2 cubic millimeters of volume, is not going to lead to a qualitative change in output. So you might prefer to use a sigmoidal function as your model rather than a Heaviside function.\nOn the other hand, sometimes a very small change in input does lead to a large change in output. For instance, a sensible model of the hardness of water as a function of temperature would include a discontinuity at \\(32^\\circ\\)F, the temperature at which water turns to ice.\nOne of author Charles Dickens’s famous characters described the relationship between income, expenditure, and happiness this way:\n\n“Annual income 20 pounds, annual expenditure 19 [pounds] 19 [shillings] and six [pence], result happiness. Annual income 20 pounds, annual expenditure 20 pounds ought and six, result misery.” — the character Wilkins Micawber in David Copperfield\n\nMacawber was referring to the common situation in pre-20th century England of putting debtors in prison, regardless of the size of their debt. Macawber’s statement suggests he would model happiness as a Heaviside function \\(H(\\text{income}- \\text{expenditure})\\).\nWhenever the output of a function is a binary (yes-or-no) value, you can anticipate that a model will involve a discontinuous function."
  },
  {
    "objectID": "Differentiation/21-cont-and-smooth.html#discontinuity",
    "href": "Differentiation/21-cont-and-smooth.html#discontinuity",
    "title": "22  Continuity and smoothness",
    "section": "22.2 Discontinuity",
    "text": "22.2 Discontinuity\nRecall the logical path that led us to the idea of the derivative of a function. We started with the differencing operator, which takes as input a function and a “small” value of \\(h\\): \\[{\\cal D}_x f(x) \\equiv \\frac{f(x+h) - f(x)}{h}\\] Then, through algebraic manipulation and numerical experiments we found that, once \\(h\\) is small enough, the graph of the slope function \\({\\cal D}_x f(x)\\) does not depend on \\(h\\). And so we defined a function \\(\\partial_x f(x)\\) where \\(h\\) doesn’t play a role, writing \\(\\lim_{h\\rightarrow 0}\\) to remember our care to never divide by zero. \\[\\partial_x f(x) \\equiv \\lim_{h\\rightarrow 0} \\frac{f(x+h) - f(x)}{h}\\ .\\] Conveniently, we found that the derivatives of the pattern-book functions can be written in terms of the pattern-book functions without making any reference to \\(h\\). For instance:\n\n\\(\\partial_x \\ln(x) = 1/x\\) No \\(h\\) appears.\n\\(\\partial_x e^x = e^x\\) No \\(h\\) appears\n\\(\\partial_x x^p = p\\, x^{p-1}\\) No \\(h\\) appears.\nand so on.\n\nWith discontinuous functions, we have no such luck. Figure 22.3 shows what happens if we compute \\({\\cal D}_x H(x)\\), the derivative of the Heaviside function, for smaller and smaller \\(h\\).\n\nH <- makeFun(ifelse(x >=0, 1, 0) ~ x)\nDH01   <- makeFun((H(x + 0.1) - H(x))/0.1 ~ x)\nDH001  <- makeFun((H(x + 0.01) - H(x))/0.01 ~ x)\nDH0001 <- makeFun((H(x + 0.001) - H(x))/0.001 ~ x)\nslice_plot(DH01(x) ~ x, domain(x=-0.02:0.02),\n           npts=500, color=\"red\", size=2) %>%\n  slice_plot(DH001(x) ~ x,\n           color=\"darkgreen\", npts=500, size=3, alpha=0.5) %>%\n  slice_plot(DH0001(x) ~ x,\n           color=\"blue\", npts=500, alpha=0.5, size=2) \n\n\n\n\nFigure 22.3: \\({\\cal D}_x H(x)\\), the slope function of the discontinuous Heaviside, function, depends on the value of \\(h\\) used for the slope function. (Red: \\(h=0.1\\); Green: \\(h=0.01\\); Blue \\(h=0.001\\))\n\n\n\n\nDifferencing the Heaviside function produces very different functions depending on the value of \\(h\\). The bump near \\(x=0\\) gets taller and taller as \\(h\\) gets smaller. Mathematicians would describe this situation as \\[\\lim_{h\\rightarrow0}{\\cal D}_x H(x=0) \\equiv \\lim_{h\\rightarrow 0} \\frac{H(0+h) - H(0)}{h}\\ \\ \\ \\text{does not exist}.\\] Of course, for any given value of \\(h\\), e.g. \\(h=0.000001\\), the function \\({\\cal D}_x H(x)\\) has a definite shape. But that shape keeps changing as \\(h \\rightarrow 0\\), so we can’t point to any specific shape as the “limit as \\(h \\rightarrow 0\\).”\nSince there is no convergence in the shape of \\({\\cal D}_x H(0)\\) as \\(h\\) gets smaller, it’s fair to say that the Heaviside function does not have a derivative at \\(x=0\\). But away from \\(x=0\\), the Heaviside function has a perfectly sensible derivative: \\(\\partial_x H(x) = 0\\) for \\(x\\neq 0\\). But there is no derivative at \\(x=0\\)."
  },
  {
    "objectID": "Differentiation/21-cont-and-smooth.html#smoothness",
    "href": "Differentiation/21-cont-and-smooth.html#smoothness",
    "title": "22  Continuity and smoothness",
    "section": "22.3 Smoothness",
    "text": "22.3 Smoothness\nSmoothness is a different concept than continuity, although the two are related. Most simply, any discontinuous function is not smooth at any input where a discontinuity occurs. But even the continuous ramp function is not smooth at the start of the ramp. Intuitively, imagine you were sliding your hand along the ramp function. You would feel the crease at \\(x=0\\).\nA function is not smooth if the derivative of that function is discontinuous. For instance, the derivative of the ramp function is the Heaviside function, so the ramp is not smooth at \\(x=0\\).\nAll of our basic modeling functions are smooth everywhere in their domain. In particular, the derivatives of the basic modeling functions are continuous, as are the second derivative, third derivative, and so on down the line. Such functions are called C-infinity, written \\(C^\\infty\\). The superscript \\(\\infty\\) means that every order of derivative is continuous.\n\n\n\nYou cannot tell from the plot that the second derivative is discontinuous. But if you were in a plane flying along that trajectory, you would feel a jerk as you crossed \\(x=0\\).\nMathematicians quantify the “smoothness” of a function by looking at the continuity of the function and its derivatives. The mathematical definition of smoothness is straightforward and phrased in terms of derivatives. Suppose you are examining the smoothness of a function \\(f(x)\\). The smoothness is assessed on a scale \\(C^0, C^1, C^2, \\ldots, C^\\infty\\).\n\n\\(C^0\\): the function \\(f()\\) is continuous. Intuitively, this means that a graph of the function can be drawn without lifting the pencil from the paper.\n\\(C^1\\): the function \\(f()\\) has a derivative over its entire domain and that derivative \\(\\partial_x f(x)\\) is continuous. (See ?fig-c1-function for an example.)\n\\(C^2\\): the function \\(\\partial_x f(x)\\) has a derivative over its entire domain and that derivative is continuous. In other words, \\(\\partial_{xx} f(x)\\) exists and is continuous.\n\\(C^n\\): Like \\(C^2\\), but we’re talking about the \\(n\\)th-derivative of \\(f(x)\\) existing and being continuous.\n\\(C^\\infty\\): Usually when we denote a sequence with an infinite number of terms, we write down something like \\(C^0, C^1, C^2, \\ldots\\). It would be entirely valid to do this in talking about the \\(C^n\\) sequence. But many of the mathematical functions we work with are infinitely differentiable, that is \\(C^\\infty\\).\n\nExamples of \\(C^\\infty\\) functions:\n\n\\(\\sin(x)\\): the derivatives are \\(\\partial_x \\sin(x) = \\cos(x)\\), \\(\\partial_{xx} \\sin(x) = -\\sin(x)\\), \\(\\partial_{xxx} \\sin(x) =-\\cos(x)\\), \\(\\partial_{xxxx} \\sin(x) =\\sin(x)\\), … You can keep going infinitely.\n\\(e^x\\): the derivatives are \\(\\partial_x e^x = e^x\\), \\(\\partial_{xx} e^x = e^x\\), and so on.\n\\(x^2\\): the derivatives are \\(\\partial_x x^2 = 2 x\\), \\(\\partial_{xx} x^2 = 2\\), \\(\\partial_{xxx} x^2 = 0\\), … Higher order derivatives are all simply 0. Boring, but still existing.\n\nExample of non-\\(C^2\\) functions: We see these often when we take two or more different \\(C^\\infty\\) functions and split their domain, using one function for one subdomain and the other(s) for other subdomain(s).\n\n\\(|x|\\), the absolute value function. \\(|x|\\) is a pasting together of two \\(C^\\infty\\) functions: \\[|x| \\equiv \\left\\{\\begin{array}{rcl}+x & \\text{for} & 0 \\leq x\\\\-x&\\text{for}& \\text{otherwise}\\end{array} \\right.\\ .\\] The domain is split at \\(x=0\\).\n\n\nFor engineering and design problems, smoothness means something substantially different than described by the mathematical concepts above. Later in the course we’ll introduce cubic splines which are continuous functions defined by a finite set of coordinate pairs, as in a data frame. Each line of the data frame corresponds to a dot in a scatter plot, but in a cubic spline it is called a “knot point.” The spline consists of cubic polynomials drawn between consecutive knot points. The domain is split at each of the knot points. Between any two adjacent knot points, the function is an ordinary cubic polynomial. At a knot point, the cubics on either side have been arranged to have their first and second derivatives match. Thus, the first two derivatives are continuous. The function is at least \\(C^2\\). The second derivative of a cubic is a straight-line function, so the second derivative of a cubic spline is a series of straight-line functions connected at the knot points. The second derivative does not itself have a derivative at the knot points. So, a cubic spline cannot satisfy the requirements for being \\(C^3\\); it is \\(C^2\\)."
  },
  {
    "objectID": "Differentiation/21-cont-and-smooth.html#exercises",
    "href": "Differentiation/21-cont-and-smooth.html#exercises",
    "title": "22  Continuity and smoothness",
    "section": "22.4 Exercises",
    "text": "22.4 Exercises"
  },
  {
    "objectID": "Differentiation/22-rules.html",
    "href": "Differentiation/22-rules.html",
    "title": "23  Derivatives of assembled functions",
    "section": "",
    "text": "In Section 20.4 we used the rules associated with \\(\\lim_{h\\rightarrow 0}\\) to confirm our claims about the derivatives of many of the pattern-book functions. We’ll call these rules h-theory for short. In this chapter, we’re going to use h-theory to find algebraic rules to calculate the derivatives of linear combinations of functions, products of functions, and composition of functions. Remarkably, we can figure out these rules without having to say specifically which functions are being combined. So the rules can be written in terms of abstractions: \\(f()\\), \\(g()\\), and \\(h()\\). Later, we’ll apply those rules to specific functions, to show how the rules are used in practical work."
  },
  {
    "objectID": "Differentiation/22-rules.html#sec-using-the-rules",
    "href": "Differentiation/22-rules.html#sec-using-the-rules",
    "title": "23  Derivatives of assembled functions",
    "section": "23.1 Using the rules",
    "text": "23.1 Using the rules\nWhen you encounter a function that you want to differentiate, you first have to examine the function to decide which rule you want to apply. In the following, we’ll to use the names \\(f()\\) and \\(g()\\), but in practice the functions will often be basic modeling functions, for instance \\(e^{kx}\\) or \\(\\sin\\left(\\frac{2\\pi}{P}t\\right)\\), etc.\nStep 1: Identify f() and g()\nWe will write the rules in terms of two function names, \\(f()\\) and \\(g()\\), which can stand for any functions whatsoever. It’s rare to see the product or the composition written explicitly as \\(f(x)g(x)\\) of \\(f(g(x))\\). Instead, you are given something like \\(e^x \\ln(x)\\). The first step in differentiating the product or composition is to identify what are \\(f()\\) and \\(g()\\) individually.\nIn general, \\(f()\\) and \\(g()\\) might be complicated functions, themselves involving linear combinations, products, and composition. But to get started, we’ll practice with cases where they are simple, pattern-book functions.\nStep 2: Find f’() and g’()\nFor differentiating either products or compositions, you will need to identify both \\(f()\\) and \\(g()\\) (the first step) and then compute the derivatives \\(\\partial_x f()\\) and \\(\\partial_x g()\\). That is, you’ll write down four functions.\nStep 3: Apply the relevant rule\nRecall from ?sec-fun-assembling that will will be working with three important forms for creating new functions out of existing functions:\n\nLinear combinations, e.g. \\(a f(x) + bg(x)\\)\nProducts of functions, e.g. \\(f(x) g(x)\\)\nCompositions of functions, e.g. \\(f\\left(g(x)\\right)\\)"
  },
  {
    "objectID": "Differentiation/22-rules.html#differentiating-linear-combinations",
    "href": "Differentiation/22-rules.html#differentiating-linear-combinations",
    "title": "23  Derivatives of assembled functions",
    "section": "23.2 Differentiating linear combinations",
    "text": "23.2 Differentiating linear combinations\nLinear combination is one of the ways in which we make new functions from existing functions. As you recall, linear combination involves scaling functions and then adding the scaled functions as in \\(a f(x) + b g(x)\\), alinear combination of \\(f(x)\\) and \\(g(x)\\). We can easily use \\(h\\) to show what is the result of differentiating a linear combination of functions. First, let’s figure out what is \\(\\partial_x\\, a f(x)\\), Going back to writing \\(\\partial_x\\) in terms of a slope function: \\[\\partial_x\\, a\\,f(x) = \\frac{a\\, f(x + h) - a\\,f(x)}{h}\\\\\n\\ \\\\\n= a \\frac{f(x+h) - f(x)}{h} = a\\, \\partial_x f(x)\\] In other words, if we know the derivative \\(\\partial_x\\, f(x)\\), we can easily find the derivative of \\(a\\, f()\\). Notice that even though \\(h\\) was used in the derivation, it appears nowhere in the result \\(\\partial_x\\, b\\,f(x) = b\\, \\partial_x\\, f(x)\\). The \\(h\\) is solvent to get the paint on the wall and evaporates once its job is done.\nNow consider the derivative of the sum of two functions, \\(f(x)\\) and \\(g(x)\\): \\[\\begin{eqnarray}\n\\partial_x\\, \\left[f(x) + g(x)\\right] & =\\frac{\\left[f(x + h) + g(x + h)\\right] - \\left[f(x) + g(x)\\right]}{h} \\\\\n\\ \\\\\n&= \\frac{\\left[f(x+h) -f(x)\\right] + \\left[g(x+h) - g(x)\\right]}{h}\\\\\n\\ \\\\\n&= \\frac{\\left[f(x+h) -f(x)\\right]}{h} + \\frac{\\left[g(x+h) - g(x)\\right]}{h}\\\\\n\\ \\\\\n&= \\partial_x\\, f(x) + \\partial_x\\, g(x)\n\\end{eqnarray}\\]\nBecause of the way that \\(\\partial_x\\) can be “passed through” a linear combination, mathematicians say that differentiation is a linear operator. Consider this new fact about differentiation as a down payment on what will eventually become a complete theory telling us how to differentiate a product of two functions or the composition of two functions. We’ll lay out the \\(h\\)-theory based algebra of this in the next two sections.\nWe can summarize the h-theory result for linear combinations this way:\n\nThe derivative of a linear combination is the linear combination of the derivatives.\n\nThat is:\n\\[\\partial_x \\left[\\strut \\color{magenta}{a} \\color{brown}{f(x)} + \\color{magenta}{b} \\color{brown}{g(x)}\\right] = \\color{magenta}{a} {\\large\\color{brown}{f'(x)}} + \\color{magenta}{b} {\\large\\color{brown}{g'(x)}}\\] as well as \\[\\partial_x \\left[\\strut \\color{magenta}{a}\\, \\color{brown}{f(x)} + \\color{magenta}{b}\\, \\color{brown}{g(x)}  + \\color{magenta}{c}\\, \\color{brown}{h(x)} + \\cdots\\right] = \\color{magenta}{a}\\, {\\large\\color{brown}{f'(x)}} + \\color{magenta}{b}\\, {\\large\\color{brown}{g'(x)}} + \\color{magenta}{c}\\, {\\large\\color{brown}{h'(x)}} + \\cdots\\]\n\nThe derivative of a polynomial is a polynomial of a lower order.\nConsider the polynomial \\[h(x) = \\color{magenta}{a}\\color{brown}{x^0}  + \\color{magenta}{b} \\color{brown}{x^1} + \\color{magenta}{c} \\color{brown}{x^2}\\] The derivative is \\[\\partial_x h(x) = \\color{brown}{0}\\, \\color{magenta}{a}  + \\color{brown}{1}\\, \\color{magenta}{b}  + \\color{magenta}{c}\\, \\color{brown}{2 x} = \\color{magenta}{b} +  \\color{magenta}{2 c}\\  x\\]"
  },
  {
    "objectID": "Differentiation/22-rules.html#product-rule-for-multiplied-functions",
    "href": "Differentiation/22-rules.html#product-rule-for-multiplied-functions",
    "title": "23  Derivatives of assembled functions",
    "section": "23.3 Product rule for multiplied functions",
    "text": "23.3 Product rule for multiplied functions\nThe question at hand is how to compute the derivative \\(\\partial_x f(x) g(x)\\). Of course, you can always use numerical differentiation. But let’s look at the problem from the point of view of symbolic differentiation. And since \\(f(x)\\) and \\(g(x)\\) are just pronoun functions, we’ll assume you are starting out already knowing the derivatives $_x f(x) \\(\\partial_x f(x)\\) and \\(\\partial_x g(x)\\).\nThis situation arises particularly when \\(f(x)\\) and \\(g(x)\\) are pattern-book functions for which you already have memorized \\(\\partial_x f(x)\\) and \\(\\partial_x g(x)\\) or are basic modeling functions whose derivatives you will memorize in Section @ref(basic-derivs).\nThe purpose of this section is to derive the formula for \\(\\partial_x f(x) g(x)\\) in terms of \\(f(x)\\), \\(g(x)\\), \\(\\partial_x f(x)\\) and \\(\\partial_x g(x)\\). This formula is called the product rule. The point of showing a derivation of the product rule is to let you see how the logic of evanescent \\(h\\) plays a role. In practice, everyone simply memorizes the rule, which has a beautiful, symmetric form:\n\\[\\text{Product rule:}\\ \\ \\ \\ \\partial_x \\left[\\strut f(x)g(x)\\right] = \\left[\\strut \\partial_x f(x)\\right]\\, g(x) + f(x)\\, \\left[\\strut\\partial_x g(x)\\right]\\] and is even prettier in Lagrange notation (where \\(\\partial_x f(x)\\) is written \\(f'\\)): \\[ \\left[\\strut f g\\right]' = f' g + g' f\\]\nAs with all derivatives, the product rule is based on the instantaneous rate of change \\[F'(x) \\equiv \\lim_{h\\rightarrow 0} \\frac{F(x+h) - F(x)}{h}\\] introduced in Section 17.5.\nWe also need two other statements about \\(h\\) and functions:\n\nThe derivative \\(F'(x)\\) is the slope of of \\(F()\\) at input \\(x\\). Taking a step of size \\(h\\) from \\(x\\) will induce a change of output of \\(h F'(x)\\), so \\[F(x+h) = f(x) + h F'(x)\\ .\\]\nAny result of the form \\(h F(x)\\), where \\(F(x)\\) is finite, gives 0. More precisely, \\(\\lim_{h\\rightarrow 0} h F(x) = 0\\)\n\nAs before, we’ll put the standard \\(\\lim_{h\\rightarrow 0}\\) disclaimer against dividing by \\(h\\) until there are no such divisions at all, at which point we can safely use the equality \\(h = 0\\).\nSuppose the function \\(F(x) \\equiv f(x) g(x)\\), a product of the two functions \\(f(x)\\) and \\(g(x)\\).\n\\[\\require{cancel} F'(x) = \\partial_x \\left[\\strut f(x) g(x) \\right] \\equiv \\lim_{h\\rightarrow 0}\\frac{f(x+h) g(x+h) - f(x) g(x)}{h}\\] We’ll replace \\(g(x_h)\\) with its equivalent \\(g(x) + h g'(x)\\) giving\n\\[= \\lim_{h\\rightarrow 0} \\frac{f(x+h) \\left[\\strut g(x) + h g'(x) \\right] - f(x) g(x)}{h} \\] \\(g(x)\\) appears in both terms in the numerator, once multiplied by \\(f(x+h)\\) and once by \\(f(x)\\). Collecting those terms give:\n\\[=\\lim_{h\\rightarrow 0}\\frac{\\left[\\strut f(x+ h) - f(x)\\right]  g(x) + \\left[\\strut f(x+h) h\\, g'(x)\\right]}{h}\\] This has two bracketed terms added together over a common denominator. Let’s split them into separate terms:\n\\[=\\lim_{h\\rightarrow 0}\\underbrace{\\left[\\strut \\frac{f(x+h) - f(x)}{h}\\right]}_{f'(x)} g(x) + \\lim_{h\\rightarrow 0}\\frac{\\left[\\strut f(x) + h f'(x)\\right]h\\,g'(x)}{h}\\]\nThe first term is \\(g(x)\\) multiplied by the familiar form for the derivative of \\(f(x)\\) \\[= f'(x) g(x) + \\lim_{h\\rightarrow 0}\\frac{f(x) h g'(x)}{h} + \\lim_{h\\rightarrow 0}\\frac{h f'(x) h g'(x)}{h}\\] In each of the last two terms there is an \\(h/h\\) involved. This is safely set to 1, since the \\(\\lim_{h\\rightarrow 0}\\) implies that \\(h\\) will not be exactly zero. There remain no divisions by \\(h\\) so we can drop the \\(\\lim_{h\\rightarrow 0}\\) in favor of \\(h=0\\): \\[= f'(x) g(x) + f(x) g'(x) + \\cancel{h f'(x) g'(x)}\\]\n\\[=f'(x) g(x) + g'(x) f(x)\\]\nThe last step relies on statement (2) above.\nSome people find it easier to read the rule in Lagrange shorthand, where \\(f\\) and \\(g\\) stand for \\(f(x)\\) and \\(g(x)\\) respectivly, and \\(f'\\) (“f-prime”) and \\(g'\\) (“g-prime”) stand for \\(\\partial f()\\) and \\(\\partial g()\\).\n\\[\\large\\text{Lagrange shorthand:}\\ \\   \\partial[\\color{magenta}f \\times \\color{brown}g] = [\\color{magenta}f \\times \\color{brown}g]' = \\color{magenta}{f'}\\color{brown}g + \\color{brown}{g'}\\color{magenta}f\\]\n\nThe expression \\(\\partial_x x^3\\) is the same as \\(\\partial_x \\left[\\strut x\\  x^2\\right]\\). Since we already know \\(\\partial_x x\\) (it’s 1) and \\(\\partial_x x^2\\) (it’s \\(2x\\)) let’s apply the product rule to find \\(\\partial_x x^3\\): \\[\\large\\partial [\\color{magenta}x \\times \\color{brown}{x^2}] = \\color{magenta}{[\\partial x]} \\times \\color{brown}{x^2} \\ +\\  \\color{brown}{[\\partial x^2]} \\times \\color{magenta}x =\\color{magenta}1\\times \\color{brown}{x^2} + \\color{brown}{2x} \\times \\color{magenta}x = 3 x^2\\]\n\n\nOccasionally, mathematics gives us a situation where being more general produces simplicity.\nIn the case of function products, the generalization is from products of two functions \\(f(x)\\cdot g(x)\\) to products of more than two functions, e.g. \\(u(x) \\cdot v(x) \\cdot w(x)\\).\nThe chain rule here takes a form that makes the overall structure much clearer:\n\\[\\begin{eqnarray}\n\\partial_x \\left[\\strut u(x) \\cdot v(x) \\cdot w(x)\\right] = \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\\\n\\color{blue}{\\partial_x u(x)} \\cdot v(x) \\cdot w(x)\\ + \\\\\nu(x) \\cdot \\color{blue}{\\partial_x v(x)} \\cdot w(x)\\ + \\\\\nu(x) \\cdot v(x) \\cdot \\color{blue}{\\partial_x w(x)}\\ \\  \\ \\\n\\end{eqnarray}\\]\\end{eqnarray}\nIn the Lagrange shorthand, the pattern is even more evident: \\[\\left[ u\\cdot v\\cdot w\\right]' = \\color{blue}{u'}\\cdot v\\cdot w\\ +\\ u\\cdot \\color{blue}{v'}\\cdot w\\ +\\ u\\cdot v\\cdot \\color{blue}{w}'\\]"
  },
  {
    "objectID": "Differentiation/22-rules.html#chain-rule-for-function-composition",
    "href": "Differentiation/22-rules.html#chain-rule-for-function-composition",
    "title": "23  Derivatives of assembled functions",
    "section": "23.4 Chain rule for function composition",
    "text": "23.4 Chain rule for function composition\nA function composition, as described in Section 9.2, involves inserting the output of one function (the “interior function”) as the input of the other function (the “exterior function”). As we so often do, we’ll be using pronouns a lot. A list might help keep things straight:\n\nThere are two functions involved in a composition. We’ll call them \\(f(y)\\) and \\(g(x)\\). In the composition \\(f(g(x))\\), the exterior function is \\(f()\\) and the interior function is \\(g()\\).\nEach of the two functions \\(f()\\) and \\(g()\\) has an input. In our examples, we’ll use \\(y\\) to stand for the input to the exterior function and \\(x\\) as the pronoun for the input to the interior function.\nAs with all rules for differentiation, we’ll need to compute the derivatives of the functions involved, each with respect to its own input. So these will be \\(\\partial_y f(y)\\) and \\(\\partial_x g(x)\\).\n\nA reason to use different pronouns for the inputs to \\(f()\\) and \\(g()\\) is to remind us that the output \\(g(x)\\) is in general not the same kind of quantity as the input \\(x\\). In a function composition, the \\(f()\\) function will take the output \\(g(x)\\) as input. But since \\(g(x)\\) is not necessarily the same kind of thing as \\(x\\), why would we want to use the same name for the input to \\(f()\\) as we use for the input to \\(g()\\).\nWith this distinction between the names of the inputs, we can be even more explicit about the composition, writing \\(f(y=g(x))\\) instead of \\(f(g(x))\\). Had we used the pronound \\(x\\) for the input to \\(f()\\) but our explicit statement, although technically correct, would be confusing: \\(f(x = g(x))\\)!\nWith all these pronouns in mind, here is the chain rule for the derivative \\(\\partial_x f(g(x))\\):\n\\[\\large\\partial_x \\left[\\strut \\color{magenta}{f\\left(\\strut\\right.}\\strut \\color{brown}{g(x)}\\color{magenta}{\\left.\\right)}\\right] = [\\color{magenta}{\\partial_y f}](\\color{brown}{g(x)}) \\times [\\color{brown}{\\partial_xg(x)}]\\] Or, using the Lagrange prime notation, where \\('\\) stands for the derivative of a function with respect to its input, we have \\[\\large\\text{Lagrange shorthand:}\\ \\   [\\color{magenta}f(\\color{brown}g)]' = \\color{magenta}{f'} (\\color{brown}g) \\times \\color{brown}{g}'\\]\n\nIn news and policy discussions, you will often hear about “inflation rate” or “birth rate” or “interest rate” or “investment rate of return.” In each case, there is a function of time combined with a derivative of that function: with the general form \\[\\frac{\\partial_t f(t)}{ f(t)}\\ .\\]\n\nInflation rate: The function is cost_of_living(\\(t\\)). The derivative is the rate of change with respect to time in the cost of living: \\(\\partial_t\\,\\)cost_of_living(\\(t\\)).\nBirth rate: The function is population(\\(t\\)). The derivative is \\(\\partial_t\\,\\)population(\\(t\\)), or at least that component of the overall \\(\\partial_t\\,\\)population(\\(t\\)) that is related to births. (Other components are deaths and the balance of in-migration and out-migration.)\nInterest rate: The function is account_balance(\\(t\\)) and the derivative is \\(\\partial_t\\,\\)account_balance(\\(t\\)).\nInvestment returns: The function is net_worth(\\(t\\)) and the derivative is \\(\\partial_t\\,\\)net_worth(\\(t\\)).\n\nIn all these cases, The “rate” is not merely “per time” as would be the case for \\(\\partial_t f(t)\\). Instead the rate is “per unit of the whole per time.” Thus the birth rate is “births per capita per year.”1 Interest and return rates are “percent per year” where the “percent” understood to be the “change-in-value divided by the current value.”\nThanks to the chain rule, there is a shortcut way of writing these sorts of “rates per time.” Exactly equivalent to the ratio \\(\\frac{\\partial_t f(t)}{ f(t)}\\) is \\[\\partial_t \\ln(f(t))\\ .\\]\nSuch changes in logarithms are encountered in fields such as economics or finance, where it’s common to consider the logarithm of the economic quantity in order to render changes as percent of the whole.\nIt’s also something to keep in mind when interpreting graphs of an amount versus time, as in Figure 23.1. Source\n\n\n\n\n\nFigure 23.1: Growth in the number of Coronavirus cases in Italy and the US early in the pandemic.\n\n\n\n\nLook closely at the two graphs in Figure 23.1. They show the same data about growing numbers of coronavirus cases, the left graph on linear axes, the right on the now-familiar semi-log axes.\nMost people are excellent at comparing slopes, even if they find it difficult or tedious to quantify a slope with a number and units. For instance, a glance suffices to show that in the left graph, well through mid-March the red curve (Italy) is steeper on any given date than the blue curve (US). This means that the number of people with coronavirus was growing faster (per day) in Italy.\nThe right graph tells a different story: up until about March 1, the Italian cases were increasing faster than the US cases. Afterwards, the US sees a larger growth rate than Italy until, around March 19, the US growth rate is substantially larger than the Italy growth rate.\nThe previous two paragraphs and their corresponding graphs may seem to contradict one another. But they are both accurate, truthful depictions of the same events. What’s different between the two graphs is that the left shows one kind of rate and the right shows another kind of rate. In the left, the slope is new-cases-per-day, the output of the derivative function \\[{\\mathbf{\\text{left graph:}}} \\ \\ \\ \\  \\partial_t \\text{daily_new_cases}(t)\\ .\\] On the right, the slope is the proportional increase in cases per day, that is, \\[{\\mathbf{\\text{right graph:}}}\\ \\ \\ \\ \\frac{\\partial_t \\text{daily_new_cases}(t)}{\\text{daily_new_cases}(t)}\\] From the chain rule, we know that \\[\\partial_t \\left[\\strut\\ln(f(t))\\right] = \\frac{\\partial_t f(t)}{f(t)}\\] Since the right graph is on semi-log axes, the slope we perceive visually is \\(\\partial_t \\left[\\strut\\ln(f(t))\\right]\\). That’s an obscure-looking bunch of notation until the chain rule reveals it to be the rate of change at time \\(t\\) divided by the value at time \\(t\\).\n\nThe derivation of the chain rule relies on two closely related statements which are expressions of the idea that near any value \\(x\\) a function can be expressed as a linear approximation with the slope equal to the derivative of the function :\n\n\\(g(x + h) = g(x) + h g'(x)\\)\n\\(f(y + \\epsilon) = f(y) + \\epsilon f'(y)\\), which is the same thing as (1) but uses \\(y\\) as the argument name and \\(\\epsilon\\) to stand for the small quantity we usually write with an \\(h\\).\n\nWe’ll now look at \\(\\partial_x f\\left({\\large\\strut} g(x)\\right)\\) by writing down the fundamental definition of the derivative. This, of course, involves the disclaimer \\(\\lim_{h\\rightarrow 0}\\) until we’re sure that there is no division by \\(h\\) involved.\n\\[\\partial_x \\left[{\\large\\strut} f\\left(\\strut g(x)\\right)\\right]  \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{magenta}{f(g(x+h))} - f(g(x))}{h}\\]\nLet’s examine closely the expression \\(\\color{magenta}{f\\left(\\strut g(x+h)\\right)}\\). Applying rule (1) above turns it into \\[\\lim_{h\\rightarrow 0} f\\left(\\strut g(x) + \\color{blue}{h g'(x)}\\right)\\] Now apply rule (2) but substituting in \\(g(x)\\) for \\(y\\) and \\(\\color{blue}{h g'(x)}\\) for \\(\\epsilon\\), giving\n\\[\\lim_{h\\rightarrow 0} \\color{magenta}{f\\left(\\strut g(x+h)\\right)} = \\lim_{h\\rightarrow 0} \\color{brown}{\\left[{\\large\\strut} f\\left(g(x)\\right) + \\color{blue}{h g'(x)}f'\\left(g(x)\\right)\\right]}\\] We’ll substitute the \\(\\color{blue}{blue}\\) and \\(\\color{brown}{brown}\\) expression for the \\(\\color{magenta}{magenta}\\) expression in \\[\\partial_x f\\left(\\strut g(x)\\right)  \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{magenta}{f(g(x+h))} - f(g(x))}{h}\\] giving \\[\\partial_x f\\left(\\strut g(x)\\right)  \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{brown}{f\\left(g(x)\\right) + \\color{blue}{h g'(x)}f'\\left(g(x)\\right)} - f\\left(g(x)\\right)}{h}\\] In the denominator, \\(f\\left(g(x)\\right)\\) appears twice and cancels itself out. That leaves a single term with an \\(h\\) in the numerator and an \\(h\\) in the denominator. Those \\(h\\)’s cancel out, at the same time obviating the need for \\(\\lim_{h\\rightarrow 0}\\) and leaving us with the chain rule: \\[\\partial_x f\\left(\\strut g(x)\\right)  \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{brown}{ \\color{blue}{h g'(x)} f'\\left(g(x)\\right)}}{h} = f'\\left(g(x)\\right)\\ g'(x)\\]\n\nUse the chain rule to find the derivative \\(\\partial_x e^{2x}\\).\nRecognize that \\(g(x) \\equiv 2x\\) is the interior function in \\(e^{2x}\\) and \\(f(x) \\equiv \\exp(x)\\) is the exterior function. Thus \\[\\partial_x e^{2x} = f'(g(x)) g'(x) = \\exp(g(x)) 2 = 2 e^{2x}\\ .\\] Happily, this is the same result as we got from using the product rule to find \\(\\partial_x e^{2x}\\).\nRecognizing \\(e^{2x}\\) as \\(e^x \\times e^x\\), we can apply the product rule.\n\n\nThe chain rule can be used in a clever way to find a formula for \\(\\partial_x \\ln(x)\\).\nWe’ve already seen that the logarithm is the inverse function to the exponential, and vice versa. That is: \\[e^{\\ln(y)} = y \\ \\ \\ \\text{and}\\ \\ \\ \\ln(e^y) = x\\] Since \\(\\ln(e^y)\\) is the same function as \\(y\\), the derivative \\(\\partial_y \\ln(e^y) = \\partial_y y = 1\\).\nLet’s differentiate the second form using the chain rule: \\[\\partial_y \\ln(e^y) = \\left[\\partial_y \\ln\\right](e^y)\\, e^x = 1\\] giving \\[\\left[\\partial_y \\ln\\right](e^y) = \\frac{1}{e^y} = \\recip(e^y)\\] Whatever the function \\(\\partial_x \\ln()\\) might be, it takes its input and produces as output the reciprocal of that input. In other words: \\[\\partial_x \\ln(x) = \\frac{1}{x}\\ .\\]\n\n\nKnowing that \\(\\partial_x \\ln(x) = 1/x\\) and the chain rule, we’re in a position to demonstrate the power-law rule \\(\\partial_x x^p = p\\, x^{p-1}\\). The key is to use the identity \\(e^{\\ln(x)} = x\\).\n\\[\\partial_x x^p = \\partial_x \\left[e^{\\ln(x)}\\right]^p\\] The rules of exponents allow us to recognize \\[\\left[e^{\\ln(x)}\\right]^p = e^{p \\ln(x)}\\] Thus, \\(x^p\\) can be seen as a composition of the exponential function onto the logarithm function.\nApplying the chain rule to this composition gives \\[\\partial_x e^{p \\ln(x)} = e^{p\\ln(x)}\\partial_x [p \\ln(x)] =\ne^{p\\ln(x)} \\frac{p}{x}\\ .\\] Of course, we already know that \\(e^{p \\ln(x)} = x^p\\), so we have \\[\\partial_x x^p = x^p \\frac{p}{x} = p x^{p-1}\\ .\\]\n\n\n\\(\\large\\partial_x [\\color{brown}\\sin(\\color{magenta}{a x + b})] = [\\partial_x \\color{brown}{\\sin}](\\color{magenta}{a x + b}) \\times \\partial_x [\\color{magenta}{ax + b}] = \\color{brown}{\\cos}(\\color{magenta}{ax + b}) \\times \\color{magenta}a\\).\n\n\n\nIn 1734, famous philosopher George Berkeley (1685-1753) published a long-titled book: The Analyst: A Discourse Addressed to an Infidel Mathematician: Wherein It Is Examined Whether the Object, Principles, and Inferences of the Modern Analysis Are More Distinctly Conceived, or More Evidently Deduced, Than Religious Mysteries and Points of Faith. In The Analyst, Berkeley took issue with the arguments of that time that it is legitimate to divide by \\(h\\) when, ultimately, \\(h\\) will be replaced by zero. Calling \\(h\\) an “evanescent increment,” he asked,\n\n“And what are these same evanescent Increments? They are neither finite Quantities nor Quantities infinitely small, nor yet nothing. May we not call them the ghosts of departed quantities?”\n\nInteresting, Berkeley believed that the ghost of \\(h\\) yielded correct results. His objection was that the framers of calculus had made two, canceling errors.\n\n“[B]y virtue of a two fold mistake you arrive, though not at science, yet truth.”\n\nBerkeley was saying that calculus had not yet been put on a solid logical foundation. It wasn’t until more than a century after Berkeley’s death that this work was accomplished. Once accomplished, the results that had been claimed true all along were confirmed."
  },
  {
    "objectID": "Differentiation/22-rules.html#sec-basic-derivs",
    "href": "Differentiation/22-rules.html#sec-basic-derivs",
    "title": "23  Derivatives of assembled functions",
    "section": "23.5 Derivatives of the basic modeling functions",
    "text": "23.5 Derivatives of the basic modeling functions\nThe basic modeling functions are the same as the pattern-book functions, but with bare \\(x\\) replaced by \\(\\line(x)\\). In other words, each of the basic modeling functions is a composition of the corresponding pattern-book function with \\(\\line(x)\\). As such, the derivatives of the basic modeling functions can be found using the chain rule.\nSuppose \\(f()\\) is one of our pattern-book functions. Then \\[\\large\\partial_x f(\\color{magenta}{ax + b}) = \\color{brown}{a} f'(\\color{magenta}{ax + b})\\] where \\(\\color{brown}{a}\\) is the derivative with respect to \\(x\\) of \\(\\color{magenta}{ax + b}\\).\nHere are the steps for differentiating a basic modeling function \\(\\color{brown}{f}(\\color{magenta}{a x + b})\\) where \\(f()\\) is one of the pattern-book functions:\n\nStep 1: Identify the particular pattern-book function \\(\\color{brown}{f}()\\) and write down its derivative \\(\\color{brown}{f'}\\). For example, if \\(f()\\) is \\(\\sin()\\), then \\(f'()\\) is \\(\\cos()\\).\nStep 2: Find the derivative of the linear interior function. If the function is \\(\\color{magenta}{ax + b}\\), then the derivative is \\(\\color{magenta}{a}\\). If the interior function is \\(\\frac{2\\pi}{P}(t-t_0)\\), the derivative is \\(\\frac{2 \\pi}{P}\\).\nStep 3: Write down the original function \\(\\large\\color{brown}{f}(\\color{magenta}{a x + b})\\) but replace \\(\\large\\color{brown}{f}\\) with \\(\\large \\color{brown}{f'}\\) and pre-multiply by the derivative of the interior function. For instance, \\[\\partial_x f(\\color{magenta}{ax + b}) = {\\large \\color{magenta}{a}}{\\large f'}(\\color{magenta}{ax + b})\\] Another example: \\[\\partial_t \\color{brown}{\\sin}\\left(\\color{magenta}{\\frac{2 \\pi}{P}(t-t_0)} \\right) = {\\large \\color{magenta}{\\frac{2 \\pi}{P}}}\\color{brown}{\\large\\cos}\\left(\\color{magenta}{\\frac{2 \\pi}{P}(t-t_0) }\\right) \\]\n\nBy convention, there are different ways of writing \\(\\line(x)\\) for the different pattern-book functions, for instance:\n\n\n\n\n\n\n\nPattern-book function \\(\\longrightarrow\\)\nBasic modeling\n\n\n\n\n\\(\\sin(x)\\ \\ \\ \\longrightarrow\\)\n\\(\\sin\\left(\\strut2 \\pi \\left[x-x_0\\right]/P\\right)\\)\n\n\n\\(\\exp(x)\\ \\ \\ \\longrightarrow\\)\n\\(\\exp(k x)\\)\n\n\n\\(x^2 \\ \\ \\ \\longrightarrow\\)\n\\(\\left[mx + b\\right]^2\\)\n\n\n\\(1/x \\ \\ \\ \\longrightarrow\\)\n\\(1/\\left[mx + b\\right]\\)\n\n\n\\(\\ln(x) \\ \\ \\ \\longrightarrow\\)\n\\(\\ln(a x + b)\\\\\\)\n\n\n\nThe rule for the derivative of any basic modeling function \\(f(\\line(x))\\) is \\[\\partial_x f(\\line(x)) = \\partial_x \\line(x) \\times \\partial_x f\\left(\\strut\\line(x)\\right)\\]\nTo illustrate:\n\n\\(\\partial_x e^{\\color{magenta}{kx}} = {\\large\\color{magenta}{k}}\\, e^{\\color{magenta}{kx}}\\) where \\(\\line(x) = kx\\).\n\\(\\partial_x \\sin(2\\pi (x-x_0)/P) = \\frac{2\\pi}{P} \\sin(2\\pi (x-x_0)/P)\\) where \\(\\line(x) = 2\\pi (x-x_0)/P)\\).\n\\(\\partial_x (mx + b)^2 = m\\, 2 (m x + b) = 2 m^2 x + m^2 b\\) where \\(\\line(x) = mx + b\\).\n\\(\\partial_x \\text{reciprocal}(mx + b) = \\partial_x \\frac{1}{mx + b} = - \\frac{m}{(mx + b)^2}\\) where \\(\\line(x) = mx + b\\) and we use the fact that \\(\\partial_x \\text{reciprocal}(x) = - 1/x^2\\)\n\\(\\partial_x \\ln(a x + b) = a/(ax+b)\\)\n\\(\\partial_x \\pnorm(x, \\text{mean}, \\text{sd}) = dnorm(x, \\text{mean}, \\text{sd})\\).\n\\(\\partial_x \\dnorm(x, \\text{mean}, \\text{sd}) = - \\frac{x-m}{\\text{sd}^2} \\dnorm(x, \\text{mean}, \\text{sd})\\)\n\nYou will be using the derivatives of the basic modeling functions so often, that you should practice and practice until you can write the derivative at a glance.\n\nThere are many possible implementations of the general concept of hump functions and sigmoidal functions. The one we use in this book is \\(\\dnorm()\\) for the hump and \\(\\pnorm()\\) for the sigmoid.\nThe names \\(\\dnorm\\) and \\(\\pnorm\\) are worth remarking on. As we’ve said before, \\(\\dnorm()\\) is called the gaussian function in many fields of science and engineering. It is also a centrally important function in statistics, where it is usually called the normal function. (That’s how important it is: it’s just “normal.”) You may also have heard the normal function described as a “bell-shaped curve.”\nIn statistical nomenclature, \\(\\dnorm()\\) is called the “normal probability density function (PDF)” and \\(\\pnorm()\\) is called the “normal cumulative density function (CDF).” That’s way too wordy for our purposes. So, for brevity, we have adopted the R name for those functions: dnorm() and pnorm().\nOwing to the origin of the names \\(\\dnorm\\) and \\(\\pnorm\\), we are writing the parameters of the functions—mean and sd—using the computer language notation. The pattern-book functions are just \\(\\dnorm(x)\\) and \\(\\pnorm(x)\\), without listing the parameters. But the basic modeling functions, with parameters, are written \\(\\dnorm(x, \\text{mean}, \\text{sd})\\) and \\(\\dnorm(x, \\text{mean}, \\text{sd})\\). This violates the convention that the basic modeling functions are the composition of the pattern-book functions with \\(\\line(x)\\). But \\(\\dnorm()\\) doesn’t work this way because, by convention, the amplitude of the peak of \\(\\dnorm()\\) changes with the input parameter sd. That’s not true for any other basic modeling function.\n\n\nComposition or product?\nThere is one family of functions for which function composition is the same thing as multiplying functions: the power-law family.\nConsider, for instance, the function \\(h(x) \\equiv \\left[3x\\right]^4\\). Let’s let \\(g(x) \\equiv 3x\\) and \\(f(y) \\equiv y^4\\). With these definitions, \\(h(x) = f(g(x))\\).\nRecognizing that \\(\\partial_y f(y) = 4 y^3\\) and \\(\\partial_x g(x) = 3\\), the chain rule gives \\[\\partial_x h(x) =\n\\underbrace{4 g(x)^3}_{f'(g(x))} \\times \\underbrace{3}_{g'(x)} = \\underbrace{4 (3 x)^3}_{f'(g(x))} \\times 3 = 4\\cdot 3^4 \\times x^3 = 324\\ x^3\\] Another way to look at the same function is \\(g(x)\\) multiplied by itself 3 times: \\[h(x) = g(x)\\cdot g(x) \\cdot g(x) \\cdot g(x)\\] This is a product of 4 terms. Applying the product rule gives \\[\\begin{eqnarray}\n\\partial_x h(x) &=& \\color{blue}{g'(x)}\\cdot g(x)\\cdot g(x) \\cdot g(x) +\\\\\n&\\ & g(x)\\cdot \\color{blue}{g(x)}'\\cdot g(x) \\cdot g(x) +\\\\\n&\\ & g(x)\\cdot g(x)\\cdot \\color{blue}{g(x)'} \\cdot g(x) +\\\\\n&\\ & g(x)\\cdot g(x)\\cdot g(x) \\cdot \\color{blue}{g'(x)}\n\\end{eqnarray}\\] Since multiplication is commutative, all of these four terms are the same, each being \\(3^4 x^3\\). The sum of all four is therefore \\(4 \\times 3^4 x^3 = 324 x^3\\).\nThese are two long-winded ways of getting to the result. For most people, differentiating power-law functions algebraically is simplified by using the rules of exponentiation rather than the product or chain rule. Here, \\[h(x) \\equiv \\left[3x\\right]^4 = 3^4 x^4\\]so \\(\\partial_x h(x)\\) is easily handled as a scalar (\\(3^4\\)) times a function \\(x^4\\). Consequently, applying the rule for differentiating power laws, \\[\\partial_x h(x) = 3^4 \\times \\partial_x x^4 = 3^4 \\times 4 x^3 = 324 x^3\\] As another example, take \\(h(x) \\equiv \\sqrt[4]{\\strut x^3}\\). This is, of course, the composition \\(f(g(x))\\) where \\(f(y) \\equiv y^{1/4}\\) and \\(g(x) \\equiv x^3\\). Applying the chain rule to find \\(\\partial_x h(x)\\) will work (of course!), but is more work than applying the rules of exponentiation followed by a simple power-law differentiation. \\[h(x) = \\sqrt[4]{\\strut x^3} = x^{3/4}\\ \\ \\text{so}\\ \\  \\partial_x h(x) = \\frac{3}{4} x^{(3/4 - 1)} = \\frac{3}{4} x^{-1/4}\\]"
  },
  {
    "objectID": "Differentiation/22-rules.html#exponentials-and-logarithms-optional",
    "href": "Differentiation/22-rules.html#exponentials-and-logarithms-optional",
    "title": "23  Derivatives of assembled functions",
    "section": "23.6 Exponentials and logarithms (optional)",
    "text": "23.6 Exponentials and logarithms (optional)\nThe natural logarithm function, \\(\\ln(x)\\), is one of our basic modeling functions. As you know, there are other logarithmic functions. The one most often used is the logarithm-base-10, written \\(\\log_{10}(x)\\) or log10(x). Ten is an integer, and a nice number to use in arithmetic. So in practice, it’s sensible to use \\(\\log_{10}()\\). (Indeed, \\(\\log_{10}()\\) is the digit() function, introduced in ?sec-magnitudes).\nThe “natural” in the “natural logarithm” means something different.\nThe base of the natural logarithm is the number called Euler’s constant and written \\(e\\). As a celebrity number, \\(e\\) is right up there with \\(\\pi\\) and \\(i\\). Just as \\(\\pi\\) has a decimal expansion that is infinitely long, the familiar \\(\\pi = 3.14159265358979...\\), Euler’s constant has an infinitely long decimal representation: \\(e = 2.71828182845905...\\)\nIt’s not obvious at first glance why \\(e = 2.71828182845905...\\) should be called “natural” by mathematicians. The reason is not the number itself, but\n\n\\(\\ln(x)\\) is the inverse of \\(e^x\\), which is special for being invariant under differentiation: \\(\\partial_x e^x = e^x\\).\nThe derivative \\(\\partial_x \\ln(x)\\) which has a particularly simple form, namely, \\(1/x\\).\n\nLet’s look at the log-base-10 and it’s computer-savvy cousin log-base-2. The very definition of logarithms means that both 10 and 2 can be written \\[10 = e^{\\ln(10)}\\ \\ \\ \\text{and}\\ \\ \\ 2 = e^{\\ln(2)}\\] This implies that the base-10 and base-2 exponential functions can be written\n\\[10^x = \\left[\\strut e^{\\strut\\ln(10)}\\right]^x = e^{\\ln(10)x} \\ \\ \\ \\text{and}\\ \\ \\ 2^x = \\left[\\strut e^{\\strut\\ln(2)}\\right]^x = e^{\\ln(2) x}\\] Calculating \\(\\partial_x 10^x\\) or \\(\\partial_x 2^x\\) is a matter of applying the chain rule:\n\\[\\partial_x [10^x] = \\partial_x [e^{\\ln(10)x}] = e^{\\ln(10)x} \\times \\ln(10) \\ =\\  10^x \\times 2.3026\\] and \\[\\partial_x [2^x] = \\partial_x [e^{\\ln(2)x}] = e^{\\ln(2)x} \\times \\ln(2) \\ = \\ 2^x \\times 0.6931\\] Like \\(e^x\\), the derivatives of \\(10^x\\) and \\(2^x\\) are proportional to themselves. For \\(e^x\\) the constant of proportionality is 1, a very natural number indeed."
  },
  {
    "objectID": "Differentiation/22-rules.html#exercises",
    "href": "Differentiation/22-rules.html#exercises",
    "title": "23  Derivatives of assembled functions",
    "section": "23.7 Exercises",
    "text": "23.7 Exercises"
  },
  {
    "objectID": "Differentiation/22-rules.html#drill",
    "href": "Differentiation/22-rules.html#drill",
    "title": "23  Derivatives of assembled functions",
    "section": "23.8 Drill",
    "text": "23.8 Drill\n\n\nPart i Which of the derivative rules should you use to find \\[\\partial_t e^{t^2}\\ ?\\]\n\nThe constant multiplier rule\nThe linear combination rule\nThe product rule\nThe chain rule\nNo rule needed, it’s so basic.\n\n\n\n\n\nPart ii Which of the derivative rules should you use to find \\[\\partial_t e^{x^2}\\ ?\\]\n\nThe constant multiplier rule\nThe linear combination rule\nThe product rule\nThe chain rule\nNo rule needed, it’s so basic.\n\n\n\n\n\nPart iii Which of the derivative rules should you use to find \\[\\partial_t e^t \\sin(t)\\ ?\\]\n\nThe constant multiplier rule\nThe linear combination rule\nThe product rule\nThe chain rule\nNo rule needed, it’s so basic.\n\n\n\n\n\nPart iv Which of the derivative rules should you use to find \\[\\partial_t e^t \\sin(x)\\ ?\\]\n\nThe constant multiplier rule\nThe linear combination rule\nThe product rule\nThe chain rule\nNo rule needed, it’s so basic.\n\n\n\n\n\nPart v Which of the derivative rules should you use to find \\[\\partial_t \\ln(t)\\ ?\\]\n\nThe constant multiplier rule\nThe linear combination rule\nThe product rule\nThe chain rule\nNo rule needed, it’s so basic.\n\n\n\n\n\nPart vi Which of the derivative rules should you use to find \\[\\partial_t\\, t\\, e^{-t}\\ ?\\]\n\nThe constant multiplier rule\nThe linear combination rule\nThe product rule\nThe chain rule\nNo rule needed, it’s so basic.\n\n\n\n\n\nPart vii Which of the derivative rules should you use to find \\[\\partial_x\\ 37 x^5\\ ?\\]\n\nThe constant multiplier rule\nThe linear combination rule\nThe product rule\nThe chain rule\nNo rule needed, it’s so basic.\n\n\n\n\n\nPart viii Which of the derivative rules should you use to find \\[\\partial_x\\ 19\\ ?\\]\n\nThe constant multiplier rule\nThe linear combination rule\nThe product rule\nThe chain rule\nNo rule needed, it’s so basic.\n\n\n\n\n\nPart ix Which of the derivative rules should you use to find \\[\\partial_x\\ 15 x^2 - 3 x + 7 \\ln(x)\\ ?\\]\n\nThe constant multiplier rule\nThe linear combination rule\nThe product rule\nThe chain rule\nNo rule needed, it’s so basic.\n\n\n\n\n\nPart x What is \\(\\partial_x\\ 15 x^2 - 3 x + 7 \\ln(x)\\)?\n\\(30 x - 3 - 7/x\\)\\(30 x - 3 + 7/x\\)\\(15 x - 3 + 7/x\\)\\(30 x - 3x + 7/x\\)\n\n\n\n\nPart xi What is \\(\\partial_t e^k + \\ln(e^2) - t\\)?\n\n\\(k e^{k} + 2 / e - t\\)\n0\n-1\n\\(e^{k} + 1/e\\)\n\n\n\n\n\nPart xii What is \\(\\partial_{x} \\ln(x)/x^2\\)? (Hint: You can write the function in a simpler way.)\n\n\\(-2 x^{-3} \\left(1/x - 1\\right)\\)\n\\(-2 x^{-3} \\ln(x)\\)\n\\(-2 x^{-1} \\ln(x)\\)\n\\(x^{-3} \\left(1 - 2 \\ln(x)\\right)\\)\n\n\n\n\n\nPart xiii What is \\(\\partial_{t} \\left(4 \\sin(2\\pi t) - 5\\right)\\)?\n\n\\(8 \\cos(2 \\pi t)\\)\n\\(4 \\pi \\cos(2 \\pi t)\\)\n\\(4 \\cos(2\\pi t) - 5\\)\n\\(8 \\pi \\cos(2 \\pi t)\\)\n\n\n\n\n\nPart xiv What is \\(\\partial_{t} \\left(7 + 8 t^2 + 3 t^4\\right)\\)?\n\\(4 t + 12 t^2\\)\\(8 t + 4 t^3\\)\\(16 t + 12 t^3\\)\\(16 t^2 + 9 t^3\\)\n\n\n\n\nPart xv The derivative \\(\\partial_x \\text{dnorm}(x) = - x\\, \\text{dnorm}(x)\\). What is \\[\\partial_x \\text{dnorm}\\left(\\frac{x^2}{4}\\right)\\ ?\\]\n\n$ - ()$\n$ - ()$\n\\(- \\frac{x^3}{8} \\text{dnorm}\\left(\\frac{x^2}{4}\\right)\\)\n$ - ()$\n\n\n\n\n\nPart xvi What is \\(\\partial_{t} \\left(6 t - 3 t^2 + 2 t^4\\right)\\)?\n\\(6 - 3 t + 8 t^2\\)\\(6 - 3 t + 6 t^3\\)\\(6 - 6 t + 8 t^3\\)\\(-3 t + 6 t^3\\)\n\n\n\n\nPart xvii What is \\(\\partial_t \\ln(t^2 + 1)\\)?\n\\(2 t \\ln(t^2 + 1)\\)\\(1/{t^2 + 1}\\)\\(\\frac{2t}{t^2+1}\\)\\(1/2t\\)\n\n\n\n\nPart xviii For the function \\[g(t) \\equiv \\sin\\left(\\frac{2 \\pi}{P} (t - t_0)\\right)\\] is the interior function linear?\nYesNo\n\n\n\n\nPart xix For the function \\[g(P) \\equiv \\sin\\left(\\frac{2 \\pi}{P} (t - t_0)\\right)\\] is the interior function linear?\nYesNo\n\n\n\n\nPart xx For the function \\[h(u) \\equiv \\ln(a^2 u - \\sqrt{b})\\] is the interior function linear?\nYesNo\n\n\n\n\nPart xxi For the function \\(f(w) \\equiv e^{kw}\\), is the interior function linear?\nYesNo\n\n\n\n\nPart xxii Saying “the interior function is linear” is not an entirely complete statement. A full statement is “the interior function is linear in terms of the input \\(x\\)” or “in terms of the input \\(u\\)” or whatever name we choose to use for the input.  Is the expression \\(V x + U\\) linear in terms of \\(U\\)?\nYesNo\n\n\n\n\nPart xxiii Saying “the interior function is linear” is not an entirely complete statement. A full statement is “the interior function is linear in terms of the input \\(x\\)” or “in terms of the input \\(u\\)” or whatever name we choose to use for the input.  Is the expression \\(V x^2 + U\\) linear in terms of \\(U\\)?\nYesNo\n\n\n\n\nPart xxiv Saying “the interior function is linear” is not an entirely complete statement. A full statement is “the interior function is linear in terms of the input \\(x\\)” or “in terms of the input \\(u\\)” or whatever name we choose to use for the input.  Is the expression \\(V x^2 + U\\) linear in terms of \\(X\\)?\nYesNo"
  },
  {
    "objectID": "Differentiation/23-optim.html",
    "href": "Differentiation/23-optim.html",
    "title": "24  Optimization",
    "section": "",
    "text": "To “optimize” means to make something as good as possible with the available resources. Optimization problems are common in science, logistics, industry, and any other area where one seeks the best solution to a problem. Some everyday examples:"
  },
  {
    "objectID": "Differentiation/23-optim.html#structure-of-the-problem",
    "href": "Differentiation/23-optim.html#structure-of-the-problem",
    "title": "24  Optimization",
    "section": "24.1 Structure of the problem",
    "text": "24.1 Structure of the problem\nIn an optimization problem, there is one or more input quantities whose value you have to choose. The amount of salt; the years to wait from planting to harvesting a tree; the angle of the trail with respect to the slope. We’ll call this the decision quantity.\nSimilarly, there is one or more output quantity that you value and want to make as good as possible. The taste of the stew; the amount of usable wood harvested; the time it takes to walk up the hill. The output quantities are called the objectives.\nIn this chapter, we will deal with optimization problems that involve only a single objective. Problems with multiple objectives are among the most interesting and important in real-world decision making. Single-objective optimization techniques are a component of the more complex decision making, but they are a good place to get started.\nThe model that relates inputs to the objective output is called the objective function. Solving an optimization problem—once the modeling phase is complete—amounts to finding a value for the decision quantity (the input to the objective function) that produces the best output from the objective function.\nSometimes the objective is something that you want to minimize, make as small as possible. In the hiking trail problem, we seek to minimize the amount of time it takes to walk up the trail. Sometimes you want to maximize the objective, as in the wood-harvest problem where the objective is to harvest the most wood per year.\n\n\nMathematically, maximization and minimization are the same thing. Every minimization problem can be turned into a maximization problem by putting a negative sign in front of the objective function. To simplify the discussion, in talking about finding the solution to an optimization problem we’ll imagine that the goal is to maximize. But keep in mind that many circumstances in the real world, “best” can mean minimization.\nRecall from Section 6.7 that there are two components to the task of maximization or minimization. The argmax is the input to the objective function which produces the largest output. The maximum is the value of that output.1 Argmin and minimum are the words used in a situation where you seek the smallest value of the objective function.\nOnce you have found the argmax you can plug that value into the objective function to find the value of the output. That value is the maximum.\n\nPeople often talk about “finding the maximum.” This is misleading. The setup for an optimization problem is:\n\nConstruct (that is, model) the objective function.\nNow that you know the objective function, find the input to that function—that is, the argmax—that produces the maximum output.\n\n\nTo illustrate the setup of an optimization problem, imagine yourself in the situation of a contest to see who can shoot a tennis ball the farthest into a field with a slingshot. During the contest, you will adjust the vertical angle of launch, place the ball into the slingshot’s cradle, pull back as far as possible, and let go. To win the contest, you need to optimize how you launch the ball.\nThe objective is to maximize the distance traveled by the ball. The objective function models the distance travelled as a function of the quantities you can control, for instance the vertical angle of launch or the amount by which you pull back the slingshot. For simplicity, we’ll imagine that the slingshot is pulled back by a standard amount, producing a velocity of the ball at release of \\(v_0\\). Since \\(v_0\\) is fixed, you’ll win or lose based on the angle of launch you choose.\nBefore you head out into the field to experiment, let’s do a bit of preparation for constructing the objective function. Using some principles of physics and mathematics (which you may not yet understand), we’ll model how far the ball will travel (horizontally) as a function of the angle of launch \\(\\theta\\) and the initial velocity \\(v_0\\).\nThe mathematics of such problems involves an area called differential equations, an important part of calculus which we’ll come to later in the course. Since you don’t have the tools yet, we’ll just state a simple model of how long the ball stays in the air. \\[\\text{duration}(v_0, \\theta) = 2 v_0 \\sin(\\theta)/g\\] \\(g\\) is the acceleration due to gravity, which is about \\(9.8 \\text{m}\\text{s}^{-2}\\), assuming that the contest is being held on Earth.\nThe horizontal distance travelled by the tennis ball will be \\[\\text{hdist}(v_0, \\theta) = \\cos(\\theta) v_0\\, \\text{duration}(v_0, \\theta) = 2 v_0^2 \\cos(\\theta)\\sin(\\theta) / g\\] Our objective function is hdist(), and we seek to find the argmax. The input \\(v_0\\) is (we have assumed) fixed, so the only decision quantity is the angle \\(\\theta\\).\nThe best choice of \\(\\theta\\) will make the quantity \\(\\cos(\\theta)\\sin(\\theta)\\) as large as possible. So in finding the argmax, we don’t need to be concerned with \\(v_0\\) or \\(g\\).\nFinding the argmax can be accomplished simply by plotting the function \\(\\cos(\\theta)\\sin(\\theta)\\). We’ll implement the function so that the input is in units of degrees.\n\n\n\n\n\n\nFigure 24.1: In the simple model of a tennis ball launched at an angle \\(\\theta\\) from the horizontal, the distance travelled is \\(2 v_0^2 / g\\) times \\(\\cos(\\theta)\\sin(\\theta)\\).\n\n\n\nYou can see that the maximum value is about 0.5 and that this occurs at an argmax \\(\\theta\\) that’s a little bit less than 50\\(^\\circ\\).\nZooming in on the \\(\\theta\\) axis let’s you find the argmax with more precision:\n\n\n\n\n\n\nFigure 24.2: Zooming in on the argmax of the objective function. It’s important to look at the scale of the vertical axis. Any value of \\(\\theta\\) between about 40 and 50 gives a very close approximation to the maximum.\n\n\n\nFrom the graph, especially the zoomed-in version, you can read off the argmax as \\(\\theta = 45^\\circ\\).\nFinding the argmax solves the problem. You may also want to present your solution by saying what the value of the output of hdist() is when the argmax is given as input. You can read off the graph that the maximum of \\(\\cos(\\theta)\\sin(\\theta)\\) is 0.5 at \\(\\theta = 45^\\circ\\), so overall the distance will be \\(v_0^2 / g\\)"
  },
  {
    "objectID": "Differentiation/23-optim.html#interpreting-the-argmax",
    "href": "Differentiation/23-optim.html#interpreting-the-argmax",
    "title": "24  Optimization",
    "section": "24.2 Interpreting the argmax",
    "text": "24.2 Interpreting the argmax\nThe graphical solution given to the slingshot problem is entirely satisfactory. Whether that solution will win the contest depends of course on whether the model we built for the objective function is correct. There are potentially important things we have left out, such as air resistence.\nSolving the optimization problem has prepared us to go out in the field and test the result. Perhaps we’ll find that the real-world optimum angle is somewhat steeper or shallower than \\(\\theta = 45^\\circ\\).\nBesides the argmax, another important quantity to read from the graph in Figure 24.1 is the precision of the argmax. In strict mathematical terms, the argmax for the tennis-ball problem is exactly 45 degrees at which point \\(\\cos(\\theta)\\sin(\\theta) = 0.5\\). Suppose, however, that the ball were launched at only 40 degrees. Five degrees difference is apparent to the eye, but the result will be essentially the same as for 45 degrees: \\(\\cos(\\theta)\\sin(\\theta) = 0.492\\). The same is true for a launch angle of 50 degrees. For both “sub-optimal” launch angles, the output is within 2 percent of the 45-degree result. It’s easy to imagine that a factor outside the scope of the simple model—the wind, for instance—could change the result by as much or more than 2 percent, so a practical report of the argmax should reasonable be “40 to 50 degrees” rather than “exactly 45 degrees.”\nContests are won or lost by margins of less than 1%, so you should not casually deviate from the argmax. On the other hand, \\(45^\\circ\\) is the argmax of the model. Reality may deviate from the model. For instance, suppose that air resistance or wind might have an effect of about 1% on the distance. Since the real-world function might deviate by as much as 1% of the model value, we shouldn’t expect the real-world argmax to be any closer to 45\\(^\\circ\\) than \\(\\pm 5^\\circ\\), since anywhere in that input domain generates an output that is within 1% of the maximum output for the model."
  },
  {
    "objectID": "Differentiation/23-optim.html#derivatives-and-optimization",
    "href": "Differentiation/23-optim.html#derivatives-and-optimization",
    "title": "24  Optimization",
    "section": "24.3 Derivatives and optimization",
    "text": "24.3 Derivatives and optimization\nWe’re now going to reframe the search for the argmax and it’s interpretation in terms of derivatives of the objective function with respect to the decision quantity (\\(\\theta\\) in the slingshot problem). For a function with one input, this will not be an improvement from the look-at-the-graph technique to find the argmax. A genuine reason to use derivatives is to set us up in the future to solve problems with more than one input, where it is hard to draw or interpret a graph. Also, describing functions in the language of derivatives can help us think more clearly about aspects of the problem, such as the precision of the argmax.\nWith a graph such as Figure 24.1, it’s easy to find the argmax; common sense carries the day. So it won’t be obvious at first why we are going to take the following approach:\nLet’s denote an argmax of the objective function \\(f(x)\\) by \\(x^\\star\\). Let’s look at the derivative \\(\\partial_x f(x)\\) in the neighborhood of \\(x^\\star\\). Referring to Figure 24.1, where \\(x^\\star = 45^\\circ\\), you may be able to see that \\(\\partial_x f(x^\\star)\\) is zero; the line tangent to the function’s graph at \\(x^\\star\\) is horizontal.\nSeen another way, the slope of \\(f(x)\\) to the left of \\(x^\\star\\) is positive. Move a tiny bit to the right (that is, increase \\(x\\) by a very small amount) leads to an increase in the output \\(f(x)\\). Just to the right of \\(x^\\star\\), the slope of \\(f(x)\\) is negative; as you reach the top of a hill and continue on, you will be going downhill. So the derivative function is positive on one side of \\(x^\\star\\) and negative on the other, suggesting that it crosses zero at the argmax.\nCommon sense is correct: Walk uphill to get to the peak, walk downhill to move away from the peak. When you come to the top of a smooth hill, the terrain is level. (Since our modeling functions are smooth, so must be the hills that we visualize the functions with.)\nInputs \\(x^\\star\\) such that \\(\\partial_x f(x^\\star) = 0\\) are called critical points. Why not call them simply argmaxes? Because a the slope will also be zero at an argmin. And it’s even possible to have the slope be zero at a point that’s neither an argmin or an argmax.\nAt this point, we know that values \\(x^\\star\\) that give \\(\\partial_x f(x^\\star) = 0\\) are “critical points,” but we haven’t said how to figure out whether a given critical point is an argmax, an argmin, or neither. This is where the behavior of \\(\\partial_x f(x)\\) near \\(x=x^\\star\\) is important. If \\(x^\\star\\) is an argmax, then \\(\\partial_x f(x)\\) will be positive to the left of \\(x^\\star\\) and negative to the right of \\(x^\\star\\); walk up the hill to get to \\(x^\\star\\), at the top the hill is flat, and just past the top the hill has a negative slope.\nFor an argmin, changing \\(x\\) from less than \\(x^\\star\\) to greater than \\(x^\\star\\); you will be walking down into the valley, then level at the very bottom \\(x=x^\\star\\), then back up the other side of the valley after you pass \\(x=x^\\star\\). Figure 24.3 shows the situation.\n\n\n\n\n\nFigure 24.3: Top row: An objective function near an argmax (left) and an argmin (right). Bottom row: The derivative of the objective function. A horizontal line (orange) has been added to mark zero on the vertical axis.\n\n\n\n\nThe bottom row of graphs in Figure 24.3 shows the derivative of the objective function \\(f(x)\\), that is, \\(\\partial_x f(x)\\). You can see that for the argmax of \\(f(x)\\), the derivative \\(\\partial_x f(x)\\) is positive to the left and negative to the right. Similarly, near the argmin of \\(f(x)\\), the derivative \\(\\partial_x f(x)\\) is negative to the left and positive to the right.\nStated another way, the derivative \\(\\partial_x f(x)\\) has a negative slope just to the left of an argmin and a positive slope to the left of an argmax.\nThe second derivative of the objective function \\(f(x)\\) at a critical point \\(x^\\star\\) is what tells us whether the critical point is an argmax, an argmin, or neither.\n\n\n\n\n\n\n\n\n\n\nCritical point \\(x^\\star\\)\n\\(\\partial_x f(x^\\star)\\)\n\\(\\partial_{xx} f(x^\\star)\\)\n\n\n\n\nargmax\n0\nnegative\n\n\nargmin\n0\npositive\n\n\nneither\n0\n0\n\n\n\n\nThroughout Block 2, we have translated features of functions that are evident on a graph into the language of derivatives:\n\nThe slope of a function \\(f(x)\\) at any input \\(x\\) is the value of the derivative function \\(\\partial_x f(x)\\) at that same \\(x\\).\nThe concavity of a function \\(f(x)\\) at any input is the slope of the derivative function, that is, \\(\\partial_{xx} f(x)\\).\nPutting (i) and (ii) together, we get that the concavity of a function \\(f(x)\\) at any input \\(x\\) is the value of the second derivative function, that is, \\(\\partial_{xx} f(x)\\).\nAt an argmax \\(x^\\star\\) of \\(f(x)\\), the value of the derivative function \\(\\partial_x f(x^\\star)\\) is zero and the value of the second derivative function \\(\\partial_{xx} f(x^\\star)\\) is negative. The situation at an argmin is along the same lines, the derivative of the objective function is zero and the second derivative is positive.\n\n\n\nWhat’s the critical point?\nYou’re familiar with the quadratic polynomial: \\[g(x) = a_0 + a_1 x + a_2 x^2\\] The graph of a quadratic polynomial is a parabola, which might be concave up or concave down. As you know, a parabola has only one critical point, which might be an argmin or an argmax.\nLet’s find the critical point. We know that the critical point is \\(x^\\star\\) such that \\(\\partial_x g(x^\\star) = 0\\). Since we know how to differentiate a power law, we can see that \\[\\partial_x g(x) = a_1 + 2 a_2 x\\] and, more specifically, at the critical point \\(x^\\star\\) the derivative will be \\[a_1 + 2 a_2 x^\\star = 0\\] The above is an equation, not a definition. It says that whatever \\(x^\\star\\) happens to be, the quantity \\(a_1 + 2 a_2 x^\\star\\) must be zero. Using plain old algebra, we can find the location of the critical point \\[x^\\star = -\\frac{a_1}{2 a_2}\\]\n\n\n\n\nIn economics, a monopoly or similar arrangement can set the price for a good or commodity. Monopolists can set the price at a level that generates the most income for themselves.\n\n\n\n\n\nFigure 24.4: Demand as a function of price, as first published by Antoine-Augustin Cournot in 1836. Source)\n\n\n\n\nIn 1836, early economist Antoine-Augustin Cournot published a theory of revenue versus demand based on his conception that demand will be a monotonically decreasing function of price. (That is, higher price means lower demand.) We’ll write as \\(\\text{Demand}(p)\\) demand as a function of price.\nThe revenue generated at price \\(p\\) is \\(R(p) \\equiv p \\text{Demand}(p)\\): price times demand.\nTo find the revenue-maximizing demand, differentiate \\(R(p)\\) with respect to \\(p\\) and find the argmax \\(p^\\star\\) at with \\(\\partial_p R(p^\\star) = 0).\\) This can be done with the product rule.\n\\[\\partial_p R(p) = p \\ \\partial_p \\text{Demand}(p) + \\text{Demand}(p)\\] At the argmax \\(p^\\star\\) we have: \\[p^\\star \\partial_p \\text{Demand}(p^\\star) + \\text{Demand}(p^\\star) = 0 \\ \\ \\stackrel{\\text{solving for}\\ p^\\star}{\\Longrightarrow} \\ \\ p^\\star = - \\frac{\\text{Demand}(p^\\star)}{\\partial_p \\text{Demand}(p^\\star)}\\]\nIf the monopolist knows the demand function \\(D(p)\\), finding the revenue maximizing price is a simple matter. But in general, the monopolist does not know the demand function in advance. Instead, an informed guess is made to set the initial price \\(p_0\\). Measuring sales \\(D(p_0)\\) gives one point on the demand curve. Then, try another price \\(p_1\\). This gives another point on the demand curve as well as an estimate \\[\\partial_p D(p_0) = \\frac{D(p_1) - D(p_0)}{p_1 - p_0}\\] Now the monopolist is set to model the demand curve as a straight-line function and easily to find \\(p^\\star\\) for the model. For instance, if the demand function is modeled as \\(D_1 (p) = a + b p\\), the optimal price will be \\(p^\\star_1 = - \\frac{a + b p^\\star}{b}\\) which can be solved as \\(p^\\star_1 = - a/2b\\).\n\\(p^\\star_1\\) is just an estimate of the optimum price. Still, the monopolist can try out that price, giving a third data point for the demand function from which a better model of the demand function can be constructed. With the better estimate, find a new a argmax \\(p^\\star_2\\). This sort of iterative process for finding an argmax of a real-world function is very common in practice."
  },
  {
    "objectID": "Differentiation/23-optim.html#sec-flat-on-top",
    "href": "Differentiation/23-optim.html#sec-flat-on-top",
    "title": "24  Optimization",
    "section": "24.4 Be practical!",
    "text": "24.4 Be practical!\nDecision making is about choosing among alternatives. In some engineering or policy contexts, this can mean finding a value for an input that will produce the “best” outcome. For those who have studied calculus, it’s natural to believe that calculus-based techniques for optimization are the route to making the decision.\nWe emphasize that the optimization techniques covered in this chapter are only part of a broader set of techniques for real-world decision-making problems. In particular, most policy contexts involve multiple objectives. For example, in designing a car one goal is to make it cheap to manufacture, another to make it attractive, and still another to make it safe. These different objectives are often at odds with one another. In Block 4 of this text, we’ll discuss some calculus techniques that help policy-makers in multi-objective settings.\nFor now, sticking with the idealized (and often unrealistic) setting of maximizing a single objective, with one or more inputs. Recall the setting for calculus-type maximization. You have a function with one or more inputs, say, \\(f(x)\\) or \\(g(x,y)\\) or, often, \\(h(x, y, z, \\ldots)\\) where \\(\\ldots\\) might be standing for tens or hundreds or thousands of inputs or more.\nIf you can graph the function (feasible for one- or two-input functions), you can often easily scan the graph by eye to find the peak. The calculus-based techniques were developed for situations where such graphing is not possible and, instead, you have a formula for the function. (Such occasions are of great theoretical interest but not all that common in practice.) The basis of the calculus techniques is the observation that, at the argmax of a smooth function, the derivative of the function is 0.\nAs an example, consider a style problem that often appears in calculus textbooks. You have been tasked to design a container for a large volume V of liquid. It is desired to make the weight of the container as little as possible. (This is a minimization problem, then.) In classical textbook fashion, you are told that the container is to be a cylinder made out of a particular metal of a particular thickness.\nThis is a lovely geometry/calculus problem. Whether it is relevant to any genuine, real-world problem is another question.\n\n\n\n\n\n\n\n\n\nUsing the notation in the diagram, the volume and surface area of the cylinder is \\[V(r, h) \\equiv \\pi r^2 h \\ \\ \\ \\text{and}\\ \\ \\ A(r, h) \\equiv 2 \\pi r^2 + 2 \\pi r h\\]\nMinimizing the weight of the cylinder is our objective (according to the problem statement) and the weight is proportional to the surface area. Since the volume \\(V\\) is given (according to the problem statement), we want to re-write the area function to use volume:\n\\[h(r, V) \\equiv V / \\pi r^2 \\ \\ \\ \\implies\\ \\ \\ A(r, V) = 2 \\pi r^2 + 2 \\pi r V/\\pi r^2 = 2 \\pi r^2 + 2 V / r\\] Suppose \\(V\\) were specified as 1000 liters. A good first step is to choose appropriate units for \\(r\\) to make sure the formula for \\(A(r, V)\\) is dimensionally consistent. Suppose we choose \\(r\\) in cm. Then we want \\(V\\) in cubic centimeters (cc). 1000 liters is 1,000,000 cc. Now we can plot a slice of the area function:\n\nA <- makeFun(2*pi*r^2 + 2*V/r ~ r, V=1000000)\nslice_plot(A(r) ~ r, domain(r=c(10, 100))) %>%\n  gf_labs(x = \"radius (cm)\", y = \"Surface area of container (square cm)\")\n\n\n\n\n\n\n\n\nAs always, the function’s derivative is zero at the optimal \\(r\\). In the graph, the argmin is near \\(r=50\\) cm at which point the minimum is about 50,000 cm\\(^2\\). Since \\(h(r,V) = V/\\pi r^2\\), the required height of cylinder will be near \\(10^6 / \\pi 50^2 = 127\\)cm.\nIn calculus courses, the goal is often to find a formula for the optimal radius as a function of \\(V\\). So we differentiate the objective function—that is, the area function for any \\(V\\) and \\(r\\) with respect to \\(r\\), \\[\\partial_r A(r, V) = 4 \\pi r - 2 V / r^2\\] Setting this to zero (which will be true at the optimal \\(r^\\star\\)) we can solve for \\(r^\\star\\) in terms of \\(V\\): \\[4 \\pi r^\\star - 2 \\frac{V}{\\left[r^\\star\\right]^2} = 0 \\ \\ \\ \\Longrightarrow\\ \\ \\ 4\\pi r^\\star = 2\\frac{V}{\\left[r^\\star\\right]^2} \\Longrightarrow\\ \\ \\ \\left[r^\\star\\right]^3 = \\frac{1}{2\\pi} V \\ \\ \\ \\Longrightarrow\\ \\ \\  r^\\star = \\sqrt[3]{V/2\\pi}\\]\nFor \\(V = 1,000,000\\) cm\\(^3\\), this gives \\(r^\\star = 54.1926\\) cm which in turn implies that the corresponding height \\(h^\\star = V/\\pi (r^\\star)^2 = 108.3852\\) cm.\nWe’ve presented the optimum \\(r^\\star\\) and \\(h^\\star\\) to the nearest micron. (Does that make sense to you? Think about it for a moment before reading on.)\nA good rule of thumb in modeling is this: “If you don’t know what a sensible precision is for reporting your result, you don’t have a complete grasp of the problem.” Here are two reasonable ways to sort out a suitable precision.\n\nSolve a closely related problem which for many practical purposes would have been equivalent.\nLook at how big a change in the output of the objective function is produced by a change from the argmax.\n\nApproach (2) is always at hand, since you already know the objective function. Let’s graph the objective function near \\(r = 54.1926\\) …\n\n\n\n\n\n\n\n\n\nLook carefully at the axes scales. Deviating from the mathematical optimum by about 5cm (that is, 50,000 microns) produces a change in the output of the objective function by about 400 units out of 55,000. In other words, about 0.7%.\nIt’s true that \\(r^\\star = 54.1926\\) cm gives the “best” outcome. And sometimes such precision is warranted. For example, improving the speed of an elite marathon racer by even 0.1% would give her a 7 second advantage: often the difference between silver and gold!\nWhat’s different is that you know exactly what is the ultimate objective of a marathon: finish faster. But you may not know the ultimate objective of the system your “optimal” tank will be a part of. For instance, your tank may be part of an external fuel pod on an aircraft. Certainly the designers of the aircraft want the tank to be as light as possible. But they also want to reduce drag as much as possible. A 54 cm diameter tube has about 17% more drag than a 50 cm tube. To save that much drag, it’s probably well worth increasing weight by 0.7%.\nIn reporting the results from an optimization problem, you ought to give the decision maker all relevant information. Here, that might be as simple as including the above graph in your report.\nWe mentioned another technique for getting a handle on what precision is meaningful: (1) solve a closely related problem. This often requires some insight and creativity to frame the new problem. Here, we note that large capacity tanks often are shaped like a lozenge: a cylinder with hemi-spherical ends.\n\n\n\n\n\n\n\n\n\nUsing \\(h\\) for the length of the cylindrical portion of the tank, and \\(r\\) for the radius, the volume and surface area are: \\[V(r, h) = \\pi r^2 h + \\frac{4}{3} \\pi r^3 \\ \\ \\ \\text{and}\\ \\ \\ A(r,h) = 2 \\pi r h + 4 \\pi r^2\\] Again, \\(V\\) was specified as 1000 liters. As detailed in Exercise 23.18, the surface area of this 1000-liter tank is about 48,400 cm\\(^2\\). This is more than 10% less than for the cylindrical tank."
  },
  {
    "objectID": "Differentiation/23-optim.html#exercises",
    "href": "Differentiation/23-optim.html#exercises",
    "title": "24  Optimization",
    "section": "24.5 Exercises",
    "text": "24.5 Exercises"
  },
  {
    "objectID": "Differentiation/24-partial.html",
    "href": "Differentiation/24-partial.html",
    "title": "25  Partial change and the gradient vector",
    "section": "",
    "text": "This is a good time to point out something we have been doing all along, but which has likely been such a persistent component of your mathematics education that you may not have realized that it is a construction.\nWe have two ways by which we represent functions:\nThese two modes are sometimes intertwined, as when we use the name “line” to refer to a computational object: \\(\\line(x) \\equiv a x + b\\).\nUnfortunately for functions of two inputs, a surface is hard to present in the formats that are most easily at hand: a piece of paper, a printed page, a computer screen. That’s because a curved surface is naturally a 3-dimensional object, while paper and screens provide two-dimensional images. Consequently, the graphics mode we prefer for presenting functions of two inputs is the contour plot, which is not a single geometrical object but a set of many objects: contours, labels, colored tiles.\nWe’ve been doing calculus on functions with one input because it is so easy to exploit both the computational mode and the graphical mode. And it might fairly be taken as a basic organizing theme of calculus that\nWhen figuring out the derivative function \\(\\partial_x f(x)\\) from a graph of \\(f(x)\\), we find the tangent to the graph at each of many input values, record the slope of the line (and throw away the intercept) and then write down the series of slopes as a function of the input, typically by representing the slope by position along the vertical axis and the corresponding input by position along the horizontal axis. Figure 25.1 shows the process.\nPanel (A) in Figure 25.1 shows a smooth function \\(f(x)\\) (thin black curve). To find the function \\(\\partial_x f(x)\\), we take the slope of \\(f(x)\\) at many closely spaced inputs. In Panel (A), we’ve highlighted short, tangent line segments at the closely-spaced points labeled A through V. The slope of each tangent line segment can be calculated by the usual rise-over-run method; the numerical value of the slope is written underneath the segment. To plot the derivative \\(\\partial_x f(x)\\), I have taken the slope information from (A) and plotted it as a function of \\(x\\).\nTo restate what you already know, in the neighborhood of any input value \\(x\\), the slope of any local straight-line approximation to \\(f(x)\\) is given by the value of of \\(\\partial_x f(x)\\)."
  },
  {
    "objectID": "Differentiation/24-partial.html#calculus-on-two-inputs",
    "href": "Differentiation/24-partial.html#calculus-on-two-inputs",
    "title": "25  Partial change and the gradient vector",
    "section": "25.1 Calculus on two inputs",
    "text": "25.1 Calculus on two inputs\nAlthough we use contour plots for good practical reasons, the graph of a function \\(g(x,y)\\) with two inputs is a surface, as described in Section @ref(surface-plot). The derivative of \\(g(x,y)\\) should encode the information needed to approximate the surface at any input \\((x,y)\\). In particular, we want the derivative of \\(g(x,y)\\) to tell us the orientation of the tangent plane to the surface.\nA tangent plane is infinite in extent. Let’s use the word facet to refer to a little patch of the tangent plane centered at the point of contact. Each facet is flat. (It’s part of a plane!) Figure 25.2 shows some facets tangent to a familiar curved surface. No two of the facets are oriented the same way.\n\n\n\n\n\n\nFigure 25.2: A melon as a model of a curved surface such as the graph of a function of two inputs. Each tangent facet has its own orientation. (Disregard the slight curvature of the small pieces of paper. Summer humidity has interfered with my attempt to model a flat facet with a piece of Post-It paper!\n\n\n\nBetter than a picture of a summer melon, pick up a hardcover book and place it on a curved surface such as a basketball. The book cover is a flat surface: a facet. The orientation of the cover will match the orientation of the surface at the point of tangency. Change the orientation of the cover and you will find that the point of tangency will change correspondingly.\nIf melons and basketballs are not your style, you can play the same game on an interactive graph of a function with two inputs. The snapshot below is a link to an applet that shows the graph of a function as a blue surface. You can specify a point on the surface by setting the value of the (x, y) input using the sliders. Display the tangent plane (which will be green) at that point by check-marking the “Tangent plane” input. (Acknowledgments to Alfredo Sánchez Alberca who wrote the applet using the GeoGebra math visualization system.)\n::: {asis eval=knitr::is_html_output()}  :::\n\n\n\nFor the purposes of computation by eye, a contour graph of a surface can be easier to deal with. Figure 25.3 shows the contour graph of a smoothly varying function. Three points have been labeled A, B, and C.\n\n\n\n\n\n\n\n\nFigure 25.3: A function of 2 inputs with 3 specific inputs marked A, B, and C\n\n\n\n\nZooming in on each of the marked points presents a simpler picture for each of them, although one that is different for each point. Each zoomed-in plot contains almost parallel, almost evenly spaced contours. If the surface had been exactly planar over the entire zoomed-in domain, the contours would be exactly parallel and exactly evenly spaced. We can approach such exact parallelness by zooming in more closely around the labeled point.\n\n\n\n\n\nFigure 25.4: Zooming in on the neighborhoods of A, B, and C in Figure 25.3 shows a simple, almost planar, local landscape. The bottom row shows the contours of the tangent plane near each of the neighborhoos in the top row.\n\n\n\n\nJust as the function \\(\\line(x) \\equiv a x + b\\) describes a straight line, the function \\(\\text{plane}(x, y) \\equiv a + b x + c y\\) describes a plane whose orientation is specified by the value of the parameters \\(b\\) and \\(c\\). (Parameter \\(a\\) is about the vertical location of the plane, not it’s orientation.)\nIn the bottom row of Figure 25.4, the facets tangent to the original surface at A, B, and C are displayed. Comparing the top and bottom rows of Figure 25.4) you can see that each facet has the same orientation as the surface; the contours face in the same way.\nRemember that the point of constructing such facets is to generalize the idea of a derivative from a function of one input \\(f(x)\\) to functions of two or more inputs such as \\(g(x,y)\\). Just as the derivative \\(\\partial_x f(x_0)\\) reflects the slope of the line tangent to the graph of \\(f(x)\\) at \\(x=x_0\\), our plan for the “derivative” of \\(g(x_0,y_0)\\) is to represent the orientation of the facet tangent to the graph of \\(g(x,y)\\) at \\((x=x_0, y=y_0)\\). The question for us now is what information is needed to specify an orientation.\nOne clue comes from the formula for a function whose graph is a plane oriented in a particular direction:\n\\[\\text{plane}(x,y) \\equiv a + b x + cy\\]\n\nTo explore the roles of the parameters \\(b\\) and \\(c\\) in setting the orientation of the line, open a SANDBOX. The scaffolding code generates a particular instance of \\(\\text{plane}(x,y)\\) and plots it in two ways: a contour plot and a surface plot. Change the numerical values of \\(b\\) and \\(c\\) and observe how the orientation of the planar surface changes in the graphs. You can also see that the value of \\(a\\) is irrelevant to the orientation of the plane, just as the intercept of a straight-line graph is irrelevant to the slope of that line.\n\nplane <- makeFun(a + b*x + c*y ~ x + y, a = 1, b = -2.5, c = 1.6)\nif (knitr::is_html_output()) {\n  interactive_plot(plane(x, y) ~ x + y, domain(x=c(-2, 2), y=c(-2, 2)))\n} else {\n  knitr::include_graphics(\"www/plane-3d.png\")\n}\n\n\n\n\ncontour_plot(plane(x, y) ~ x + y, domain(x=c(-2, 2), y=c(-2, 2))) %>%\n  gf_refine(coord_fixed())\n\n\n\n\n\n\n\n\nAs always it can be difficult to extract quantitative information from a surface plot. For the example here, you can see that the high-point on the surface is when \\(x\\) is most negative and \\(y\\) is most positive. Compare that to the contour plot to verify that two modes are displaying the same surface.\n(Note: The gf_refine(coord_fixed()) part of the contour-plot command makes numerical intervals on the horizontal and vertical axes have the same length.)\n\nAn instructive experience is to pick up a rigid, flat object, for instance a smartphone or hardcover book. Hold the object level with pinched fingers at the mid-point of each of the short ends, as shown in ?fig-hold-book (left).\n\n\n\n\n\nFigure 25.5: Combining two simple movements can tip a plane to all sorts of different orientations.\n\n\n\n\n\n\n\nFigure 25.6: Combining two simple movements can tip a plane to all sorts of different orientations.\n\n\n\n\n\n\n\nFigure 25.7: Combining two simple movements can tip a plane to all sorts of different orientations.\n\n\n\n\nYou can tip the object in one direction by raising or lowering one hand. (middle picture) And you can tip the object in the other coordinate direction by rotating the object around the line joining the points grasped by the left and right hands. (right picture) By combining these two motions, you can orient the surface of the object in a wide range of directions.1\nThe purpose of this lesson is to show that two-numbers are sufficient to dictate the orientation of a plane. In terms of ?fig-hold-book these are 1) the amount that one hand is raised relative to the other and 2) the angle of rotation around the hand-to-hand axis.\nSimilarly, in the formula for a plane, the orientation is set by two numbers, \\(b\\) and \\(c\\) in \\(\\text{plane}(x, y) \\equiv a + b x + c y\\).\nHow do we find the right \\(b\\) and \\(c\\) for the tangent facet to a function \\(g(x,y)\\) at a specific input \\((x_0, y_0)\\)? Taking slices of \\(g(x,y)\\) provides the answer. In particular, these two slices: \\[\\text{slice}_1(x) \\equiv g(x, y_0) = a + b\\, x + c\\, y_0 \\\\ \\text{slice}_2(y) \\equiv g(x_0, y) = a + b x_0 + c\\, y\\]\nLook carefully at the formulas for the slices. In \\(\\text{slice}_1(x)\\), the value of \\(y\\) is being held constant at \\(y=y_0\\). Similarly, in \\(\\text{slice}_2(y)\\) the value of \\(x\\) is held constant at \\(x=x_0\\).\nThe parameters \\(b\\) and \\(c\\) can be read out from the derivatives of the respective slices: \\(b\\) is equal to the derivative of the slice\\(_1\\) function with respect to \\(x\\) evaluated at \\(x=x_0\\), while \\(c\\) is the derivative of the slice\\(_2\\) function with respect to \\(y\\) evaluated at \\(y=y_0\\). Or, in the more compact mathematical notation:\n\\[b = \\partial_x \\text{slice}_1(x)\\left.\\strut\\right|_{x=x_0} \\ \\ \\text{and}\\ \\ c=\\partial_y \\text{slice}_2(y)\\left.\\strut\\right|_{y=y_0}\\] These derivatives of slice functions are called partial derivatives. The word “partial” refers to examining just one input at a time. In the above formulas, the \\({\\large |}_{x=x_0}\\) means to evaluate the derivative at \\(x=x_0\\) and \\({\\large |}_{y=y_0}\\) means something similar.\nYou don’t need to create the slices explicitly in order to calculate the partial derivatives. Simply differentiate \\(g(x, y)\\) with respect to \\(x\\) in order to get parameter \\(b\\) and differentiate \\(g(x, y)\\) with respect to \\(y\\) to get parameter \\(c\\). To demonstrate, we’ll make use of the sum rule: \\[\\partial_x g(x, y) = \\underbrace{\\partial_x a}_{=0} + \\underbrace{\\partial_x b x}_{=b} + \\underbrace{\\partial_x cy}_{=0} = b\\] Similarly, \\[\\partial_y g(x, y) = \\underbrace{\\partial_y a}_{=0} + \\underbrace{\\partial_y b x}_{=0} + \\underbrace{\\partial_y cy}_{=c} = c\\]\n\nGet in the habit of noticing the subscript on the differentiation symbol \\(\\partial\\). When taking, for instance, \\(\\partial_y f(x,y,z, \\ldots)\\), all inputs other than \\(y\\) are to be held constant. Some examples:\n\\[\\partial_y 3 x^2 = 0\\ \\ \\text{but}\\ \\ \\\n\\partial_x 3 x^2 = 6x\\\\\n\\ \\\\\n\\partial_y 2 x^2 y = 2x^2\\ \\ \\text{but}\\ \\ \\\n\\partial_x 2 x^2 y = 4 x y\n\\]"
  },
  {
    "objectID": "Differentiation/24-partial.html#all-other-things-being-equal",
    "href": "Differentiation/24-partial.html#all-other-things-being-equal",
    "title": "25  Partial change and the gradient vector",
    "section": "25.2 All other things being equal …",
    "text": "25.2 All other things being equal …\nRecall that the derivative of a function with one input, say, \\(\\partial_x f(x)\\) tells you, at each possible value of the input \\(x\\), how much the output will change proportional to a small change in the value of the input.\nNow that we are in the domain of multiple inputs, writing \\(h\\) to stand for “a small change” is not entirely adequate. Instead, we’ll write \\(dx\\) for a small change in the \\(x\\) input and \\(dy\\) for a small change in the \\(y\\) input.\nWith this notation, we write the first-order polynomial approximation to a function of a single input \\(x\\) as \\[f(x+dx) = f(x) + \\partial_x f(x) \\times dx\\] Applying this notation to functions of two inputs, we have: \\[g(x + \\color{magenta}{dx}, y) = g(x,y) + \\color{magenta}{\\partial_x} g(x,y) \\times \\color{magenta}{dx}\\] and \\[g(x, y+\\color{brown}{dy}) = g(x,y) + \\color{brown}{\\partial_y} g(x,y) \\times \\color{brown}{dy}\\]\nEach of these statements is about changing one input while holding the other input(s) constant. Or, as the more familiar expression goes, “The effect of changing one input all other things being equal or all other things held constant.2\nEverything we’ve said about differentiation rules applies not just to functions of one input, \\(f(x)\\), but to functions with two or more inputs, \\(g(x,y)\\), \\(h(x,y,z)\\) and so on."
  },
  {
    "objectID": "Differentiation/24-partial.html#gradient-vector",
    "href": "Differentiation/24-partial.html#gradient-vector",
    "title": "25  Partial change and the gradient vector",
    "section": "25.3 Gradient vector",
    "text": "25.3 Gradient vector\nFor functions of two inputs, there are two partial derivatives. For functions of three inputs, there are three partial derivatives. We can, of course, collect the partial derivatives into Cartesian coordinate form. This collection is called the gradient vector.\nJust as our notation for differences (\\(\\cal D\\)) and derivatives (\\(\\partial\\)) involves unusual typography on the letter “D,” the notation for the gradient involves such unusual typography although this time on \\(\\Delta\\), the Greek version of “D.” For the gradient symbol, turn \\(\\Delta\\) on its head: \\(\\nabla\\). That is, \\[\\nabla g(x,y) \\equiv \\left(\\stackrel\\strut\\strut\\partial_x g(x,y), \\ \\ \\partial_y g(x,y)\\right)\\]\nNote that \\(\\nabla g(x,y)\\) is a function of both \\(x\\) and \\(y\\), so in general the gradient vector differs from place to place in the function’s domain.\nThe graphics convention for drawing a gradient vector for a particular input, that is, \\(\\nabla g(x_0, y_0)\\), puts an arrow with its root at \\((x_0, y_0)\\), pointing in direction \\(\\nabla g(x_0, y_0)\\), as in Figure 25.8.\n\n## Warning in makeFun.formula(g(x, y) ~ x, suppress.warnings = FALSE): Implicit\n## variables without default values (dangerous!): y\n\n## Warning in makeFun.formula(g(x, y) ~ x, suppress.warnings = FALSE): Implicit\n## variables without default values (dangerous!): y\n## Warning in makeFun.formula(g(x, y) ~ y, suppress.warnings = FALSE): Implicit\n## variables without default values (dangerous!): x\n\n## Warning in makeFun.formula(g(x, y) ~ y, suppress.warnings = FALSE): Implicit\n## variables without default values (dangerous!): x\n\n\n\n\nFigure 25.8: The gradient vector \\(\\nabla g(x=1,y=2)\\). The vector points in the steepest uphill direction. Consequently, it is perpendicular to the contour passing through its root.\n\n\n\n\nA gradient field (see Figure 25.9) is the value of the gradient vector at each point in the function’s domain. Graphically, in order to prevent over-crowding, the vectors are drawn at discrete points. The lengths of the drawn vectors are set proportional to the numerical length of \\(\\nabla g(x, y)\\), so a short vector means the surface is relatively level, a long vector means the surface is relatively steep.\n\n\n\n\n\nFigure 25.9: A plot of the gradient field \\(\\nabla g(x,y)\\)."
  },
  {
    "objectID": "Differentiation/24-partial.html#total-derivative-optional",
    "href": "Differentiation/24-partial.html#total-derivative-optional",
    "title": "25  Partial change and the gradient vector",
    "section": "25.4 Total derivative (optional)",
    "text": "25.4 Total derivative (optional)\nThe name “partial derivative” suggests the existence of some kind of derivative that’s not just a part, but the whole thing. The total derivative is such a whole and gratifyingly made up of it’s parts, that is, the partial derivatives.\nSuppose you are modeling the temperature of some volume of the atmosphere, given as \\(T(t, x, y, z)\\). This merely says that the temperature depends on both time and location, something that is familiar from everyday life.\nThe partial derivatives have an easy interpretation: \\(\\partial_t T()\\) tells how the temperature is changing over time at a given location, perhaps because of the evaporation or condensation of water vapor. \\(\\partial_x T()\\) tells how the temperature changes in the \\(x\\) direction, and so on.\nThe total derivative gives an overall picture of the changes in a parcel of air, which you can thnk of as a tiny balloon-like structure but without the balloon membrane. The temperature inside the “balloon” may change with time (e.g. condensation or evaporation of water), but as the ballon drifts along with the motion of the air (that is, the wind), the evolving location can change the temperature as well. Think of a balloon caught in an updraft: the temperature goes down as the balloon ascends.\nFor an imaginary observer located in the balloon, the temperature is changing with time. Part of this change is the instrinsic change measured by \\(\\partial_t T\\) but we need to add to that the changes induces by the evolving location of the balloon. The partial change in temperature due to a change in altitude is \\(\\partial_z T\\), but it’s important to realize that the coordinates of the location are themselves functions of time: \\(x(t), y(t), z(t)\\). Seeing the function \\(T()\\) for the observer in the balloon as a function of \\(t\\), we have \\(T(t, x(t), y(t), z(t))\\). This is a function composition: \\(T()\\) composed with each of \\(x()\\), \\(y()\\), and \\(z()\\). Recall in the chain rule \\(\\partial_v f(g(v)) = \\partial_v f(g(v)) \\partial_v g(v)\\) that the derivative of the composed quantity is the product of two derivatives.\nLikewise, the total derivative of temperature with respect to the observer riding in the balloon will be add together the parts due to changes in time (holding position constant), x-coordinate (holding time and the other space coordinates constant), and the like. Signifying the total differentiation with a capital \\(D\\), we have \\[D\\, T(t) = \\partial_t T() + \\partial_x T() \\cdot\\partial_t x + \\partial_y T()\\cdot \\partial_t y + \\partial_z T() \\cdot\\partial_t z\\] Note that \\(\\partial_t x\\) is the velocity of the balloon in the x-direction, and similarly for the other coordinate directions. Writing these velocities as \\(v_x, v_y, v_z\\), the total derivative for temperature of a parcel of air embedded in a moving atmosphere is\n\\[D\\ T(t) = \\partial_t T + v_x\\, \\partial_x T + v_y\\, \\partial_y T + v_z\\, \\partial_z T\\] Formulations like this, which put the parts of change together into a whole, are often seen in the mathematics of fluid flow as applied in meteorology and oceanology."
  },
  {
    "objectID": "Differentiation/24-partial.html#sec-differential-skier",
    "href": "Differentiation/24-partial.html#sec-differential-skier",
    "title": "25  Partial change and the gradient vector",
    "section": "25.5 Differentials",
    "text": "25.5 Differentials\n\nA little bit of this, a little bit of that. — Stevie Wonder, “The Game of Love”\n\nWe have framed calculus in terms of functions: transformations that take one (or more!) quantities as input and return a quantity as output. This was not the original formulation. In this section, we will use the original style in order to demonstrate how you can sometimes skip the step of constructing a function before differentiating to answer a question of the sort: “If this quantity changes by a little bit, how much will another, related quantity change?”\nAs an example, consider the textbook-style problem of a water skier being pulled along the water by a rope pulled in from the top of a tower of height \\(H\\). The skier is distance \\(x\\) from the tower. As the rope is winched in at a constant rate, does the skier go faster or slower as she approaches the tower.\n\n\n\n\n\n\n\n\n\nIn the function style of approach, we can write the position function \\(x(t)\\) with input the length of the rope \\(L(t)\\). Using the diagram, you can see that \\[x(t) = \\sqrt{\\strut L(t)^2 - H^2}\\ .\\]\nDifferentiate both sides with respect to \\(t\\) to get the velocity of the skier: \\(\\partial_t x(t)\\) through the chain rule: \\[\\underbrace{\\partial_t x(t)}_{\\partial_t f(g(t))} = \\underbrace{\\frac{1}{2\\sqrt{\\strut L(t)^2 - H^2}}}_{\\left[ \\partial_t f \\right](g(t)) } \\times \\underbrace{\\left[2 \\partial_t L(t)\\right]}_{\\partial_t g(t)} = \\frac{\\partial_t L(t)}{\\strut\\sqrt{L(t)^2 - H^2}}\\]\nNow to reformulate the problem without defining a function.\nNewton referred to “flowing quantities” or “fluents” and to what today is universally called derivatives as “fluxions.” Newton did not have a notion of inputs and output.3\nAt about the same time as Newton’s inventions, very similar ideas were being given very different names by mathematicians on the European continent. There, an infinitely small change in a quantity was called a “differential” and the differential of \\(x\\) was denoted \\(dx\\).\nThe first calculus textbook was subtitled, Of the Calculus of Differentials, in other words, how to calculate differentials. (See Figure 25.10.) Section I of this 1696 text is entitled, “Where we give the rules of this calculation,” those rules being recognizably the same as presented in Section 23 of this book.\n\n\n\n\n\n\nFigure 25.10: From the start of the first calculus textbook, by le marquis de l’Hôpital, 1696.\n\n\n\nDefinition I of Section I states,\n\n“We call quantities variable* that grow or decrease continuously; and to the contrary constant quantities are those that remain the same while the others change. … The infinitely small amount by which a continuous quantity increases or decreases is called the differential.*”\n\nThe differential is not a derivative. The differential is an infinitely small change in a quantity and a derivative is a rate of change. The differential of a quantity \\(x\\) is written \\(dx\\) in the textbook.4\nThe point of Section I of de l’Hôpital’s textbook is to present the rules by which the differentials of complex quantities can be calculated. You’ll recognize the product rule in de l’Hôpital’s notation:\n\n\n\n\n\n\nThe differential of \\(x\\,y\\) is \\(y\\,dx + x\\,dy\\)\n\n\n\nThe Pythagorean theorem relates the various quantities this way:\n\\[L^2 = x^2 + H^2\\]\nThe differential of each side of the equation refers to “a little bit” of increase in the quantity on that side of the equation: \\[d(L^2) = d(x^2)\\ \\ \\ \\implies\\ \\ \\ 2 L\\, dL = 2 x\\, dx\\] where we’ve used one of the “rules” for calculating differentials. This gives us \\[dx = \\frac{L}{x} dL\\] Think of this as a recipe for calculating \\(dx\\). If you tell me \\(L\\), \\(x\\), and \\(dL\\) then you can calculate the value of \\(dx\\). For instance, suppose the tower is 52 feet tall and that there is \\(L=173\\) feet of tow-rope extending to the skier. The Pythagorean theorem tells us the skier is \\(x=165\\) feet from the base of the tower. The rope is, let us suppose, being pulled in at the top of the tower at \\(dL = 10\\) feet per second. How fast is \\(x\\) changing? \\[dx = \\frac{173\\ \\text{ft}}{165\\ \\text{ft}} \\times 10 \\text{ft s}^{-2} = 10.05\\ \\text{ft s}^{-1}\\]\nWe’ll return to “a little bit of this” when we explore how to add up little bits to get the whole in Section 38."
  },
  {
    "objectID": "Differentiation/24-partial.html#exercises",
    "href": "Differentiation/24-partial.html#exercises",
    "title": "25  Partial change and the gradient vector",
    "section": "25.6 Exercises",
    "text": "25.6 Exercises"
  },
  {
    "objectID": "Differentiation/24-partial.html#drill",
    "href": "Differentiation/24-partial.html#drill",
    "title": "25  Partial change and the gradient vector",
    "section": "25.7 Drill",
    "text": "25.7 Drill\n\n\nPart i What is \\(\\partial_x x\\)?\n\\(0\\)\\(1\\)\\(x\\)\\(y\\)\n\n\n\n\nPart ii What is \\(\\partial_x y\\)?\n\\(0\\)\\(1\\)\\(x\\)\\(y\\)\n\n\n\n\nPart iii What is \\(\\partial_x a\\, x\\)?\n\\(0\\)\\(a\\)\\(x\\)\\(y\\)\n\n\n\n\nPart iv What is \\(\\partial_x x\\, y\\)?\n\\(0\\)\\(1\\)\\(x\\)\\(y\\)\n\n\n\n\nPart v What is \\(\\partial_y x\\, y\\)?\n\\(0\\)\\(1\\)\\(x\\)\\(y\\)\n\n\n\n\nPart vi What is \\(\\partial_x A e^{kt}\\)?\n\\(0\\)\\(A k e^{kx}\\)\\(t\\)\n\n\n\n\nPart vii What is \\(\\partial_t A e^{kt}\\)?\n\\(0\\)\\(k A e^{kt}\\)\\(k A e^{kx}\\)\\(t A e^{kt}\\)\n\n\n\n\nPart viii What is \\(\\partial_x A x e^{kt}\\)?\n\\(A e^{kt}\\)\\(A x e^{kt}\\)\\(0\\)\\(A k x e^{kt}\\)\n\n\n\n\nPart ix What is \\(\\partial_t A x e^{kt}\\)?\n\\(A e^{kt}\\)\\(A k e^{kt}\\)\\(0\\)\\(A k x e^{kt}\\)\n\n\n\n\nPart x What is \\(\\partial_x \\left[\\strut a_0 + a_1 x + a_2 x^2 \\right]\\)?\n0\\(a_1 + 2 a_2 x\\)\\(a_1 + a_2 x\\)\\(a_0 + a_1 x\\)\n\n\n\n\nPart xi What is \\(\\partial_y \\left[\\strut a_0 + a_1 x + a_2 x^2 \\right]\\)?\n0\\(a_1 + 2 a_2 x\\)\\(a_1 + a_2 x\\)\\(a_1 + 2 a_2 y\\)\n\n\n\n\nPart xii What is \\(\\partial_x \\left[\\strut a_0 + a_1 y + a_2 y^2 \\right]\\)?\n0\\(a_1 + 2 a_2 x\\)\\(a_1 + a_2 x\\)\\(a_1 + 2 a_2 y\\)\n\n\n\n\nPart xiii What is \\(\\partial_x \\left[\\strut a_0 + a_1 x + b_1 y + c x y \\right]\\)?\n\\(a_1 + c\\)\\(a_1\\)\\(a_1 + cy\\)\\(a_1 + b1 + c\\)\n\n\n\n\nPart xiv What is \\(\\partial_y \\left[\\strut a_0 + a_1 x + b_1 y + c x y \\right]\\)?\n\\(b_1 + c\\)\\(b_1\\)\\(b_1 + cx\\)\\(a_1 + b1 + c\\)\n\n\n\n\nPart xv What is \\(\\partial_x \\partial_y \\left[\\strut a_0 + a_1 x + b_1 y + c x y \\right]\\)? (Usually we would write \\(\\partial_{xy}\\) instead of \\(\\partial_x \\partial_y\\), but they amount to the same thing.)\n\\(0\\)\\(a_1\\)\\(c\\)\\(b_1\\)\n\n\n\n\nPart xvi What is \\(\\partial_x \\partial_x \\left[\\strut a_0 + a_1 x + b_1 y + c x y \\right]\\)? (Usually we would write \\(\\partial_{xx}\\) instead of \\(\\partial_x \\partial_x\\), but they amount to the same thing.)\n\\(0\\)\\(a_1\\)\\(c\\)\\(b_1\\)\n\n\n\n\nPart xvii What is \\(\\partial_x \\partial_x \\left[\\strut a_0 + a_1 x + b_1 y + c x y + a_2 x^2 + b_2 y^2 \\right]\\)? (Usually we would write \\(\\partial_{xx}\\) instead of \\(\\partial_x \\partial_x\\), but they amount to the same thing.)\n\\(0\\)\\(a_2\\)\\(2 a_2\\)\\(c + a_2\\)\n\n\n\n\nPart xviii What is \\(\\partial_y \\partial_x \\left[\\strut a_0 + a_1 x + b_1 y + c x y + a_2 x^2 + b_2 y^2 \\right]\\)? (Usually we would write \\(\\partial_{yx}\\) instead of \\(\\partial_y \\partial_x\\), but they amount to the same thing.)\n\\(0\\)\\(2 a_2\\)\\(c\\)\\(2 b_2\\)\n\n\n\n\nPart xix What is \\(\\partial_x \\left[\\strut A x^n y^m \\right]\\)?\n\n\\(A y^m\\)\n\\(A n m x^{n-1} y^{m-1}\\)\n\\(A n x^{n-1} y^m\\)\n\\(A m x^{n} y^{m-1}\\)\n\n\n\n\n\nPart xx What is \\(\\partial_y \\left[\\strut A x^n y^m \\right]\\)?\n\n\\(A m y^{m-1}\\)\n\\(A n m x^{n-1} y^{m-1}\\)\n\\(A n x^{n-1} y^m\\)\n\\(A m x^{n} y^{m-1}\\)\n\n\n\n\n\nPart xxi What is \\(\\partial_{xy} \\left[\\strut A x^n y^m \\right]\\)?\n\n\\(A m x^{n-1} y^{m-1}\\)\n\\(A n m x^{n-1} y^{m-1}\\)\n\\(A n x^{n-1} y^{m-1}\\)\n\\(A m x^{n} y^{m-1}\\)\n\n\n\n\n\nPart xxii What is \\(\\partial_x \\left[\\strut f(x) + y\\right]\\)?\n\n\\(0\\)\n\\(\\partial_x f(x) + 1\\)\n\\(\\partial_x f(x)\\)\n\\(\\partial_x f(x) + y\\)\n\n\n\n\n\nPart xxiii What is \\(\\partial_x \\left[\\strut f(x) + g(y)\\right]\\)?\n\n\\(0\\)\n\\(\\partial_x f(x) + \\partial_x g(y)\\)\n\\(\\partial_x f(x)\\)\n\\(\\partial_x f(x) + \\partial_y g(y)\\)\n\n\n\n\n\nPart xxiv What is \\(\\partial_y \\left[\\strut f(x) + g(y)\\right]\\)?\n0\\(\\partial_x g(y)\\)\\(\\partial_x f(x)\\)\\(\\partial_y g(y)\\)\n\n\n\n\nPart xxv What is \\(\\partial_x \\partial_y \\left[\\strut f(x) + g(y)\\right]\\)?\n\n0\n\\(\\partial_x \\partial_y g(y)\\)\n\\(\\partial_x f(x)\\)\n\\(\\partial_y g(y)\\)\n\n\n\n\n\nPart xxvi What is \\(\\partial_y \\partial_y \\left[\\strut f(x) + g(y)\\right]\\)?\n01\\(\\partial_y g(y)\\)\\(\\partial_{yy} g(y)\\)\n\n\n\n\nPart xxvii What is \\(\\partial_y f(x) g(y)\\)?\n\n\\(g(y)\\ \\partial_y f(x) + f(x) \\ \\partial_y g(y)\\)\n\\(f(x)\\ \\partial_{y} g(y)\\)\n\\(\\partial_y g(y)\\)\n0\n\n\n\n\n\nPart xxviii What is \\(\\partial_y h(x,y) g(y)\\)?\n\n$ g(y) _y h(x,y) + h(x,y) _y g(y)$\n\\(g(y) \\partial_y h(x, y)\\)\n\\(\\partial_y g(y)\\)\n0\n\n\n\n\n\nPart xxix What is \\(\\partial_x h(x,y) g(y)\\)?\n\n\\(g(y) \\partial_y h(x, y)\\)\n\\(g(y)\\ \\partial_x h(x,y) + h(x,y)\\ \\partial_x g(y)\\)\n\\(\\partial_x h(x, y)\\)\n\\(g(y) \\partial_x h(x, y)\\)\n\n\n\n\n\nPart xxx What is \\(\\partial_{yx} h(x,y) g(y)\\)?\n\n\\((\\partial_x g(y))\\  (\\partial_x h(x, y)) + g(y) (\\partial_{xx} h(x, y) )\\)\n\\(g(y) \\partial_{yx} h(x,y) + h(x,y)\\ \\partial_y g(y)\\)\n\\(\\partial_{yx} h(x, y)\\)\n\\((\\partial_y g(y)) \\ (\\partial_x h(x, y)) + g(y)\\ (\\partial_{yx} h(x, y))\\)\n\n\n\n\n\nPart xxxi What is the “with-respect-to” input in \\(\\partial_y xy\\)?\n\\(y\\)\\(x\\)\\(1\\)\n\n\n\n\nPart xxxii What is the “with-respect-to” input in \\(\\partial_x y\\)?\n\\(y\\)\\(x\\)\\(1\\)\n\n\n\n\nPart xxxiii What is the “with-respect-to” input in \\(\\partial_t y\\)?\n\\(y\\)\\(t\\)\\(1\\)\n\n\n\n\nPart xxxiv At which of these inputs is the function steepest in the x-direction?\n\\((x=0, y=1)\\)\\((x=1, y=5)\\)\\((x=0, y=6)\\)\\((x=-2, y=6)\\)\n\n\n\n\nPart xxxv At which of these inputs is the function practically flat?\n\\((x=0, y=1)\\)\\((x=1, y=2)\\)\\((x=0, y=6)\\)\\((x=-2, y=3)\\)\n\n\n\n\nPart xxxvi You are standing on the input point \\((x=-1,y=4)\\). In terms of the compass points (where north would be up and east to the right), which direction points most steeply uphill from where you are standing.\nNESESWNW\n\n\n\n\nPart xxxvii You are standing on the input point \\((x=2,y=1)\\). In terms of the compass points (where north would be up and east to the right), which direction points most steeply uphill from where you are standing.\nNESESWNW\n\n\n\n\nPart xxxviii You have been hiking all day and have reached map coordinate (x=2, y=2). You are completely exhausted. Time for a break. You want to walk along the hill, without any change of elevation. Which compass direction should you head in to get started?\nNE or SWSE but not NWNW or SENW but not SE"
  },
  {
    "objectID": "Differentiation/25-approximation.html",
    "href": "Differentiation/25-approximation.html",
    "title": "26  Local approximations [DRAFT]",
    "section": "",
    "text": "NOTE NOTE NOTE\nThe material in this first section has been moved to the new Block 1 chapter 5. CHANGE THIS CHAPTER TO FOCUS ON THE ANALYSIS OF LOW-ORDER POLYNOMIALS BY differentiation.\nThe information that you have about the relationship often takes the form of a data table. Each row records one trial in which the values of the inputs have been measured and the corresponding output value recorded. We’ll discuss the methods of constructing functions to match such data in Block 5 of this course.\nAnother common form for the information about the relationship is about derivatives. That is, you know something about the derivative of a relationship even though you don’t (yet) have a form for the function describing the relationship. As an example, think about building a model of the sustainable speed of a bicycle as a function of the gear selected and the grade of the road—up or down.\nConsider these three questions that any experienced bicyclist can likely answer:\nUsing the methods in this chapter, the answers to those three questions let you choose an appropriate form for the speed(gear, grade) function. Then, using methods in Block 5 of this text, you can make a few measurements for any given rider and construct a model customized to that rider.\nNote that the three questions all have to do with derivatives. An “optimal gear” is a gear at which \\(\\partial_\\text{gear} \\text{speed}(\\text{gear}, \\text{grade}) = 0\\). That you ride slower the higher the numerical value of the slope means that \\(\\partial_\\text{grade} \\text{speed}(\\text{gear}, \\text{grade}) < 0\\). And we know that \\(\\partial_\\text{gear} \\text{speed}(\\text{gear}, \\text{grade})\\) depends on the grade; that’s why there’s a different optimal gear at each grade.\nEND OF MATERIAL COPIED From modeling/05-low-order-polynomials.Rmd\nWe have focused in this book on a small set of basic modeling functions and three operations for assembling new functions out of old ones: linear combination, multiplication, and composition. All of these have a domain that is the whole number line, or the positive half of the number line, or perhaps the whole number line leaving out zero or some other isolated point. Consider such domains to be global.\nWe also discussed the components of piecewise functions. Each component is a function defined on a limited domain, an interval \\(a \\leq x \\leq b\\). In contrast to the global domains, we’ll call the limited domains local.\nIn this chapter, we’ll explore a simple and surprisingly powerful method to approximate any function locally, that is, over a small domain.\nThe information that you have about the relationship often takes the form of a data table. Each row records one trial in which the values of the inputs have been measured and the corresponding output value recorded. We’ll discuss the methods of constructing functions to match such data in Block 5 of this course.\nAnother common form for the information about the relationship is about derivatives. That is, you know something about the derivative of a relationship even though you don’t (yet) have a form for the function describing the relationship. As an example, think about building a model of the sustainable speed of a bicycle as a function of the gear selected and the grade of the road—up or down.\nConsider these three questions that any experienced bicyclist can likely answer:\nUsing the methods in this chapter, the answers to those three questions let you choose an appropriate form for the speed(gear, grade) function. Then, using methods in Block 5 of this text, you can make a few measurements for any given rider and construct a model customized to that rider.\nNote that the three questions all have to do with derivatives. An “optimal gear” is a gear at which \\(\\partial_\\text{gear} \\text{speed}(\\text{gear}, \\text{grade}) = 0\\). That you ride slower the higher the numerical value of the slope means that \\(\\partial_\\text{grade} \\text{speed}(\\text{gear}, \\text{grade}) < 0\\). And we know that \\(\\partial_\\text{gear} \\text{speed}(\\text{gear}, \\text{grade})\\) depends on the grade; that’s why there’s a different optimal gear at each grade."
  },
  {
    "objectID": "Differentiation/25-approximation.html#eight-simple-shapes",
    "href": "Differentiation/25-approximation.html#eight-simple-shapes",
    "title": "26  Local approximations [DRAFT]",
    "section": "26.1 Eight simple shapes",
    "text": "26.1 Eight simple shapes\nIn many modeling situations with a single input, you can get very close to a good modeling function \\(f(x)\\) by selecting one of eight simple shapes, shown in Figure 26.1.\n\n\n\n\n\nFigure 26.1: The eight simple shapes, locally, of functions with one input. (See Section 26.)\n\n\n\n\n\n\n\n\n\nFigure 26.2: The eight simple shapes, locally, of functions with one input. (See Section 26.)\n\n\n\n\nTo choose among these shapes, consider your modeling context:\n\nis the relationship positive (slopes up) or negative (slopes down)\nis the relationship monotonic or not\nis the relationship concave up, concave down, or neither\n\nSome examples, scenarios where the modeler knows about the derivative and concavity of the relationship being modeled. It’s often the case that your knowledge of the system comes in this form.\n\nThe incidence of an out-of-control epidemic versus time is concave up, but shallow-then-steep. As the epidemic is brought under control, the decline is steep-then-shallow and concave up. Over the whole course of an epidemic, there is a maximum incidence. Experience shows that epidemics can have a phase where incidence reaches a local minimum: a decline as people practice social distancing followed by an increase as people become complacent.\nHow many minutes can you run as a function of speed? Concave down and shallow-then-steep; you wear out faster if you run at high speed. How far can you walk as a function of time? Steep-then-shallow and concave down; your pace slows as you get tired.\nHow does the stew taste as a function of saltiness. The taste improves as the amount of salt increases … up to a point. Too much salt and the stew is unpalatable.\nThe temperature of cooling water or the emission of radioactivity as functions of time are concave up and steep-then-shallow.\nHow much fuel is consumed by an aircraft as a function of distance? For long flights the function is concave up and shallow-then-steep; fuel use increases with distance, but the amount of fuel you have to carry also increases with distance and heavy aircraft use more fuel per mile.\nIn micro-economic theory there are production functions that describe how much of a good is produced at any given price, and demand functions that describe how much of the good will be purchased as a function of price.\n\nAs a rule, production increases with price and demand decreases with price. In the short term, production functions tend to be concave down, since it’s hard to squeeze increased production out of existing facilities.\n\nFor demand in the short term, functions will be concave up when there is some group of consumers who have no other choice than to buy the product. An example is the consumption of gasoline versus price: it’s hard in the short term to find another way to get to work. In the long term, consumption functions can be concave down as consumers find alternatives to the high-priced good. For example, high prices for gasoline may, in the long term, prompt a switch to more efficient cars, hybrids, or electric vehicles. This will push demand down steeply."
  },
  {
    "objectID": "Differentiation/25-approximation.html#low-order-polynomials",
    "href": "Differentiation/25-approximation.html#low-order-polynomials",
    "title": "26  Local approximations [DRAFT]",
    "section": "26.2 Low-order polynomials",
    "text": "26.2 Low-order polynomials\nThere is a simple, familiar functional form that, by selecting parameters appropriately, can take on each of the eight simple shapes: the second-order polynomial. \\[g(x) \\equiv a + b x + c x^2\\] As you know, the graph of \\(g(x)\\) is a parabola.\n\nThe parabola opens upward if \\(0 < c\\). That’s the shape of a local minimum.\nThe parabola opens downward if \\(c < 0\\). That’s the shape of a local maximum\n\nConsider what happens if \\(c = 0\\). The function becomes simply \\(a + bx\\), the straight-line function.\n\nWhen \\(0 < b\\) the line slopes upward.\nWhen \\(b < 0\\) the line slopes downward.\n\nWith the appropriate choice of parameters, the form \\(a + bx + cx^2\\) is capable of representing four of the eight simple shapes. What about the remaining four? This is where the idea of local becomes important. Those remaining four shapes are the sides of parabolas, as in Figure 26.3.\n\n\n\n\n\nFigure 26.3: Four of the eight simple shapes correspond to the sides of the parabola. The labels refer to the graphs in Figure 26.2."
  },
  {
    "objectID": "Differentiation/25-approximation.html#sec-low-order-two",
    "href": "Differentiation/25-approximation.html#sec-low-order-two",
    "title": "26  Local approximations [DRAFT]",
    "section": "26.3 The low-order polynomial with two inputs",
    "text": "26.3 The low-order polynomial with two inputs\nFor functions with two inputs, the low-order polynomial approximation looks like this:\n\\[g(x, y) \\equiv a_0 + a_x x + a_y y + a_{xy} x y + a_{yy} y^2 + a_{xx} x^2\\] In reading this form, note the system being used to name the polynomial’s coefficients. First, we’ve used \\(a\\) as the root name of all the coefficients. Sometimes we might want to compare two or more low-order polynomials, so it’s convenient to be able to use \\(a\\) for one, \\(b\\) for another, and so on.\nThe subscripts on the coefficients describes exactly which term in the polynomial involves each coefficient. For instance, the \\(a_{yy}\\) coefficient applies to the \\(y^2\\) term, while \\(a_x\\) applies to the \\(x\\) term.\nEach of \\(a_0, a_x,\\) \\(a_y,\\) \\(a_{xy}, a_{yy}\\), and \\(a_{xx}\\) will, in the final model, be a constant quantity. Don’t be confused by the use of \\(x\\) or \\(y\\) in the name of the coefficients. Each coefficient is a constant and not a function of the inputs. Often, your prior knowledge of the system being modeled will tell you something about one or more of the coefficients, for example, whether it is positive or negative. Finding a precise value is often based on quantitative data about the system.\nIt helps to have different names for the various terms. It’s not too bad to say something like, “the \\(a_{xy}\\) term.” (Pronounciation: “a sub x y” or “a x y”) But the proper names are: linear terms, quadratic terms, and interaction term. And a shout out to \\(a_0\\), the constant term.\n\\[g(x, y) \\equiv a_0 + \\underbrace{a_x x + a_y y}_\\text{linear terms} \\ \\ \\ +\n\\underbrace{a_{xy} x y}_\\text{interaction term} +\\ \\ \\  \\underbrace{a_{yy} y^2 + a_{xx} x^2}_\\text{quadratic terms}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 26.4: A saddle\n\n\n\n\nIf you’re like many people, you find it harder to walk uphill than down, and find it takes more out of you to walk longer distances than shorter. Let’s build a model of this, using nothing more than your intuition and the method of low-order polynomial approximations.\nLet’s call the map distance walked \\(d\\). (“Map distance” is the horizontal change in position, disregarding vertical changes.) The steepness of the hill will be the “grade” \\(g\\), which is measured as the horizontal distance covered divided by the vertical climb. If you’re going downhill, the grade is negative.\nThe key ingredient in the model: We’ll measure the “difficulty” or “exertion” to walking as the energy consumed during the walk: \\(E(d, g)\\).\nSome assumptions about walking and energy consumed:\n\nIf you don’t walk, you consume zero energy walking.\nThe energy consumed should be proportional to the length of the walk. This is an assumption, and is probably valid, only for walks of short to medium distances, as opposed to forced marches over tens of miles.\n\nWe’ll start with the full 2nd-order polynomial in two inputs, and then seek to eliminate terms that aren’t needed.\n\\[E_{big}(d, g) \\equiv a_0 + a_d\\, d + a_g\\, g + a_{dg}\\, d\\, g + a_{dd}\\,d^2 + a_{gg}\\,g^2\\] According to assumption (1), when \\(E(d=0, g) = 0\\). Of course, if you are walking zero distance, it doesn’t matter what the grade is; the energy consumed is still zero.\nConsequently, we know that all terms that don’t include a \\(d\\) should go away. This leaves us with\n\\[E_{medium}(d, g) \\equiv  a_d\\, d + a_{dg}\\, d\\, g + a_{dd}\\,d^2 = d \\left[\\strut a_d + a_{dg}\\, g + a_{dd}\\,d\\right]\\] Assumption (2) says that energy consumed is proportional to \\(d\\). The multiplier on \\(d\\) in \\(E_{medium}()\\) is \\(\\left[\\strut a_d + a_{dg}\\, g + a_{dd}\\,d\\right]\\) which is itself a function of \\(d\\). A proportional relationship implies a multiplier that doesn’t depend on the quantity itself. This means that \\(a_{dd} = 0\\).\nThis leaves us with a very simple model: \\[E(d, g) \\equiv \\left[\\strut a_1 + a_2\\, g\\right]\\, d\\] where we have simplified the labeling on the coefficients since there are only two in the model.\nPerhaps assumption (2) is mis-placed and that the energy consumed per unit distance in a walk increases with the length of the walk. If so, we would need to return to the question of \\(a_{dd}\\). This is typical of the modeling cycle. Trying to be economical with model terms highlights the question of which terms are so small they can be ignored.\n\n\nIn selecting cadets for pilot training, two criteria are the cadet’s demonstrated flying aptitude and the leadership potential of the cadet. Let’s assume that the overall merit \\(M\\) of a candidate is a function of flying aptitude \\(F\\) and leadership potential \\(L\\).\nCurrently, the merit score is a simple function of the \\(F\\) and \\(L\\) scores: \\[M_{current}(F, L) \\equiv F + L\\]\nThe general in charge of the training program is not satisfied with the current merit function. “I’m getting too many cadets who are great leaders but poor pilots, and too many pilot hot-shots who are not good leaders. I would rather have an good pilot who is a good leader than have a great pilot who is a poor leader or a poor pilot who is a great leader.” (You might reasonably agree or disagree with this point of view, but the general is in charge.)\nThe general has tasked you to revise the formula to better match her views about the balance betwen flying ability and leadership potential.\nHow should you go about constructing \\(M_{improved}(F, L)\\)?\nYou recognize that \\(F + L\\) is a low-order polynomial: just the linear terms are present without a constant or interaction term or quadratic terms. Low-order polynomials are a good way to approximate any formula locally, so you have decided to follow that route.\nQuadratic terms are appropriate when a model needs to feature a locally optimal level of the of the inputs. But it will never be the case that a lower flying score will be more favored than a higher score, and the same thing for the leadership score. So your model doesn’t need quadratic terms.\nThat leaves the interaction term as the way forward. The low-order polynomial model will be \\[M_{improved}(F, L) \\equiv d_0 + F + L + d_{FL} FL\\] Should \\(d_{FL}\\) be positive or negative?\nImagine a cadet Drew with acceptable and equal F and L scores. Another cadet, Blake, has scores that are \\(F+\\epsilon\\) and \\(L-\\epsilon\\), where \\(\\epsilon\\) might be positive or negative. Under the original formula for merit, Drew and Blake have equal merit. Under the new criteria, Drew should have a higher merit than Blake. In other words: \\[M_{improved}(F, L) - M_{improved}(F+\\epsilon, L-\\epsilon) > 0\\]\nReplace \\(M_{improved}(F, L)\\) with the low-order polynomial approximation given earlier. \\[\\underbrace{d_0 + F + L + d_{FL} FL}_{M_{improved}(F, L)} - \\underbrace{\\left[{\\large\\strut} d_0 + \\left[ F + \\epsilon\\right] + \\left[ L - \\epsilon\\right] + d_{FL} (FL -\\epsilon L + \\epsilon F -  \\epsilon^2)\\right]}_{M_{improved}(F+\\epsilon, L-\\epsilon)} > 0\\] Collecting and cancelling terms in the above gives \\[- d_{FL}(\\epsilon(F-L) + \\epsilon^2) > 0\\] Since \\(F\\) and \\(L\\) were assumed equal, this results in \\[M_{improved}(F, L) - M_{improved}(F+\\epsilon, L-\\epsilon) = d_{FL}\\, \\epsilon^2 > 0\\] Thus, \\(d_{FL}\\) will have to be positive."
  },
  {
    "objectID": "Differentiation/25-approximation.html#sec-partial-thought",
    "href": "Differentiation/25-approximation.html#sec-partial-thought",
    "title": "26  Local approximations [DRAFT]",
    "section": "26.4 Thinking partially",
    "text": "26.4 Thinking partially\nThe expression for a general low-order polynomial in two inputs can be daunting to think about all at once: \\[g(x, y) \\equiv a_0 + a_x x + a_y y + a_{xy} x y + a_{xx} x^2 + a_{yy} y^2\\] As with many complicated settings, a good approach can be to split things up into simpler pieces. With a low-order polynomial, one such splitting up involves partial derivatives. There are six potentially non-zero partial derivatives for a low-order polynomial, of which two are the same; so only five quantities to consider.\n\n\\(\\partial_x g(x,y) = a_x + a_{xy}y + 2 a_{xx} x\\)\n\\(\\partial_y g(x,y) = a_y + a_{xy}x + 2 a_{yy} y\\)\n\\(\\partial_{xy} g(x,y) = \\partial_{yx} g(x,y) = a_{xy}\\). These are the so-called mixed partial derivatives. It doesn’t matter whether you differentiate by \\(x\\) first or by \\(y\\) first. The result will always be the same for any smooth function.\n\\(\\partial_{xx} g(x,y) = 2 a_{xx}\\)\n\\(\\partial_{yy} g(x,y) = 2 a_{yy}\\)\n\nThe above list states neutral mathematical facts that apply generally to any low-order polynomial whatsoever.3 Those facts, however, shape a way of asking questions of yourself that can help you shape the model of a given phenomenon based on what you already know about how things work.\nTo illustrate, consider the situation of modeling the effect of study \\(S\\) and of tutoring \\(T\\) (a.k.a. office hours, extended instruction) on performance \\(P(S,T)\\) on an exam. In the spirit of partial derivatives, we’ll assume that all other factors (student aptitude, workload, etc.) are held constant.\nTo start, pick fiducial values for \\(S\\) and \\(T\\) to define the local domain for the model. Since \\(S=0\\) and \\(T=0\\) are easy to envision, we’ll use those for the fiducial values.\nNext, ask five questions, in this order, about the system being modeled.\n\nDoes performance increase with study time? Don’t over-think this. Remember that the approximation is around a fiducial point. Here, a reasonable answer is, “yes.” We’ll take\\(\\partial_S P(S, T) > 0\\) to imply that \\(a_S > 0\\). This is appropriate because close to the fiducial point, the other contributors to \\(\\partial_S P(S, T)\\), namely \\(a_{ST}T + 2 a_{SS} S\\) will be vanishingly small.\nDoes performance increase with time spent being tutored? Again, don’t over-think this. Don’t worry (yet) that your social life is collapsing because of the time spent studying and being tutored, and the consequent emotional depression will cause you to fail the exam. We’re building a model here and the heuristic being used is to consider factors in isolation. Since (as we expect you’ll agree) \\(\\partial_T P(S, T) > 0\\), we have that \\(a_T > 0\\).\n\nNow the questions get a little bit harder and will exercise your calculus-intuition since you’ll have to think about changes in the rates of change.\n\nThis question has to do with the mixed partial derivative, which we’ve written variously as \\(\\partial_{ST} P(S,T)\\) or \\(\\partial_{TS} P(S,T)\\) and which it might be better to think about as \\(\\partial_S \\left[\\partial_T P(S,T) \\right]\\) or \\(\\partial_T \\left[\\partial S P(S,T)\\right]\\). Although these are mathematically equal, often your intuition will favor one form or the other. Recall that we’re working on the premise that \\(\\partial_S P(S,T) > 0\\), or, in other words, study will help you do better on the exam. Now for \\(\\partial_T \\left[\\partial S P(S,T)\\right]\\). This is a the matter of whether some tutoring will make your study more effective. Let’s say yes here, since tutoring can help you overcome a misconception that’s a roadblock to effective study. So \\(\\partial_{TS} P(S,T) > 0\\) which implies \\(a_{ST} > 0\\).\n\nThe other way round, \\(\\partial_S \\left[\\partial_T P(S,T) \\right]\\) is a matter of whether increasing study will enhance the positive effect of tutoring. We’ll say yes here again, because a better knowledge of the material from studying will help you follow what the tutor is saying and doing. From pure mathematics, we already know that the two forms of mixed partials are equivalent, but to the human mind they sometimes (and incorrectly) appear to be different in some subtle, ineffable way.\nIn some modeling contexts, there might be no clear answer to the question of \\(\\partial_{xy}\\, g(x,y)\\). That’s also a useful result, since it tells us that the \\(a_{xy}\\) term may not be important to understanding that system.\n\nOn to the question of \\(\\partial_{SS} P(S,T)\\), that is, whether \\(a_{SS}\\) is positive, negative, or negligible. We know that \\(a_{SS} S^2\\) will be small whenever \\(S\\) is small, so this is our opportunity to think about bigger \\(S\\). So does the impact of a unit of additional study increase or decrease the more you study? One point of view is that there is some moment when “it all comes together” and you understand the topic well. But after that epiphany, more study might not accomplish as much as before the epiphany. Another bit of experience is that “cramming” is not an effective study strategy. And then there’s your social life … So let’s say, provisionally, that there is an argmax to study, beyond which point you’re not helping yourself. This means that \\(a_{SS} < 0\\).\nFinally, consider \\(\\partial_{TT} P(S, T)\\). Reasonable people might disagree here, which is itself a reason to suspect that \\(a_{TT}\\) is negligible.\n\nAnswering these questions doesn’t provide a numerical value for the coefficients on the low-order polynomial, and says nothing at all about \\(a_0\\), since all the questions are about change.\nAnother step forward in extracting what you know about the system you are modeling is to construct the polynomial informed by questions 1 through 5. Since you don’t know the numerical values for the coefficients, this might seem impossible. But there is a another modeler’s trick that might help.\nLet’s imagine that the domain of both \\(S\\) and \\(T\\) or the interval zero to one. This is not to say that we think one hour of study is the most possible but simply to defer the question of what are appropriate units for \\(S\\) and \\(T\\). Very much in this spirit, for the coefficients we’ll use \\(+0.5\\) when are previous answers indicated that the coefficient should be greater than zero, \\(-0.5\\) when the answers pointed to a negative coefficient, and zero if we don’t know. Using this technique, here’s the model, which mainly serves as a basis for checking whether our previous answers are in line with our broader intuition before we move on quantitatively.\n\nP <- makeFun(0.5*S + 0.5*T + 0.5*S*T - 0.5*S^2 ~ S & T)\ncontour_plot(P(S, T) ~ S & T, domain(S=0:1, T=0:1))\n\n\n\n\nFigure 26.5: The result of our intuitive investigation of the effects of study and tutoring on exam performance. The units are not yet assigned.\n\n\n\n\nNotice that for small values of \\(T\\), the horizontal spacing between adjacent contours is large. That is, it takes a lot of study to improve performance a little. At large values of \\(T\\) the horizontal spacing between contours is smaller."
  },
  {
    "objectID": "Differentiation/25-approximation.html#finding-coefficients-from-data",
    "href": "Differentiation/25-approximation.html#finding-coefficients-from-data",
    "title": "26  Local approximations [DRAFT]",
    "section": "26.5 Finding coefficients from data",
    "text": "26.5 Finding coefficients from data\nLow-order polynomials are often used for constructing functions from data. In this section, I’ll demonstrate briefly how this can be done. The full theory will be introduced in Block 5 of this text.\nThe data I’ll use for the demonstration is a set of physical measurements of height, weight, abdominal circumference, etc. on 252 human subjects. These are contained in the Body_fat data frame, shown below. ::: {.cell layout-align=“center” fig.showtext=‘false’} ::: {.cell-output-display}\n\n\n::: :::\nOne of the variables records the body-fat percentage, that is, the fraction of the body’s mass that is fat. This is thought to be an indicator of fitness and health, but it is extremely hard to measure and involves weighing the person when they are fully submerged in water. This difficulty motivates the development of a method to approximation body-fat percentage from other, easier to make measurements such as height, weight, and so on.\nFor the purpose of this demonstration, we’ll build a local polynomial model of body-fat percentage as a function of height (in inches) and weight (in pounds).\nThe polynomial we choose will omit the quadratic terms. It will contain the constant, linear, and interaction terms only. That is \\[\\text{body.fat}(h, w) \\equiv c_0 + c_h h + c_w w + c_{hw} h w\\] The process of finding the best coefficients in the polynomial is called linear regression. Without going into the details, we’ll use linear regression to build the body-fat model and then display the model function as a contour plot.\n\nmod <- lm(bodyfat ~ height + weight + height*weight,\n          data = Zcalc::Body_fat)\nbody_fat_fun <- makeFun(mod)\ncontour_plot(body_fat_fun(height, weight) ~ height + weight,\n             domain(weight=c(100, 250), height = c(60, 80))) %>%\n  gf_labs(title = \"Body fat percentage\")\n\n\n\n\nFigure 26.6: A low order polynomial model of body fat percentage as a function of height (inches) and weight (lbs).\n\n\n\n\nThat we can build such a model doesn’t mean that it’s useful for anything. In Block 5 of the text we’ll return to the question of how well a model constructed from data represents the real-world relationships that the model attempts to describe."
  },
  {
    "objectID": "Differentiation/25-approximation.html#exercises",
    "href": "Differentiation/25-approximation.html#exercises",
    "title": "26  Local approximations [DRAFT]",
    "section": "26.6 Exercises",
    "text": "26.6 Exercises"
  },
  {
    "objectID": "Differentiation/26-taylor.html",
    "href": "Differentiation/26-taylor.html",
    "title": "27  Polynomials",
    "section": "",
    "text": "A big part of the high-school algebra curriculum is about polynomials. In some ways, this is appropriate since polynomials played an outsized part in the historical development of mathematical theory. Indeed, the so-called “Fundamental theorem of algebra” is about polynomials.1\nFor modelers, polynomials are a mixed bag. They are very widely used in modeling. Sometimes this is entirely appropriate, for instance the low-order polynomials that are the subject of Section 26. The problems come when high-order polynomials are selected for modeling purposes. Building a reliable model with high-order polynomials requires a deep knowledge of mathematics, and introduces serious potential pitfalls. Modern professional modelers learn the alternatives to high-order polynomials, but newcomers often draw on their experience in high-school and give unwarranted credence to polynomials. This chapter attempts to guide you to the ways you are likely to see polynomials in your future work and to help you avoid them when better alternatives are available."
  },
  {
    "objectID": "Differentiation/26-taylor.html#sec-polynomial-basics",
    "href": "Differentiation/26-taylor.html#sec-polynomial-basics",
    "title": "27  Polynomials",
    "section": "27.1 Basics of polynomials with one input",
    "text": "27.1 Basics of polynomials with one input\nA polynomial is a linear combination of a particular class of functions: power-law functions with non-negative, integer exponents: 1, 2, 3, …. The individual functions are called monomials, a word that echoes the construction of chemical polymers out of monomers; for instance, the material polyester is constructed by chaining together a basic chemical unit called an ester.\nIn one input, say \\(x\\), the monomials are \\(x^1, x^2, x^3\\), and so on. (There’s also \\(x^0\\), but that’s better thought of as the constant function.) An n-th order polynomial has monomials up to exponent \\(n\\). For example, the form of a third-order polynomial is \\[a_0 + a_1 x^1 + a_2 x^2 + a_3 x^3\\]\nThe domain of polynomials, like the power-law functions they are assembled from, is the real numbers, that is, the entire number line \\(-\\infty < x < \\infty\\). But for the purposes of understanding the shape of high-order polynomials, it’s helpful to divide the domain into three parts: a wriggly domain at the center and two tail domains to the right and left of the center.\n\n\n\n\n\nFigure 27.1: A \\(n\\)th-order polynomial can have up to \\(n-1\\) critical points that it wriggles among. A 7-th order polynomial is shown here in which there are six local maxima or minima alternatingly.\n\n\n\n\nFigure 27.1 shows a 7th order polynomial—that is, the highest-order term is \\(x^7\\). In one of the tail domains the function value heads off to \\(\\infty\\), in the other to \\(-\\infty\\). This is a necessary feature of all odd-order polynomials: 1, 3, 5, 7, …\nIn contrast, for even-order polynomials (2, 4, 6, …) the function value in the two tail domains go in the same direction, either both to \\(\\infty\\) (Hands up!) or both to \\(-\\infty\\).\nIn the wriggly domain in Figure 27.1, there are six argmins or argmaxes.\nAn \\(n\\)th-order polynomial can have up to \\(n-1\\) extrema.\nNote that the local polynomial approximations in Section 26) are at most 2nd order and so there is at most 1 wriggle: a unique argmax. If the approximation does not include the quadratic terms (\\(x^2\\) or \\(y^2\\)) then there is no argmax for the function."
  },
  {
    "objectID": "Differentiation/26-taylor.html#multiple-inputs",
    "href": "Differentiation/26-taylor.html#multiple-inputs",
    "title": "27  Polynomials",
    "section": "27.2 Multiple inputs?",
    "text": "27.2 Multiple inputs?\nHigh-order polynomials are rarely used with multiple inputs. One reason is the proliferation of coefficients. For instance, here is the third-order polynomial in two inputs, \\(x\\), and \\(y\\). \\[\\underbrace{b_0 + b_x x + b_y y}_\\text{first-order terms} + \\underbrace{b_{xy} x y + b_{xx} x^2 + b_{yy} y^2}_\\text{second-order terms} + \\underbrace{b_{xxy} x^2 y + b_{xyy} x y^2 + b_{xxx} x^3 + b_{yyy} y^3}_\\text{third-order terms}\\]\nThis has 10 coefficients. With so many coefficients it’s hard to ascribe meaning to any of them individually. And, insofar as some feature of the function does carry meaning in terms of the modeling situation, that meaning is spread out and hard to quantify."
  },
  {
    "objectID": "Differentiation/26-taylor.html#sec-high-order-approx",
    "href": "Differentiation/26-taylor.html#sec-high-order-approx",
    "title": "27  Polynomials",
    "section": "27.3 High-order approximations",
    "text": "27.3 High-order approximations\nThe potential attraction of high-order polynomials is that, with their wriggly interior, they can take on a large number of appearances. This chameleon-like behavior has historically made them the tool of choice for understanding the behavior of approximations. That theory has motivated the use of polynomials for modeling patterns in data, but, paradoxically, has shown that high-order polynomials should not be the tool of choice for modeling data.2\nPolynomial functions lend themselves well to calculations, since the output from a polynomial function can be calculated using just the basic arithmetic functions: addition, subtraction, multiplication, and division. To illustrate, consider this polynomial: \\[g(x) \\equiv x - \\frac{1}{6} x^3\\] Since the highest-order term is \\(x^3\\) this is a third-order polynomial. (As you’ll see, we picked these particular coefficients, 0, 1, 0, -1/6, for a reason.) With such simple coefficients the polynomial is easy to handle by mental arithmetic. For instance, for \\(g(x=1)\\) is \\(5/6\\). Similarly, \\(g(x=1/2) = 23/48\\) and \\(g(x=2) = 2/3\\). A person of today’s generation would use an electronic calculator for more complicated inputs, but the mathematicians of Newton’s time were accomplished human calculators. It would have been well within their capabilities to calculate, using paper and pencil, \\(g(\\pi/4) = 0.7046527\\).3\nOur example polynomial, \\(g(x) \\equiv x - \\frac{1}{6}x^3\\), graphed in color in Figure 27.2, doesn’t look exactly like the sinusoid. If we increased the extent of the graphics domain, the disagreement would be even more striking, since the sinusoid’s output is always in \\(-1 \\leq \\sin(x) \\leq 1\\), while the polynomial’s tails are heading off to \\(\\infty\\) and \\(-\\infty\\). But, for a small interval around \\(x=0\\), exactly aligns with the sinusoid.\n\n\n\n\n\nFigure 27.2: The polynomial \\(g(x) \\equiv x -x^3 / 6\\) is remarkably similar to \\(\\sin(x)\\) near \\(x=0\\).\n\n\n\n\nIt’s clear from the graph that the approximation is excellent near \\(x=0\\) and gets worse as \\(x\\) gets larger. The approximation is poor for \\(x \\approx \\pm 2\\). We know enough about polynomials to say that the approximation will not get better for larger \\(x\\); the sine function has a range of \\(-1\\) to \\(1\\), while the left and right tails of the polynomial are heading off to \\(\\infty\\) and \\(-\\infty\\) respectively.\nOne way to measure the quality of the approximation is the error \\({\\cal E}(x)\\) which gives, as a function of \\(x\\), the difference between the actual sinusoid and the approximation: \\[{\\cal E}(x) \\equiv |\\strut\\sin(x) - g(x)|\\] The absolute value used in defining the error reflects our interest in how far the approximation is from the actual function and not so much in whether the approximation is below or above the actual function. Figure 27.3 shows \\({\\cal E}(x)\\) as a function of \\(x\\). Since the error is the same on both sides of \\(x=0\\), only the positive \\(x\\) domain is shown.\n\n\n\n\n\nFigure 27.3: The error \\({\\cal E}(x)\\) of \\(x - x^3/6\\) as an approximation to \\(\\sin(x)\\). Top panel: linear scale. Bottom panel: on a log-log scale.\n\n\n\n\nFigure 27.3 shows that for \\(x < 0.3\\), the error in the polynomial approximation to \\(\\sin(x)\\) is in the 5th decimal place. For instance, \\(\\sin(0.3) = 0.2955202\\) while \\(g(0.3) = 0.2955000\\).\nThat the graph of \\({\\cal E}(x)\\) is a straight-line on log-log scales diagnoses \\({\\cal E}(x)\\) as a power law. That is: \\({\\cal E}(x) = A x^p\\). As always for power-law functions, we can estimate the exponent \\(p\\) from the slope of the graph. It’s easy to see that the slope is positive, so \\(p\\) must also be positive.\nThe inevitable consequence of \\({\\cal E}(x)\\) being a power-law function with positive \\(p\\) is that \\(\\lim_{x\\rightarrow 0} {\\cal E}(x) = 0\\). That is, the polynomial approximation \\(x - \\frac{1}{6}x^3\\) is exact as \\(x \\rightarrow 0\\).\nThroughout this book, we’ve been using straight-line approximations to functions around an input \\(x_0\\). \\[g(x) = f(x_0) + \\partial_x f(x_0) [x-x_0]\\] One way to look at \\(g(x)\\) is as a straight-line function. Another way is as a first-order polynomial. This raises the question of what a second-order polynomial approximation should be. Rather than the polynomial matching just the slope of \\(f(x)\\) at \\(x_0\\), we can arrange things so that the second-order polynomial will also match the curvature of the \\(f()\\). Since the curvature involves only the first and second derivatives of a function, the polynomial constructed to match both the first and the second derivative will necessarily match the slope and curvature of \\(f()\\). This can be accomplished by setting the polynomial coefficients appropriately.\nStart with a general, second-order polynomial centered around \\(x_0\\): \\[g(x) \\equiv a_0 + a_1 [x-x_0] + a_2 [x - x_0]^2\\] The first- and second-derivatives, evaluated at \\(x=x_0\\) are: \\[\\partial_x g(x)\\left.{\\Large\\strut}\\right|_{x=x_0} = a_1 + 2 a_2 [x  - x_0] \\left.{\\Large\\strut}\\right|_{x=x_0} = a_1\\] \\[\\partial_{xx} g(x)\\left.{\\Large\\strut}\\right|_{x=x_0} =  2 a_2\\] Notice the 2 in the above expression. When we want to write the coefficient \\(a_2\\) in terms of the second derivative of \\(g()\\), we’ll end up with\n\\[a_2 = \\frac{1}{2} \\partial_{xx} g(x)\\left.{\\Large\\strut}\\right|_{x=x_0}\\]\nTo make \\(g(x)\\) approximate \\(f(x)\\) at \\(x=x_0\\), we need merely set \\[a_1 = \\partial_x f(x)\\left.{\\Large\\strut}\\right|_{x=x_0}\\] and \\[a_2 = \\frac{1}{2} \\partial_{xx} f(x) \\left.{\\Large\\strut}\\right|_{x=x_0}\\] This logic can also be applied to higher-order polynomials. For instance, to match the third derivative of \\(f(x)\\) at \\(x_0\\), set \\[a_3 = \\frac{1}{6} \\partial_{xxx} f(x)  \\left.{\\Large\\strut}\\right|_{x=x_0}\\] Remarkably, each coefficient in the approximating polynomial involves only the corresponding order of derivative. \\(a_1\\) involves only \\(\\partial_x f(x) \\left.{\\Large\\strut}\\right|_{x=x_0}\\); the \\(a_2\\) coefficient involves only \\(\\partial_{xx} f(x) \\left.{\\Large\\strut}\\right|_{x=x_0}\\); the \\(a_3\\) coefficient involves only \\(\\partial_{xx} f(x) \\left.{\\Large\\strut}\\right|_{x=x_0}\\), and so on.\nNow we can explain where the polynomial that started this section, \\(x - \\frac{1}{6} x^3\\) came from and why those coefficients make the polynmomial approximate the sinusoid near \\(x=0\\).\n\n\n\n\n\n\n\n\nOrder\n\\(\\sin(x)\\) derivative\n\\(x - \\frac{1}{6}x^3\\) derivative\n\n\n\n\n0\n\\(\\sin(x) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\\(\\left( 1 - \\frac{1}{6}x^3\\right)\\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\n\n1\n\\(\\cos(x) \\left.{\\Large\\strut}\\right|_{x=0} = 1\\)\n\\(\\left(1 - \\frac{3}{6} x^2\\right) \\left.{\\Large\\strut}\\right|_{x=0}= 1\\)\n\n\n2\n\\(-\\sin(x) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\\(\\left(- \\frac{6}{6} x\\right) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\n\n3\n\\(-\\cos(x) \\left.{\\Large\\strut}\\right|_{x=0} = -1\\)\n\\(- 1\\left.{\\Large\\strut}\\right|_{x=0} = -1\\)\n\n\n4\n\\(\\sin(x) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\\(0\\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\n\n\nThe first four derivatives of \\(x - \\frac{1}{6} x^3\\) exactly match, at \\(x=0\\), the first four derivatives of \\(\\sin(x)\\).\nThe polynomial constructed by matching successive derivatives of a function \\(f(x)\\) at some input \\(x_0\\) is called a Taylor polynomial.\n\nLet’s construct a 3rd-order Taylor polynomial approximation to \\(f(x) = e^x\\) around \\(x=0\\).\nWe know it will be a 3rd order polynomial: \\[g_{\\exp}(x) \\equiv a_0 + a_1 x + a_2 x^2 + a_3 x^3\\] The exponential function is particularly nice for examples because the function value and all it’s derivatives are identical: \\(e^x\\). So \\[f(x= 0) = 1\\]\n\\[ \\partial_x f(x=0) = 1\\] \\[\\partial_{xx} f(x=0) = 1\\] \\[\\partial_{xxx} f(x=0) = 1\\] and so on.\nThe function value and derivatives of \\(g_{\\exp}(x)\\) at \\(x=0\\) are: \\[g_{\\exp}(x=0) = a_0\\] \\[\\partial_{x}g_{\\exp}(x=0) = a_1\\] \\[\\partial_{xx}g_{\\exp}(x=0) = 2 a_2\\] \\[\\partial_{xxx}g_{\\exp}(x=0) = 2\\cdot3\\cdot a_3 = 6\\, a_3\\] Matching these to the exponential evaluated at \\(x=0\\), we get \\[a_0 = 1\\] \\[a_1 = 1\\] \\[a_2 = \\frac{1}{2}\\] \\[a_3 = \\frac{1}{2 \\cdot 3} = \\frac{1}{6}\\]\nResult: the 3rd-order Taylor polynomial approximation to the exponential at \\(x=0\\) is \\[g_{\\exp}(x) = 1 + x + \\frac{1}{2} x^2 +  \\frac{1}{2\\cdot 3} x^3 +\\frac{1}{2\\cdot 3\\cdot 4} x^4\\]\nFigure 27.4 shows the exponential function \\(e^x\\) and its 3th-order Taylor polynomial approximation near \\(x=0\\):\n\n\n\n\n\nFigure 27.4: The 3th-order Taylor polynomial approximation to \\(e^x\\) arount \\(x=0\\)\n\n\n\n\nThe polynomial is exact at \\(x=0\\). The error \\({\\cal E}(x)\\) grows with increasing distance from \\(x=0\\):\n\n\n\n\n\nFigure 27.5: The error from a 3rd-order Taylor polynomial approximation to \\(e^x\\) around \\(x=0\\) is a power-law function with exponent \\(4\\).\n\n\n\n\n\n\n\nFigure 27.6: The error from a 3rd-order Taylor polynomial approximation to \\(e^x\\) around \\(x=0\\) is a power-law function with exponent \\(4\\).\n\n\n\n\nThe plot of \\(\\log_{10} {\\cal E}(x)\\) versus \\(\\log_{10} | x |\\) in ?fig-taylor-exp-5 shows that the error grows from zero at \\(x=0\\) as a power-law function. Measuring the exponent of the power-law from the slope of the graph on log-log axes give \\({\\cal E}(x) = a |x-x_0|^5\\). This is typical of Taylor polynomials: for a polynomial of degree \\(n\\), the error will grow as a power-law with exponent \\(n+1\\). This means that the higher is \\(n\\), the faster \\(\\lim_{x\\rightarrow x_0}{\\cal E}(x) \\rightarrow 0\\). On the other hand, since \\({\\cal E}_x\\) is a power law function, as \\(x\\) gets further from \\(x_0\\) the error grows as \\(\\left(x-x_0\\right)^{n+1}\\).\n\n\nBrooke Taylor (1685-1731), a near contemporary of Newton, published his work on approximating polynomials in 1715. Wikipedia reports: “[T]he importance of [this] remained unrecognized until 1772, when Joseph-Louis Lagrange realized its usefulness and termed it ‘the main [theoretical] foundation of differential calculus’.”Source\n\n\n\n\n\nFigure 27.7: Brook Taylor\n\n\n\n\nDue to the importance of Taylor polynomials in the development of calculus, and their prominence in many calculus textbooks, many students assume their use extends to constructing models from data. They also assume that third- and higher-order monomials are a good basis for modeling data. Both these assumptions are wrong. Least squares is the proper foundation for working with data.\nTaylor’s work preceded by about a century the development of techniques for working with data. One of the pioneers in these new techniques was Carl Friedrich Gauss (1777-1855), after whom the gaussian function is named. Gauss’s techniques are the foundation of an incredibly important statistical method that is ubiquitous today: least squares. Least squares provides an entirely different way to find the coefficients on approximating polynomials (and an infinite variety of other function forms). The R/mosaic fitModel() function for polishing parameter estimates is based on least squares. In Block 5, we’ll explore least squares and the mathematics underlying the calculations of least-squares estimates of parameters."
  },
  {
    "objectID": "Differentiation/26-taylor.html#indeterminate-forms",
    "href": "Differentiation/26-taylor.html#indeterminate-forms",
    "title": "27  Polynomials",
    "section": "27.4 Indeterminate forms",
    "text": "27.4 Indeterminate forms\nLet’s return to an issue that has bedeviled calculus students since Newton’s time. The example we’ll use is the function \\[\\text{sinc}(x)  \\equiv \\frac{\\sin(x)}{x}\\]\nThe sinc() function (pronounced “sink”) is still important today, in part because of its role in converting discrete-time measurements (as in an mp3 recording of sound) into continuous signals.\nWhat is the value of \\(\\text{sinc}(0)\\)? One answer, favored by arithmetic teachers is that \\(\\text{sinc}(0)\\) is meaningless, because it involves division by zero.\nOn the other hand, \\(\\sin(0) = 0\\) as well, so the sinc function evaluated at zero involves 0/0. This quotient is called an indeterminant form. The logic is this: Suppose we assume that \\(0/0 = b\\) for some number \\(b\\). then \\(0 = 0 \\times b = 0\\). So any value of \\(b\\) would do; the value of \\(0/0\\) is “indeterminant.”\nStill another answer is suggested by plotting out sinc(\\(x\\)) near \\(x=0\\) and reading the value off the graph: sinc(0) = 1.\n\nslice_plot(sin(x) / x ~ x, domain(x=c(-10,10)), npts=500)\n\n\n\n\nFigure 27.8: To judge from this plot, \\(\\sin(0) = 1\\).\n\n\n\n\nThe graph of sinc() looks smooth and the shape makes sense. Even if we zoom in very close to \\(x=0\\), the graph continues to look smooth. We call such functions well behaved.\nCompare the well-behaved sinc() to a very closely related function (which doesn’t seem to be so important in applied work): \\(\\frac{\\sin(x)}{x^3}\\).\nBoth \\(\\sin(x)/x\\) and \\(\\sin(x) / x^3\\), evaluated at \\(x=0\\) involve a divide by zero. Both are indeterminate forms 0/0 at \\(x=0\\). But the graph of \\(\\sin(x) / x^3\\) (see ?fig-sinc2) is not we’ll behaved. \\(\\sin(x) / x^3\\) does not have any particular value at \\(x=0\\); instead, it has an asymptote.\n\nslice_plot(sin(x) / x ~ x, domain(x=c(-0.1, 0.1)), npts=500) %>%\n  gf_refine(scale_y_log10())\nslice_plot(sin(x) / x^3 ~ x, domain(x=c(-0.1, 0.1)), npts=500) %>%\n  gf_refine(scale_y_log10())\n\n\n\n\nFigure 27.9: Zooming in around the division by zero. Left: The graph of \\(\\sin(x)/x\\) versus \\(x\\). Right: The graph of \\(\\sin(x)/x^2\\). The vertical scales on the two graphs are utterly different.\n\n\n\n\n\n\n\nFigure 27.10: Zooming in around the division by zero. Left: The graph of \\(\\sin(x)/x\\) versus \\(x\\). Right: The graph of \\(\\sin(x)/x^2\\). The vertical scales on the two graphs are utterly different.\n\n\n\n\nSince both \\(\\sin(x)/x\\left.{\\Large\\strut}\\right|_{x=0}\\) and \\(\\sin(x)/x^3\\left. {\\Large\\strut}\\right|_{x=0}\\) involve a divide-by-zero, the answer to the utterly different behavior of the two functions is not to be found at zero. Instead, it’s to be found near zero. For any non-zero value of \\(x\\), the arithmetic to evaluate the functions is straight-forward. Note that \\(\\sin(x) / x^3\\) starts its mis-behavior away from zero. The slope of \\(\\sin(x) / x^3\\) is very large near \\(x=0\\), while the slope of \\(\\sin(x) / x\\) smoothly approaches zero.\nSince we’re interested in behavior near \\(x=0\\), a useful technique is to approximate the numerator and denominator of both functions by polynomial approximations.\n\n\\(\\sin(x) \\approx x - \\frac{1}{6} x^3\\) near \\(x=0\\)\n\\(x\\) is already a polynomial.\n\\(x^3\\) is already a polynomial.\n\nRemember, these approximations are exact as \\(x\\) goes to zero. So sufficiently close to zero,\n\\[\\frac{\\sin(x)}{x} = \\frac{x - \\frac{1}{6} x^3}{x} = 1 + \\frac{1}{6} x^2\\] Even at \\(x=0\\), there’s nothing indeterminant about \\(1 + x^2/6\\); it’s simply 1.\nCompare this to the polynomial approximation to \\(\\sin(x) / x^3\\): \\[\\frac{\\sin(x)}{x^3} = \\frac{x - \\frac{1}{6} x^3}{x^3} = \\frac{1}{x^2} - \\frac{1}{6}\\]\nEvaluating this at \\(x=0\\) involves division by zero. No wonder it’s badly behaved.\nThe procedure for checking whether a function involving division by zero behaves well or poorly is described in the first-ever calculus textbook, published in 1697. The title (in English) is: The analysis into the infinitely small for the understanding of curved lines. In honor of the author, the Marquis de l’Hospital, the procedure is called l’Hopital’s rule.4\nConventionally, the relationship is written \\[\\lim_{x\\rightarrow x_0} \\frac{u(x)}{v(x)} = \\lim_{x\\rightarrow x_0} \\frac{\\partial_x u(x)}{\\partial_x v(x)}\\]\nLet’s try this out with our two example functions around \\(x=0\\):\n\\[\\lim_{x\\rightarrow 0} \\frac{\\sin(x)}{x} = \\frac{\\lim_{x\\rightarrow 0} \\cos(x)}{\\lim_{x \\rightarrow 0} 1} = \\frac{1}{1} = 1\\]\n\\[\\lim_{x\\rightarrow 0} \\frac{\\sin(x)}{x^3} = \\frac{\\lim_{x\\rightarrow 0} \\cos(x)}{\\lim_{x \\rightarrow 0} 3x^2} = \\frac{1}{0} \\ \\ \\text{indeterminate}!\\]"
  },
  {
    "objectID": "Differentiation/26-taylor.html#computing-with-indeterminate-forms",
    "href": "Differentiation/26-taylor.html#computing-with-indeterminate-forms",
    "title": "27  Polynomials",
    "section": "27.5 Computing with indeterminate forms",
    "text": "27.5 Computing with indeterminate forms\nIn the early days of electronic computers, division by zero would cause a fault in the computer, often signaled by stopping the calculation and printing an error message to some display. This was inconvenient, since programmers did not always forsee division-by-zero situations and avoid them.\nAs you’ve seen, modern computers have adopted a convention that simplifies programming considerably. Instead of stopping the calculation, the computer just carries on normally, but produces as a result one of two indeterminant forms: Inf and NaN.\nInf is the output for the simple case of dividing zero into a non-zero number, for instance:\n\n17/0\n## [1] Inf\n\nNaN, standing for “not a number,” is the output for more challenging cases: dividing zero into zero, or multiplying Inf by zero, or dividing Inf by Inf.\n\n0/0\n## [1] NaN\n0 * Inf\n## [1] NaN\nInf / Inf\n## [1] NaN\n\nThe brilliance of the idea is that any calculation that involves NaN will return a value of NaN. This might seem to get us nowhere. But most programs are built out of other programs, usually written by other people interested in other applications. You can use those programs (mostly) without worrying about the implications of a divide by zero. If it’s important to respond in some particularly way, you can always check the result for being NaN in your own programs. (Much the same is true for Inf, although dividing a non-Inf number by Inf will return 0.)\nPlotting software will often treat NaN values as “don’t plot this.” That’s why it’s possible to make a sensible plot of \\(\\sin(x)/x\\) even when the plotting domain includes zero."
  },
  {
    "objectID": "Differentiation/26-taylor.html#exercises",
    "href": "Differentiation/26-taylor.html#exercises",
    "title": "27  Polynomials",
    "section": "27.6 Exercises",
    "text": "27.6 Exercises"
  },
  {
    "objectID": "Differentiation/26-taylor.html#drill",
    "href": "Differentiation/26-taylor.html#drill",
    "title": "27  Polynomials",
    "section": "27.7 Drill",
    "text": "27.7 Drill\n\n\nPart i Here is a function \\(f(x)\\):  In the Taylor polynomial approximation to \\(f(x)\\) centered at \\(x=-2\\), what will be the sign of the coefficient on the first-order term. Choose the best answer.\n\npositive\nzero\nnegative\nno such coefficient exists in a Taylor polynomial\n\n\n\n\n\nPart ii Here is a function \\(f(x)\\):  In the Taylor polynomial approximation to \\(f(x)\\) centered at \\(x=1\\), what will be the sign of the coefficient on the first-order term. Choose the best answer.\n\npositive\nzero\nnegative\nno such coefficient exists in a Taylor polynomial\n\n\n\n\n\nPart iii Here is a function \\(f(x)\\):  In the Taylor polynomial approximation to \\(f(x)\\) centered at \\(x=1\\), what will be the sign of the coefficient on the second-order term. Choose the best answer.\n\npositive\nzero\nnegative\nno such coefficient exists in a Taylor polynomial\n\n\n\n\n\nPart iv Here is a function \\(f(x)\\):  In the Taylor polynomial approximation to \\(f(x)\\) centered at \\(x=-4\\), what will be the sign of the coefficient on the first-order term. Choose the best answer.\n\npositive\nzero\nnegative\nno such coefficient exists in a Taylor polynomial\n\n\n\n\n\nPart v Here is a function \\(f(x)\\):  In the Taylor polynomial approximation to \\(f(x)\\) centered at \\(x=4\\), what will be the sign of the coefficient on the second-order term. Choose the best answer.\n\npositive\nzero\nnegative\nno such coefficient exists in a Taylor polynomial\n\n\n\n\n\nPart vi Here is a function \\(f(x)\\):  In the Taylor polynomial approximation to \\(f(x)\\) centered at \\(x=4\\), what will be the sign of the coefficient on the constant (zeroth-order) term. Choose the best answer.\n\npositive\nzero\nnegative\nno such coefficient exists in a Taylor polynomial\n\n\n\n\n\nPart vii Here is a function \\(f(x)\\):  In the Taylor polynomial approximation to \\(f(x)\\) centered at \\(x=3\\), what will be the sign of the coefficient on the second-order term. Choose the best answer.\n\npositive\nzero\nnegative\nno such term exists in a Taylor polynomial\n\n\n\n\n\nPart viii Here is a function \\(f(x)\\):  In the Taylor polynomial approximation to \\(f(x)\\) centered at \\(x=0\\), what will be the sign of the coefficient on the reciprocal term. Choose the best answer.\n\npositive\nzero\nnegative\nno such term exists in a Taylor polynomial\n\n\n\n\n\nPart ix Here is a function \\(g(x)\\):  Two Taylor polynomials, centered on the same \\(x\\) are shown. One is fifth-order, the other is third-order. Which is which?\n\nThe third-order polynomial is brown.\nThe third-order polynomial is magenta.\n\n\n\n\n\nPart x Here is a function \\(g(x)\\):  with a Taylor polynomial shown in magenta. What is the order of the polynomial?\nzeroonetwothreefour"
  },
  {
    "objectID": "Linear-combinations/B5-Vectors.html",
    "href": "Linear-combinations/B5-Vectors.html",
    "title": "28  Vectors",
    "section": "",
    "text": "Until now, our presentation of calculus has featured functions, sometimes expressed as formulas involving combinations of the basic modeling functions, sometimes generated directly from data by splines. Now we turn to a new framework for expressing functions, the inputs on which they operate, and the kind of outputs they generate.\nThis framework is central to technical work in a huge range of fields. The usual name given to it by mathematicians is linear algebra, although only the word “linear” conveys useful information about the subject. The physicists developing the first workable quantum theory called it matrix mechanics. The framework is fundamental to scientific computation and is often the approach of choice even to non-linear problems. Application of the framework to problems of information access was the spark that ignited the modern era of search engines.\nAlthough the words “algebra” and “quantum” may suggest that conceptual difficulties are in store, in fact human intuition is well suited to establishing a useful understanding. We will use two formats to introduce linear algebra: (1) geometric and visual; (2) via simple arithmetic, numbers, and algorithms."
  },
  {
    "objectID": "Linear-combinations/B5-Vectors.html#length-direction",
    "href": "Linear-combinations/B5-Vectors.html#length-direction",
    "title": "28  Vectors",
    "section": "28.1 Length & direction",
    "text": "28.1 Length & direction\nA vector is a mathematical idea that is deeply rooted in everyday physical experience. Geometrially, a vector is simply an object consisting only of length and direction.\nA pencil is a good physical metaphor for a vector, but a pencil has other, non-vector qualities such as diameter, color, and an eraser. And, being a physical object, a pencil always has position: the place it’s at.\n\n\n\n\n\nFigure 28.1: Three pencils, but just two vectors. The yellow and blue pencils have the same length and direction, so they are the same vector. Pencils have position, but vectors don’t. The green pencil shares the same direction, but it has a different length, so it is a different vector from the blue/yellow vector.\n\n\n\n\nYou can move in a given direction either forward or in reverse. To eliminate this ambiguity, it’s helpful to imagine vectors having a tip and a tail. For the pencil illustrations, the writing end it the tip and the eraser is the tail.\n\n\n\n\n\nFigure 28.2: Two different vectors. They have the same length and are parallel, but they point in opposite directions.\n\n\n\n\nVectors are always embedded in a vector space. Our physical stand-ins for vectors, the pencils, were photographed on a table top: a two-dimensional space. Naturally, the pencil-vectors are also embedded in our everyday three-dimensional space. The table-top can be thought of as a representation of a two-dimensional subspace of three-dimensional space.\nOften, we will use vectors to represent a change in position, that is, a step or displacement in the sense of “step to the left” or “step forward.” An individual vector describes a step of a specific length in a particular direction. Much of the useful mathematics of vectors can be understood as constructing instructions for reaching a target: “take three and a half steps along the green vector, then turn and take two steps backwards along the yellow vector.”\nVectors embedded in three-dimensional space are central to physics and engineering. Quantities such as force, acceleration, and velocity are properly represented not as simple numerical quantities but as vectors with magnitude (that is, length) and direction. The statement, “The plane’s velocity is 450 miles per hour to the north-north-west” is perfectly intelligible to most people, describing magnitude and direction. Note that the vector velocity can be understood without having to know where the plane is located; vectors have only the two qualities of magnitude and direction. Position is irrelevant to describing velocity, or, for that matter, force or acceleration.\nThe gradients that we studied with partial differentiation (Chapter 24) are vectors. A gradient’s direction points directly uphill from a given point; it’s magnitude tells how steep the hill is at that point.\nVectors are a practical tool in many situations such as relative motion. Consider the problem of finding an aircraft heading and speed to intercept another plane that’s also moving. The US Navy training movie from the 1950s shows how such calculations used to be done with paper and pencil.\n\n\nNowadays such relative motion calculations are computerized. You may well wonder how the computer is able to represent vectors, since pencils aren’t part of computer hardware. The answer is disappointingly simple: the properties of direction and magnitude can also be represented by a set of numbers. Two numbers will do for a vector embedded in two-dimensional space, three for a vector embedded in three-dimensional space.\nRepresenting a vector as a set of numbers requires the imposition of a framework: a coordinate system. In Figure 28.3, the vector (that is, the green pencil) has been placed in a coordinate system. Usually you would expect there to be labels for each of the coordinate lines, but this labeling is not necessarily to show a vector (even if it is needed to specify a position). The two coordinates to be assigned to the vector are the difference between the tip and the tail. In the figure, there are 20 units horizontally and 16 units vertically, so the vector is \\((20, 16)\\).\n\n\n\n\n\nFigure 28.3: Representing a vector as a set of numbers requires reference to a coordinate system, shown here as graph paper.\n\n\n\n\nBy convention, when we write a vector as a set of coordinate numbers, we write the numbers in a column. For instance, the vector in Figure @ref(fig:vector-graph-paper), which we’ll call \\(\\vec{green}\\), is written numerically as:\n\\[\\vec{green} \\equiv \\left[\\begin{array}{c}20\\\\16\\end{array}\\right]\\] In more advanced linear algebra, the distinction between a column vector (like \\(\\vec{green}\\)) and a row vector (like \\(\\left[20 \\ 16\\right]\\)) is important. For our purposes in this block, we will only have need of column vectors.\n\nColumn vectors can be constructed with the rbind() function, as in\n\nrbind(1,3,-4)\n##      [,1]\n## [1,]    1\n## [2,]    3\n## [3,]   -4\n\nNote that the elements are separated by commas in the same way as any other R function.\nLater in this block, we will be using data frames to define vectors. We’ll introduce the R syntax for that when we need it.\n\n\nIf you have previous experience with R, say in a statistics course, or if you regard yourself as an expert, please note the following. The R language has a data structure called a “vector,” which is a set of elements without the information needed to consider it a row or column vector. As such, the native R “vector” is not suited for linear-algebra computations in R.\nMany people construct mathematical vectors using the matrix() function. Such a matrix() command to produce a column vector would look like matrix(c(1, 3, -4), ncol=1). This is a professional practice, but we regard it as too verbose for our purposes in this book. We will use rbind() instead. Note that combining rbind() with c() or other preconstructed R “vectors” will not produce a mathematical row vector\n\nrbind(c(1,2,3))\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n\nrbind(1:5)\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    2    3    4    5\n\nEventually, we will be constructing matrices. We’ll do this by concatenating column vectors, e.g.\n\ncbind(\n    rbind( 1,  2,  3),\n    rbind(10, 11, 12)\n)\n##      [,1] [,2]\n## [1,]    1   10\n## [2,]    2   11\n## [3,]    3   12\n\n\nIn physics and engineering vectors are used to describe positions, velocities, acceleration, forces, momentum, and other such functions of time or space. In mathematical notation, such a velocity can be written \\(\\vec{v}(t)\\). It’s common to perform calculus operations such a differentiation, writing it as \\(\\partial_t \\vec{v}(t)\\).\nThe vector-valued function of time \\(\\vec{v}(t)\\) can also be written in terms of scalar-valued components assembled into a vector. For instance, a subscript is often used to identify which component is which, so that \\[\\vec{v}(t) = \\left[\\begin{array}{c}v_x(t)\\\\v_y(t)\\\\v_z(t)\\end{array}\\right]\\] where the \\(x\\), \\(y\\), and \\(z\\) refer to the axes of the coordinate system.\nSince the physics and engineering vectors are typically 2- or 3-dimensional, when working numerically there’s not much lost by keeping track of the components with a set of scalar quantities rather than as a vector. You saw this already in Chapter 33 when we represented the instantaneous vector position of a robot arm as a pair of scalar-valued functions \\(x(t)\\) and \\(y(t)\\).\nIn linear algebra, vectors often have many more than 3 components. In this book, we will work with vectors with hundreds of components. Services like Google search rely on vector calculations with millions of components. When programming such systems, representing the vectors as individual scalar components is unwieldy. The programming must rely on handling the whole vector as a single entity."
  },
  {
    "objectID": "Linear-combinations/B5-Vectors.html#the-nth-dimension",
    "href": "Linear-combinations/B5-Vectors.html#the-nth-dimension",
    "title": "28  Vectors",
    "section": "28.2 The nth dimension",
    "text": "28.2 The nth dimension\nLiving as we do in a palpably three-dimensional space, and being part of a species whose senses and brains developed in three dimensions, it’s hard and maybe even impossible to get a grasp on what higher-dimensional spaces would be like.\nA lovely 1884 book, Flatland features the inhabitants of a two-dimensional world. The central character, Square, receives a visitor, Sphere, from the three-dimensional world in which Flatland is embedded. Only with difficulty can Square assemble a conception of Sphere from the appearing, growing, and vanishing of Sphere’s intersection with the flat world. Square’s attempt to convince Sphere that his three-dimensional world might be embedded in a four-dimensional one leads to rejection and disgrace.\n\n\n\n\n\n\n\n\n\nEven if the spatial extent of higher dimensions is not accessible, the one-dimensional vector inhabitants of any such space can be readily perceived and constructed as a list of numbers. With this device, allow us to introduce vectors from 4, 5, and 6 dimensions, and even \\(n\\) dimensional space.\n\\[\\left[\\begin{array}{r}6.4\\\\3.0\\\\-2.5\\\\17.3\\end{array}\\right]\\ \\ \\ \\left[\\begin{array}{r}-14.2\\\\-6.9\\\\18.0\\\\1.5\\\\-0.3\\end{array}\\right]\\ \\ \\ \\left[\\begin{array}{r}5.3\\\\-9.6\\\\84.1\\\\5.7\\\\-11.3\\\\4.8\\end{array}\\right]\\ \\ \\ \\cdots\\ \\ \\ \\left.\\left[\\begin{array}{r}7.2\\\\-4.4\\\\0.6\\\\-4.1\\\\4.7\\\\\\vdots\\ \\ \\\\-7.3\\\\8.3\\end{array}\\right]\\right\\} n\\]\nSensible people may consider it mathematical ostentation to promote an everyday column of numbers into a vector in high-dimensional space. The utility of doing so is to help us think about the arithmetic we are about to do on vectors in terms of familiar geometrical concepts: lengths, angles, alignment, and so on. Perhaps unexpectedly, it also guides us to think about data—which consists of columns of numbers in a data frame—using our powerful geometrical intuition.\nThere’s nothing science-fiction-like about so-called “high-dimensional” spaces; they are not usually intended to correspond to a physical space. Vectors with many components are often used in advanced physics to represent the state of a particle. For instance, the state vector could contain both the position and velocity and might be written \\((x, y, z, v_x, v_y, v_z)\\), but you can easily see this as the concatenation of a position vector and a velocity vector, each of which is 3-dimensional. In statistics, engineering, and statistical mechanics, the term degrees of freedom is used as an alternative to “dimension.” Another example: computer-controlled machine tools are often described as having 5 degrees of freedom (or more). There is a cutting head whose \\(x, y, z\\) position can be set as well as the head’s orientation (tilt) as an azimuth and inclination. If ever you start to freak out about the idea of a 10-dimensional space, just close your eyes and remember that this is only shorthand for the set of arrays with 10 elements."
  },
  {
    "objectID": "Linear-combinations/B5-Vectors.html#geometry-arithmetic",
    "href": "Linear-combinations/B5-Vectors.html#geometry-arithmetic",
    "title": "28  Vectors",
    "section": "28.3 Geometry & arithmetic",
    "text": "28.3 Geometry & arithmetic\nThere are three common mathematical tasks involving vectors that can be understood with simple geometry. Given a set of vectors drawn on paper, you can carry out these tasks by pencil assisted by a simple ruler and a protractor.\n\nMeasure the length of a vector.\nMeasure the angle between two vectors.\nCreate a new vector by scaling a vector. Scaling makes the new vector longer or shorter or point in the opposite orientation, but the direction remains identical to the original.\n\nThe geometrical perspective is helpful for many purposes, but often we need to work with vectors using computers. For this, we use the numerical representation of vectors.\nThis section introduces the arithmetic of vectors. With this arithmetic in hand, we can carry out the above tasks (and more!) on vectors that consist of a column of numbers. Especially noteworthy is that the arithmetic enables to to apply simple geometrical concepts to vectors in three or more dimensions.\nTo scale a vector \\(\\vec{w}\\) means more or less to change the vector’s length. A good mental image for scaling is based on thinking about the vector as a step or displacement in the direction of \\(\\vec{w}\\). Scaling means to go on a simple walk, taking one step after the other in the same direction as the \\(\\vec{w}\\). We write a scaled vector by placing a number in front of the name of the vector. \\(3 \\vec{w}\\) is a short walk of three steps; \\(117 \\vec{w}\\) is a considerably longer walk; \\(-5 \\vec{w}\\) means to take five steps backwards. You can also take fraction steps: \\(0.5 \\vec{w}\\) is half a step, \\(19.3 \\vec{w}\\) means to take 19 steps followed by a 30% step. Scaling a vector by \\(-1\\) means flipping the vector tip-for-tail; this doesn’t change the length, just the orientation.\nArithmetically, scaling a vector is accomplished simply by multiplying each of the vector’s components by the same number. Suppose that we are working with a vector \\(\\vec{v}\\) that has \\(n\\) components. (We’ll also define another vector \\(\\vec{w}\\) to use in examples.)\n\\[\\vec{v} \\equiv \\left[\\begin{array}{r}6\\\\2\\\\-4\\\\\\vdots\\\\1\\\\8\\end{array}\\right]\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vec{w} \\equiv \\left[\\begin{array}{r}-3\\\\1\\\\-5\\\\\\vdots\\\\2\\\\5\\end{array}\\right]\\] To scale a vector by 3 is accomplished by multiplying each component by 3\n\\[3\\, \\vec{v} = 3\\left[\\begin{array}{r}6\\\\2\\\\-4\\\\\\vdots\\\\1\\\\8\\end{array}\\right] = \\left[\\begin{array}{r}18\\\\6\\\\-12\\\\\\vdots\\\\3\\\\24\\end{array}\\right]\\] This is perfectly ordinary multiplication applied component by component, that is, componentwise.\nScaling involves a number (the “scalar”) and a single vector. There are other sorts of multiplication however, that involve two or more vectors.\nThe dot product is one approach to multiplication of one vector with another. The dot product between \\(\\vec{v}\\) and \\(\\vec{w}\\) is written \\[\\vec{v} \\bullet \\vec{w}\\].\nThe arithmetic of the dot product involves two steps:\n\nMultiply the two vectors component-wise. For instance: \\[\\underset{\\Large \\vec{v}}{\\left[\\begin{array}{r}6\\\\2\\\\-4\\\\\\vdots\\\\1\\\\8\\end{array}\\right]}\\  \\underset{\\Large \\vec{w}}{\\left[\\begin{array}{r}-3\\\\1\\\\-5\\\\\\vdots\\\\2\\\\5\\end{array}\\right]} = \\left[\\begin{array}{r}-18\\\\2\\\\20\\\\\\vdots\\\\2\\\\40 \\end{array}\\right]\\]\nSum the elements in the component-wise product. For the component-wise product of \\(\\vec{v}\\) and \\(\\vec{w}\\), this will be \\(-18 + 2 + 20 + \\cdots +2 + 40\\). The resulting sum, which is an ordinary quantity, that is, a scalar, is the output of the dot product. That is, the dot product takes two vectors as inputs and produces a scalar as an output.\n\n\nR/mosaic provides a beginner-friendly function for computing a dot product. To mimic the use of the dot, as in \\(\\vec{v} \\bullet \\vec{w}\\), the function will be invoked using infix notation. You have a huge amount of experience with infix notation, even if you never heard the term. Some examples:\n3 + 2       7 / 4      6 - 2      9 * 3     2 ^ 4\nInfix notation is distinct from the functional notation that you are also familiar with, for instance sin(2) or makeFun(x^2 ~ x).\nIn principle, you could invoke the +, -, *, /, and ^ operations using functional notation. Nobody does this because the commands are so ugly:\n\n\n`+`(3, 2)\n\n\n## [1] 5\n\n\n\n`/`(7, 4)\n\n\n## [1] 1.75\n\n\n\n`-`(6, 2)\n\n\n## [1] 4\n\n\n\n`*`(9, 3)\n\n\n## [1] 27\n\n\n\n`^`(2, 4)\n\n\n## [1] 16\n\n\n\n \nThe R language makes it possible to define new infix operators, but there is a catch. The new operators must always have a name that begins and ends with the % symbol, for example %in% or %*% or %dot%. You’ll be using %*% and %dot% a lot in this block and the next.\nHere’s an example of using %dot% to calculate the dot product of two vectors:\n\na <- rbind(1, 2, 3, 5, 8, 13)\nb <- rbind(1, 4, 2, 3, 2, -1)\na %dot% b\n## [1] 33\n\nThe vectors being combined with %dot% must both have the same number of elements. Otherwise, an error message will result:\n\nrbind(2, 1) %dot% rbind(3, 4, 5)\n## Error in rbind(2, 1) %dot% rbind(3, 4, 5): Vector <u> must have the same number of elements as vector <b>.\n\n\nTo the student encountering the dot product for the first time, a natural response is to wonder what such a two-step operation might be good for. As we progress through this block, you’ll see the dot product playing a central role.\n\nYou will be seeing a lot of the dot product, so it’s important to have it firmly in mind that a dot product is not ordinary multiplication."
  },
  {
    "objectID": "Linear-combinations/B5-Vectors.html#sec-vector-length",
    "href": "Linear-combinations/B5-Vectors.html#sec-vector-length",
    "title": "28  Vectors",
    "section": "28.4 Vector lengths",
    "text": "28.4 Vector lengths\nThe arithmetic used to calculate the length of a vector is based on the Pythagorean theorem. For a vector \\(\\vec{u} = \\left[\\begin{array}{c}4\\\\3\\end{array}\\right]\\) the vector is the hypotenuse of a right triangle with legs of length 4 and 3 respectively. Therefore, \\[\\|\\vec{u}\\| = \\sqrt{4^2 + 3^2} = 5\\ .\\] For vectors with more than two components, follow the same pattern: sum the squares of the components then take the square root.\nThe length of a vector \\(\\vec{u}\\) can also be computed using the dot product: \\[\\|\\vec{u}\\| = \\sqrt{\\strut\\vec{u} \\bullet \\vec{u}}\\ .\\] Although length has an obvious physical interpretation, in many areas of science including statistics and quantum physics, the square length is a more fundamental quantity. The square length of \\(\\vec{u}\\) is simply \\(\\|\\vec{u}\\|^2 = \\vec{u}\\bullet \\vec{u}\\).\n\nConsider the two vectors \\[\\vec{u} \\equiv \\left(\\begin{array}{c}3\\\\4\\end{array}\\right) \\  \\  \\ \\mbox{and}  \\ \\ \\ \\vec{w} \\equiv \\left(\\begin{array}{c}1\\\\1\\\\1\\\\1\\end{array}\\right)\n\\]\nThe length of \\(\\vec{u}\\) is \\(|| \\vec{u} || = \\sqrt{\\strut 3^2 + 4^2} = \\sqrt{\\strut 25} = 5\\).\nThe length of \\(\\vec{w}\\) is \\(|| \\vec{w} || = \\sqrt{\\strut 1^2 + 1^2 + 1^2 + 1^2} = \\sqrt{\\strut 4} = 2\\).\n\n\nIn statistics, the many applications of linear algebra often involve a simple constant vector, which we’ll write \\(\\vec{1}\\). It is simply a column vector of 1s, \\[\\vec{1} \\equiv \\left[\\begin{array}{c}1\\\\1\\\\1\\\\\\vdots\\\\1\\\\1\\\\ \\end{array}\\right]\\ .\\] Common statistical calculations can be expressed compactly in vector notation. For example, if \\(\\vec{x}\\) is an \\(n\\)-dimensional vector, then the mean of the components of \\(\\vec{x}\\), which is often written \\(\\bar{x}\\), is \\[\\bar{x} \\equiv \\frac{1}{n}\\  \\vec{x} \\bullet \\vec{1}\\ .\\] The symbol \\(\\bar{}\\) is pronounced “bar”, and \\(\\bar{x}\\) is pronounced “x-bar.”.\nAnother commonly used statistic is the variance of the components of a vector \\(\\vec{x}\\). This is only slightly more complicated than the mean: \\[\\text{var}(x) \\equiv \\frac{1}{n-1}\\  (\\vec{x} - \\bar{x}) \\bullet (\\vec{x} - \\bar{x})\\ .\\] The quantity \\(\\vec{x} - \\bar{x}\\) is an example of scalar subtraction, which is done on a component-wise basis. For instance, with \\[\\vec{x} = \\left[\\begin{array}{r}1\\\\2\\\\3\\\\4\\\\\\end{array}\\right]\\] then \\(\\bar{x} = 2.5\\). This being the case, \\[\\vec{x} - \\bar{x} = \\left[\\begin{array}{c}-1.5\\\\-0.5\\\\\\ 0.5\\\\\\ 1.5\\\\\\end{array}\\right]\\ ,\\] with the variance of \\(\\vec{x}\\) being \\[\\frac{1}{4-1} \\left[\\begin{array}{r}-1.5\\\\-0.5\\\\0.5\\\\1.5\\\\\\end{array}\\right] \\bullet \\left[\\begin{array}{r}-1.5\\\\-0.5\\\\0.5\\\\1.5\\\\\\end{array}\\right] = \\frac{5}{3}\\ .\\]"
  },
  {
    "objectID": "Linear-combinations/B5-Vectors.html#angles",
    "href": "Linear-combinations/B5-Vectors.html#angles",
    "title": "28  Vectors",
    "section": "28.5 Angles",
    "text": "28.5 Angles\nAny two vectors of the same dimension have a distinct angle between them. This is easily seen for two-dimensional vectors. Draw two vectors on a sheet of paper. Since vectors have only two properties, length and direction, in your mind’s eye you can pick up one of the vectors and relocate its “tail” to meet the tail of the other vector.\nMeasure the angle between two vectors the short way round: between 0 and 180 degrees. Any larger angle, say 260 degrees, will be identified with its circular complement: 100 degrees is the complement of a 260 degree angle.\nIn 2- and 3-dimensional spaces, we can measure the angle between two vectors using a protractor: arrange the vectors so they are tail to tail, align the baseline of the protractor with one of the vectors and read off the angle marked by the second vector.\nIt’s also possible to measure the angle using arithmetic. Suppose we have vectors \\(\\vec{v}\\) and \\(\\vec{w}\\) that are in the same dimensional space. That is, \\(\\vec{v}\\) and \\(\\vec{w}\\) have the same number of components:\n\\[\\vec{v} = \\left[\\begin{array}{c}v_1\\\\v_2\\\\\\vdots\\\\v_n\\\\\\end{array}\\right]\\ \\ \\ \\text{and}\\ \\ \\ \\vec{w} = \\left[\\begin{array}{c}w_1\\\\w_2\\\\\\vdots\\\\w_n\\\\\\end{array}\\right]\\ ,\\]\nUsing the dot-product and length notation, we can write the formula for the cosine of the angle between two vectors as \\[\\cos(\\theta) \\equiv \\frac{\\vec{v}\\cdot\\vec{w}}{\\|\\vec{v}\\|\\ \\|\\vec{w}\\|}\\ .\\]\n\nRemember that the dot-product-based formula above gives the cosine of the angle between the two vectors. It turns out that in many applications, the cosine is exactly what’s needed. If you insist on knowing the angle \\(\\theta\\) rather than \\(\\cos(\\theta)\\), the trigonometric function \\(\\arccos()\\) will do the job. For instance, if \\(\\theta\\) is such that \\(\\cos(\\theta) = 0.6\\), compute the angle in degrees with ::: {.cell layout-align=“center” fig.showtext=‘false’}\nacos(0.6)*180/pi\n## [1] 53.1301\n\nThe trigonometric functions in R (and in most other languages) do calculations with angles in units of radians. The 180/pi is the factor that converts radians to degrees. Figure @ref(fig:cosine-conversion) shows a graph of converting \\(\\cos(\\theta)\\) to \\(\\theta\\) in degrees.\n\n\n\n\n\nFigure 28.4: The \\(\\arccos()\\) function (acos() in R) converts \\(\\cos(\\theta)\\) to \\(\\theta\\).\n\n\n\n\n:::\n\nWhat does the angle \\(\\theta\\) between two vectors tell us?\nIn geometrical terms, the angle tells us how strongly aligned the vectors are. An angle of 0 tells us the vectors point in the same direction, and angle of 180 degrees means that the vectors point in exactly opposing directions. Either of these—0 or 180 degrees—indicates that the two vectors are perfectly aligned. Such alignment means that by appropriate scalar multiplication, the two vectors could be made exactly equal to one another and, consequently, that the scaled vectors would be one and the same.\nAngles such as 5 or 175 degrees indicate that the two vectors are mostly aligned, but imperfectly. When the angle is 90 degrees of course—a right angle—the two vectors are perpendicular.\nThe vector alignment has a particularly important meaning in terms of data. Suppose the two vectors are two columns in a data frame: two different variables. In statistics there is an important quantity called the correlation coefficient, denoted \\(r\\). To say that two variables are correlated means that the variables are connected to one another in some way. For instance, among children, height and age are correlated. Since height tends to increase along with age (for children), the two variables are said to be positively correlated. The largest possible correlation is \\(r=1\\).\nA negative correlation means that as one variable increases the other tends to decrease. Temperature and elevation are negatively correlated, as are the pressure and volume of a gas at a given temperature. The most negative possible correlation is \\(r=-1\\).\nA zero correlation indicates that there is no simple relationship between the two variables. This occurs when the variables are orthogonal, a term described in Section 28.6.\nIn terms of vectors, that is, the columns in the data frame, the correlation coefficient \\(r\\) is the same quantity as the cosine of the angle between the vectors. At the time the correlation coefficient was invented in the 1880s, it was not widely appreciated that \\(r\\) is simply the cosine of an angle. Perhaps the several generations of statistics students who have studied correlation would have had a better grasp on the subject if it had been called alignment and measured in degrees."
  },
  {
    "objectID": "Linear-combinations/B5-Vectors.html#sec-orthogonality",
    "href": "Linear-combinations/B5-Vectors.html#sec-orthogonality",
    "title": "28  Vectors",
    "section": "28.6 Orthogonality",
    "text": "28.6 Orthogonality\nTwo vectors are said to be orthogonal when the angle between them is 90 degrees. In everyday speech we call a 90 degree angle a “right angle.” The word “orthogonal” is really just a literal translation of “right angle.” (The syllable “gon” indicates an angle, as in the five-angled pentagon or six angled hexagon. “Ortho” means “right” or “correct,” as in “orthodox” (right beliefs) or “orthodontics” (right teeth) or “orthopedic” (right feet).)\nTwo vectors are at right angles—we prefer “orthogonal” since “right” has many meanings not related to angles—when the dot product between them is zero.\n\n\n \n\nFind a vector that is orthogonal to \\(\\left[\\strut\\begin{array}{r}1\\\\2\\end{array}\\right]\\).\nThe arithmetic trick is to reverse the order of the components and put a minus sign in front of one of them, so \\(\\left[\\strut\\begin{array}{r}-2\\\\1\\end{array}\\right]\\).\nWe can confirm the orthogonality by calculating the dot product: \\(\\left[\\begin{array}{c}-2\\\\\\ 1\\end{array}\\right] \\cdot \\left[\\strut\\begin{array}{r}1\\\\2\\end{array}\\right] = -2\\times1 + 1 \\times 2 = 0\\).\nIn R, this can be written\n\nu <- rbind( 1, 2)\nv <- rbind(-2, 1)\nu %dot% v\n## [1] 0\n\n\n\n\n \n\nFind a vector orthogonal to \\(\\left[\\strut\\begin{array}{r}1\\\\2\\\\3\\end{array}\\right]\\).\nWe have a little more scope here. A simple approach is to insert a zero component in the new vector and then use the two-dimensional trick to fill in the remaining components.\nFor instance, starting with \\(\\left[\\strut\\begin{array}{r}0\\\\\\_\\\\ \\_\\end{array}\\right]\\) the only non-zero components of the dot product will involve the 2 and 3 of the original vector. So \\(\\left[\\strut\\begin{array}{r}0\\\\ -3\\\\ 2\\end{array}\\right]\\) is orthogonal. Or, if we start with \\(\\left[\\strut\\begin{array}{r}\\_\\\\0\\\\\\_\\end{array}\\right]\\) we would construct \\(\\left[\\strut\\begin{array}{r}-3\\\\ 0\\\\ 1\\end{array}\\right]\\)."
  },
  {
    "objectID": "Linear-combinations/B5-Vectors.html#exercises",
    "href": "Linear-combinations/B5-Vectors.html#exercises",
    "title": "28  Vectors",
    "section": "28.7 Exercises",
    "text": "28.7 Exercises"
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html",
    "href": "Linear-combinations/B5-linear-combinations.html",
    "title": "29  Linear combinations of vectors",
    "section": "",
    "text": "In this chapter, we introduce linear combinations of vectors. As you recall, a linear combination is a sum of basic elements each of which has been scaled. For instance, in Block 1 we looked at linear combinations of functions such as \\[g(t) = A + B e^{kt}\\] which involves the basic functions \\(\\text{one}(t)\\) and \\(e^{kt}\\) scaled respectively by \\(A\\) and \\(B\\).\nLinear combinations of vectors involve scaling and addition, which are simple seen either as numerical operations or a geometric ones. A useful concept will be the set of all vectors that can be constructed as linear combinations of given vectors. This set of all possibilities, called the subspace spanned by the given vectors is key to understanding how to find the “best” scalars for a given purpose."
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#scaling-vectors",
    "href": "Linear-combinations/B5-linear-combinations.html#scaling-vectors",
    "title": "29  Linear combinations of vectors",
    "section": "29.1 Scaling vectors",
    "text": "29.1 Scaling vectors\nTo scale a vector means to change its length without altering its direction. Scaling by a negative number flips the vector tip-for-tail. Figure 29.1 shows two vectors \\(\\vec{v}\\) and \\(\\vec{w}\\) together with several scaled versions of each.\n\n\n\n\n\nFigure 29.1: Vectors \\(\\vec{v}\\) and \\(\\vec{w}\\) and some scaled versions of them.\n\n\n\n\nThe vectors we create by scalar multiplication can be placed anywhere we like.\nArithmetically, scaling a vector is accomplished by multiplying each component of the vector by the scalar, e.g.\n\\[\\vec{u} = \\left[\\begin{array}{r}1.5\\\\-1\\end{array}\\right]\\ \\ \\ \\ 2\\vec{u} = \\left[\\begin{array}{r}3\\\\-2\\end{array}\\right]\\ \\ \\ \\\n-\\frac{1}{2}\\vec{u} = \\left[\\begin{array}{r}-0.75\\\\0.5\\end{array}\\right]\\ \\ \\ \\ \\]\nEvery vector is associated with a subspace that is one-dimensional; you can only reach the points on a line by stepping in the direction of a vector."
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#adding-vectors",
    "href": "Linear-combinations/B5-linear-combinations.html#adding-vectors",
    "title": "29  Linear combinations of vectors",
    "section": "29.2 Adding vectors",
    "text": "29.2 Adding vectors\nTo add two vectors, choose either one of the vectors as a start, then move the tail of the second vector to the tip of the first, as in Figure 29.2.\n\n\n\n\n\nFigure 29.2: Adding two vectors, yellow and green, by placing them tail to tip. The result is the vector going from the tail of yellow to the tip of green. This resultant is equivalent to the blue vector.\n\n\n\n\nAdding vectors in this way takes advantage of the rootlessness of a vector. So long as we keep the direction and length the same, we can move a vector to whatever place is convenient. For adding vectors, the convenient arrangement is to place the tail of the second vector at the tip of the first. The result—the blue pencil in the picture above—has the length and direction from the tail of the first pencil (yellow) to the tip of the second (green). But so long as we maintain this length and direction, we can put the result (blue) anywhere we want.\nArithmetically, vector addition is simply a matter of applying addition component-by-component, that is, componentwise. For instance, consider adding two vectors \\(\\vec{v}\\) and \\(\\vec{w}\\):\nTHIS MATH MARKUP DOESN’T COMPILE TO PDF\n<!--\\underbrace{\\left[\\begin{array}{r}1.5\\\\-1\\\\2\\\\6\\end{array}\\right]}_\\vec{v} + \\underbrace{\\left[\\begin{array}{r}2\\\\4\\\\-2\\\\-3.2\\end{array}\\right]}_\\vec{w} = \\underbrace{\\left[\\begin{array}{r}3.5\\\\3\\\\0\\\\2.8\\end{array}\\right]}_{\\vec{v} + \\vec{w}}-->\nUnlike our pencil exemplars of vectors, which must of physical necessity always be in the three-dimensional space we inhabit, mathematical vectors can be embedded in any-dimensional space. Addition is applicable to vectors embedded in the same space. Arithmetically, this means that the two vectors to be added must have the same number of components.\nArithmetic subtraction of one vector from another is a simple component-wise operation. For example:\n$$\\underbrace{\\left[\\begin{array}{r}1.5\\\\-1\\\\2\\\\6\\end{array}\\right]}_\\vec{v} {\\Large -} \\underbrace{\\left[\\begin{array}{r}2\\\\4\\\\-2\\\\-3.2\\end{array}\\right]}_\\vec{w} = \\underbrace{\\left[\\begin{array}{r}-0.5\\\\-5\\\\4\\\\9.2\\end{array}\\right]}_{\\vec{v} - \\vec{w}}\\ .$$\nFrom a geometrical point of view, many people like to think of \\(\\vec{v} - \\vec{w}\\) in terms of placing the two vectors tail to tail as in Figure 29.3. Read out the result as the vector running from the tip of \\(\\vec{v}\\) to the tip of \\(\\vec{w}\\). In Figure 29.3, the yellow vector is \\(\\vec{v}\\), the blue vector is \\(\\vec{w}\\). The result of the subtraction is the green vector.\n\n\n\n\n\nFigure 29.3: Subtracting blue from yellow gives green."
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#linear-combinations",
    "href": "Linear-combinations/B5-linear-combinations.html#linear-combinations",
    "title": "29  Linear combinations of vectors",
    "section": "29.3 Linear combinations",
    "text": "29.3 Linear combinations\nIn the previous chapter, we suggested that you think of a vector as a “step” or displacement in a given direction and of a given magnitude as in, “1 foot to the northeast.” This interpretation highlights the mathematical structure of vectors: just a direction and a length, nothing else.\nThe “step”-interpretation is also faithful to an important reason why vectors are useful. We use steps to get from one place to another. Similarly, a central use for the formalism of vectors is to guide our thinking and our algorithms for figuring out how best to get from one “place” to another. We’ve used quotation marks around “place” because we are not necessarily referring to a physical destination. We’ll get to what else we might mean by “place” later in this Block.\nAs a fanciful example of getting to a “place,” consider a treasure hunt. You are given these instructions to get there:\n\n\nOn June 1, go to the flagpole before sunrise.\nAt 6:32, walk 213 paces away from the sun.\nAt 12:19, walk 126 paces toward the sun.\n\n\nThe sun position varies over the day, so the direction to the sun on June 1 at 6:32 will be different than at 12:19. Figure 29.4 the Sun vectors at 6:32 and 12:19 on June 1.\n\n\n\n\n\nFigure 29.4: For June 1: \\(\\color{magenta}{ ext{Sun's direction at 6:32}}\\) and $$. (Location: latitude 38.0091, /longitude -104.8871). Source: suncalc.org\n\n\n\n\nThe treasure-hunt directions are in the form of a linear combination of vectors. For each of the two vectors described in the treasure instructions, the length of the vector is 1 pace. (Admittedly, not a scientific unit of length.) Scaling \\(\\color{magenta}{\\text{the magenta vector}}\\) by -213 and \\(\\color{blue}{\\text{the blue vector}}\\) by 126, then adding the two scaled vectors gives a vector that takes you from the flagpole to the treasure.\nA stickler for details might point out tht the “direction of the sun” has an upward component. Common sense will guide you to walk in the direction of the Sun as projected onto Earth’s surface. Section 30 deals with projections of vectors."
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#functions-as-vectors",
    "href": "Linear-combinations/B5-linear-combinations.html#functions-as-vectors",
    "title": "29  Linear combinations of vectors",
    "section": "29.4 Functions as vectors",
    "text": "29.4 Functions as vectors\nThis is a calculus book, and calculus is about functions. So you can imagine that there is going to be some connection between functions and vectors.\nWe’ll start with the idea that a vector is a column of numbers. Recall from Block 1 (Section 4.002) the idea of representing a function as a table of inputs and the corresponding outputs.\nHere is such a table with some of our pattern-book functions.\n\n\n\n \n  \n    t \n    one(t) \n    identity(t) \n    exp(t) \n    sin(t) \n    pnorm(t) \n  \n \n\n  \n    0.0 \n    1 \n    0.0 \n    1.000000 \n    0.0000000 \n    0.5000000 \n  \n  \n    0.1 \n    1 \n    0.1 \n    1.105171 \n    0.0998334 \n    0.5398278 \n  \n  \n    0.2 \n    1 \n    0.2 \n    1.221403 \n    0.1986693 \n    0.5792597 \n  \n  \n    0.3 \n    1 \n    0.3 \n    1.349859 \n    0.2955202 \n    0.6179114 \n  \n  \n    0.4 \n    1 \n    0.4 \n    1.491825 \n    0.3894183 \n    0.6554217 \n  \n  ... and so on ...\n  \n  \n  \n    4.6 \n    1 \n    4.6 \n    99.48432 \n    -0.9936910 \n    0.9999979 \n  \n  \n    4.7 \n    1 \n    4.7 \n    109.94717 \n    -0.9999233 \n    0.9999987 \n  \n  \n    4.8 \n    1 \n    4.8 \n    121.51042 \n    -0.9961646 \n    0.9999992 \n  \n  \n    4.9 \n    1 \n    4.9 \n    134.28978 \n    -0.9824526 \n    0.9999995 \n  \n  \n    5.0 \n    1 \n    5.0 \n    148.41316 \n    -0.9589243 \n    0.9999997 \n  \n\n\n\n\n\nIn this representation, each of the pattern-book functions is a column of numbers, that is, a vector.\nFunctions that we construct by linear combination are, in this vector format, just a linear combination of the vectors. For instance, the function \\(g(t) \\equiv 3 - 2 t\\) is \\(3\\cdot \\text{one}(t) - 2 \\cdot \\text{identity}(t)\\)\n\n\n\n \n  \n    t \n    one(t) \n    identity(t) \n    g(t) \n  \n \n\n  \n    0.0 \n    1 \n    0.0 \n    3.0 \n  \n  \n    0.1 \n    1 \n    0.1 \n    2.8 \n  \n  \n    0.2 \n    1 \n    0.2 \n    2.6 \n  \n  \n    0.3 \n    1 \n    0.3 \n    2.4 \n  \n  \n    0.4 \n    1 \n    0.4 \n    2.2 \n  \n  ... and so on ...\n  \n  \n  \n    4.6 \n    1 \n    4.6 \n    -6.2 \n  \n  \n    4.7 \n    1 \n    4.7 \n    -6.4 \n  \n  \n    4.8 \n    1 \n    4.8 \n    -6.6 \n  \n  \n    4.9 \n    1 \n    4.9 \n    -6.8 \n  \n  \n    5.0 \n    1 \n    5.0 \n    -7.0 \n  \n\n\n\n\n\nThe table above is a collection of four vectors: \\(\\vec{\\mathtt t}\\), \\(\\vec{\\mathtt{ one(t)}}\\), \\(\\vec{\\mathtt{identity(t)}}\\), and \\(\\vec{\\mathtt{g(t)}}\\). Each of those vectors has 51 components. In math-speak, we can say that the vectors are “embedded in a 51-dimensional space.”\nThe functions in the table are being represented as discrete values. Still, a table, combined with interpolation (Chapter 33) can produce a continuum."
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#matrices-and-linear-combinations",
    "href": "Linear-combinations/B5-linear-combinations.html#matrices-and-linear-combinations",
    "title": "29  Linear combinations of vectors",
    "section": "29.5 Matrices and linear combinations",
    "text": "29.5 Matrices and linear combinations\nA collection of vectors, such as the one displayed in the previous table, is called a matrix. Each of the vectors in a matrix must have the same number of components.\nAs mathematical notation, we will use bold-faced, capital letters to stand for matrices, for example \\(\\mathit{M}\\). The symbol \\(\\rightleftharpoons\\) is a reminder that a matrix can contain multiple vectors, just as the symbol \\(\\rightharpoonup\\) in \\(\\vec{v}\\) reminds us that the name “\\(v\\)” refers to a vector.\nIn the conventions for data, we give a name to each column of a data frame so that we can refer to it individually. In the conventions used in vector mathematics, single letter are used to refer to the individual vectors.\nAs a case in point, let’s look at a matrix \\(\\mathit{M}\\) containing the two vectors which we’ve previously called \\(\\vec{\\mathtt{one(t)}}\\) and \\(\\vec{\\mathtt{identity(t)}}\\): \\[\\mathit{M} \\equiv \\left[\\begin{array}{rr}1 & 0\\\\\n1 & 0.1\\\\\n1 & 0.2\\\\\n1 & 0.3\\\\\n\\vdots & \\vdots\\\\\n1 & 4.9\\\\\n1 & 5.0\\\\\n\\end{array}\\right]\\ .\\] The linear combination which we might previous have called \\(3\\cdot \\vec{\\mathtt{t}} - 2\\,\\vec{\\mathtt{identity(t)}}\\) can be thought of as\n$$\\left[\\overbrace{\\begin{array}{r}\n1\\\\\n1 \\\\\n1 \\\\\n1 \\\\\n\\vdots &\\\\\n1 \\\\\n1 \n\\end{array}}^{3 \\times}\n\\stackrel{\\begin{array}{r}\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\end{array}}{\\Large + \\ }\n\\overbrace{\\begin{array}{r}\n0\\\\\n0.1 \\\\\n0.2 \\\\\n0.3 \\\\\n\\vdots\\\\\n4.9 \\\\\n5.0 \n\\end{array}}^{-2 \\times}\\right] = \\left[\\begin{array}{r}\n\\\\ \\\\ 3\\\\\n2.8\\\\2.6\\\\2.4\\\\\\vdots\\\\-6.8\\\\-7.0\\\\ \\\\ \\\\\n\\end{array}\\right]\\ ,$$ but this is not conventional notation. Instead, we would write this more concisely as \n$$\\stackrel{\\Large\\mathit{M}}{\\left[\\begin{array}{rr}1 & 0\\\\\n1 & 0.1\\\\\n1 & 0.2\\\\\n1 & 0.3\\\\\n\\vdots & \\vdots\\\\\n1 & 4.9\\\\\n1 & 5.0\\\\\n\\end{array}\\right]} \\ \n\\stackrel{\\Large\\vec{w}}{\\left[\\begin{array}{r}2\\\\-3\\end{array}\\right]}$$\nIn symbolic form, the linear combination of the columns of \\(\\mathit{M}\\) using respectively the scalars in \\(\\vec{w}\\) is simply \\(\\mathit{M} \\, \\vec{w}\\). This is called matrix multiplication.\nNaturally, the operation only makes sense if there are as many components to \\(\\vec{w}\\) as there are columns in \\(\\mathit{M}\\).\n\n“Matrix multiplication” might better have been called “\\(\\mathit{M}\\) linearly combined by \\(\\vec{w}\\).” But “matrix multiplication” is the standard term for such linear combinations.\n\n\nIn R, you can make vectors with the rbind() command, short for “bind rows,” as in ::: {.cell layout-align=“center” fig.showtext=‘false’}\nrbind(2, 5, -3)\n##      [,1]\n## [1,]    2\n## [2,]    5\n## [3,]   -3\n\nwith the components of the vector presented as successive arguments to the function.\nOne way to make a matrix is with the cbind() command, short for “bind columns”. The arguments to cbind() will typically be vectors created by rbind(). For instance, the matrix \\[\\mathit{A} \\equiv \\left[\\vec{u}\\ \\ \\vec{v}\\right]\\ \\ \\text{where}\\ \\ \\vec{u} \\equiv \\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\right]\\ \\ \\text{and}\\ \\ \\vec{v} \\equiv \\left[\\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right]\\] can be constructed in R with these commands.\n\nu <- rbind(2, 5, -3)\nv <- rbind(1, -4, 0)\nA <- cbind(u, v)\nA\n##      [,1] [,2]\n## [1,]    2    1\n## [2,]    5   -4\n## [3,]   -3    0\n\nTo compute the linear combination \\(3 \\vec{u} + 1 \\vec{v}\\), that is, \\(\\mathit{A} \\cdot \\left[\\begin{array}{r}3\\\\1\\end{array}\\right]\\) you use the matrix multiplication operator %*%. For instance, the following defines a vector \\[\\vec{x} \\equiv \\left[\\begin{array}{r}3\\\\1\\end{array}\\right]\\] to do the job in a way that’s easy to read:\n\nx <- rbind(3, 1)\nA %*% x\n##      [,1]\n## [1,]    7\n## [2,]   11\n## [3,]   -9\n\n:::\n\nIt’s a mistake to use * instead of %*% for matrix multiplication. Remember that * is for componentwise multiplication which is different from matrix multiplication. Componentwise multiplication with vectors and matrices will usually give an error message as with:\n\nA * x\n## Error in A * x: non-conformable arrays\n\nThe phrase “non-conformable arrays” is R-speak for saying “I don’t know how to do component-wise multiplication with two differently shaped objects.\n\nIn chapters to come, we will sometimes make several different linear combinations of the vectors in a matrix. Of course the result of each individual linear combination will be a vector, so the “several different linear combinations” can be thought of as a collection of vectors, that is, a matrix.\nFor example, consider the possible linear combinations of the two vectors in a matrix \\[\\mathit{A} = \\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\ \\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right]\\ .\\]\nThe combinations we have in mind are: \\[\n\\mathit{A}\\left[\\begin{array}{r}3\\\\1\\end{array}\\right]=\n\\left[\\begin{array}{r}7\\\\11\\\\-9\\end{array}\\right]\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\\n\\mathit{A} \\left[\\begin{array}{r}-0\\\\2\\end{array}\\right]= \\left[\\begin{array}{r}2\\\\-8\\\\0\\end{array}\\right]\n\\ \\ \\ \\ \\ \\ \\  \\ \\ \\\n\\mathit{A} \\left[\\begin{array}{r}-1\\\\0\\end{array}\\right] = \\left[\\begin{array}{r}-2\\\\-5\\\\3\\end{array}\\right]\n\\] A more concise way to write this collects the vectors with the values for the scalars into a matrix, which we’ll call \\[\\mathit{X} \\equiv \\left[\\begin{array}{rrr}3 & 0 & -1\\\\1 & 2 & 0\\end{array}\\right]\\ .\\]\n\\[\\mathit{A} \\ \\mathit{X}  = \\left[\\begin{array}{rrr}7 &2 &-2\\\\11 & -8 & -5\\\\-9 & 0 & 3\\end{array}\\right]\\]\n\nIn R, to create the set of linear combinations, we create the matrices \\(\\mathit{A}\\) and \\(\\mathit{X}\\) and combine them with matrix multiplication. ::: {.cell layout-align=“center” fig.showtext=‘false’}\nA <- cbind(\n       rbind( 2,  5, -3),\n       rbind( 1, -4,  0)\n     )\n\nA is a collection of two vectors. Therefore, each vector in X must have two components: one for each vector in A\n\nX <- cbind(\n       rbind( 3, 1),\n       rbind( 0, 2),\n       rbind(-1, 0)\n)\n\nThe overall result will be a new matrix, containing three vectors, one for each vector in X:\n\nA %*% X\n##      [,1] [,2] [,3]\n## [1,]    7    2   -2\n## [2,]   11   -8   -5\n## [3,]   -9    0    3\n\n:::"
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#sub-spaces",
    "href": "Linear-combinations/B5-linear-combinations.html#sub-spaces",
    "title": "29  Linear combinations of vectors",
    "section": "29.6 Sub-spaces",
    "text": "29.6 Sub-spaces\nRecall that a vector with \\(n\\) components can be said to be embedded in an \\(n\\)-dimensional space. You might like to think of the embedding space as a kind of club with restricted membership. A vector with 2 elements is entitled to join the 2-dimensional club, but a vector with more or fewer than 2 elements cannot be admitted to the club. Similarly, there are clubs for 3-component vectors, 4-component vectors, and so on.\nThe clubhouse itself is a kind of space, the space in which any and all of the vectors that are eligible for membership can be embedded.\nNow imagine the clubhouse arranged into meeting rooms. Each meeting room is just part of the clubhouse space. Which part? That depends on a set of vectors who sponsor the meeting. For instance, in the ten-dimensional clubhouse, a few members, let’s say \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\) decide to sponsor a meeting. That meeting room, part of the whole clubhouse space, is called a subspace.\nA subspace has its own rules for admission. Vectors belong to the subspace only if they can be constructed as a linear combination of the sponsoring members. Mathematically, although the subspace is defined by the founding vectors, the subspace itself consists f all the possible vectors can be constructed by a linear combination of the sponsors.\nAs an example, consider the clubhouse that is open to any and all vectors with three components. The diagram in ?fig-two-vecs shows the clubhouse with just two members present, \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\).\nAny vector can sponsor its own subspace. In ?fig-two-vecs the subspace sponsored by \\(\\color{blue}{\\vec{u}}\\) is the extended line through \\(\\color{blue}{\\vec{u}}\\), that is, all the possible scaled versions of \\(\\color{blue}{\\vec{u}}\\). Similarly, the subspace sponsored by \\(\\color{magenta}{\\vec{v}}\\) is the extended line through \\(\\color{magenta}{\\vec{v}}\\). Each of these subspaces is one-dimensional.\n\n\n\n\n\n\n\n\n\nTwo vectors \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\) embedded in 3-dimensional space. The subspace spanned by each vector, individually, is shown as a line. NEED TO PROVIDE PRINTABLE LINK FOR PDF version\n\n\n\n\nMultiple vectors can sponsor a subspace. The subspace sponsored by both \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\) contains all the vectors that can be constructed as linear combinations of \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\). In Figure 29.5, this subspace is shown in gray.\n\n\n\n\n\n\n\n\n\nFigure 29.5: Two vectors \\(\\vec{u}\\) and \\(\\vec{w}\\). The subspace spanned by two vectors is a plane, shown as a gray surface. NEED TO PROVIDE LINK for PDF version\n\n\n\n\nOn the other hand, the subspace sponsored by \\(\\color{magenta}{\\vec{v}}\\) and \\(\\color{blue}{\\vec{u}}\\) is not the entire clubhouse. \\(\\color{magenta}{\\vec{v}}\\) and \\(\\color{blue}{\\vec{u}}\\) lie in a common plane, but not all the vectors in the 3-dimensional clubhouse lied in that plane. In fact, if you rotate ?fig-two-vecs-plane to “look down the barrel” of either \\(\\color{magenta}{\\vec{v}}\\) or \\(\\color{blue}{\\vec{u}}\\), the plane will entirely disappear from view. A subspace is an infinitesimal slice of the embedding space.\n“Sponsored a subspace” is metaphorical. In technical language we speak of the subspace spanned by a set of vectors in the same embedding space. Usually, we refer to a “set of vectors” as a matrix. For instance, letting \\[\\mathit{M} \\equiv \\left[{\\Large \\strut}\\color{blue}{\\vec{u}}\\ \\ \\color{magenta}{\\vec{v}}\\right]\\ ,\\] the gray plane in Figure 29.5 is the subspace spanned by \\(\\mathit{M}\\) or, more concisely, \\(span(\\mathit{M})\\).\nFor a more concrete, everyday representation of the subspace spanned by two vectors, a worthwhile experiment is to pick up two pencils pointing in different directions. Place the eraser ends together, pinched between thumb and forefinger. You can point the whole rigid assembly in any direction you like. The angle between them will remain the same.\nPlace a card on top of the pencils, slipping it between your pressed fingers to hold it tightly in place. The card is another kind of geometrical object: a planar surface. The orientation of two vectors together determine the orientation of the surface. This simple fact will be extremely important later on.\nYou could replace the pencils with line segments drawn on the card underneath each pencil. Now you have the angle readily measurable in two dimensions. The angle between two vectors in three dimensions is the same as the angle drawn on the two-dimension surface that rests on the vectors.\nNotice that you can also lay a card along a single vector. What’s different here is that you can roll the card around the pencil; there are many different orientations for such a card even while the vector stays fixed. So a single fixed vector does not determine uniquely the orientation of the planar surface in which the two vectors can reside. But with two fixed vectors, there is only one such surface."
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#exercises",
    "href": "Linear-combinations/B5-linear-combinations.html#exercises",
    "title": "29  Linear combinations of vectors",
    "section": "29.7 Exercises",
    "text": "29.7 Exercises"
  },
  {
    "objectID": "Linear-combinations/B5-projection.html",
    "href": "Linear-combinations/B5-projection.html",
    "title": "30  Projection & residual",
    "section": "",
    "text": "Many problems in physics and engineering involve the task of decomposing a vector \\(\\vec{b}\\) into two perpendicular component vectors \\(\\hat{b}\\) and \\(\\vec{r}\\), such that \\(\\hat{b} + \\vec{r} = \\vec{b}\\) and \\(\\hat{b} \\cdot \\vec{r} = 0\\). There is an infinite number of ways to accomplish such a decomposition, one for each way or orienting \\(\\hat{b}\\) relative to \\(\\vec{b}\\). Figure 30.1 shows a few examples.\nThe task of decomposition is important also outside of physics and engineering. Our particular interest will be in finding how best to take a linear combination of the columns of a matrix \\(\\mathit{A}\\) in order to make the best approximation to a given vector \\(\\vec{b}\\). This problem solves all sorts of problems: finding a linear combination of functions to match a relationship laid out in data, constructing statistical models such as those found in machine learning, effortlessly solving sets of simultaneous linear equations with any number of equations and any number of unknowns."
  },
  {
    "objectID": "Linear-combinations/B5-projection.html#projection-terminology",
    "href": "Linear-combinations/B5-projection.html#projection-terminology",
    "title": "30  Projection & residual",
    "section": "30.1 Projection terminology",
    "text": "30.1 Projection terminology\nThe problem of decomposition can be considered to be a special case of projection. The word “projection” may bring to mind the casting of shadows on a screen in the same manner as an old-fashioned slide projector or movie projector. The light source is arranged to generate parallel rays which arrive perpendicularly to the screen. A movie screen is two-dimensional, a subspace defined by two vectors. Imagining those two vectors to be collected into matrix \\(\\mathit{A}\\), the idea is to decompose \\(\\vec{b}\\) into a component that lies in the subspace defined by \\(\\mathit{A}\\) and another component that is perpendicular to the screen. That perpendicular component is what we have been calling \\(\\vec{r}\\) while the vector \\(\\hat{b}\\) is the projection of \\(\\vec{b}\\) onto the screen. To make it easier to keep track of the various roles played by \\(\\vec{b}\\), \\(\\hat{b}\\), \\(\\vec{r}\\) and \\(\\mathit{A}\\), we’ll give these vectors English-language names. The motivation for these names will become apparent in later chapters, but for now, here they are. You will want to memorize them.\n\n\\(\\vec{b}\\) the target vector\n\\(\\hat{b}\\) the model vector\n\\(\\vec{r}\\) the residual vector\n\\(\\mathit{A}\\) the model space (or “model subspace”)\n\nProjection is the process of finding the model vector that is as close as possible to the target vector \\(\\vec{b}\\). Another way to see this is as finding the model vector that makes the residual vector as short as possible.\n\nFigure 30.2 shows a a solved projection problem in 3-dimensional space. The figure can be rotated or set spinning, which makes it much easier to interpret the diagram as a three dimensional object. In addition to \\(\\vec{b}\\) and the vectors \\(\\vec{u}\\) and \\(\\vec{b}\\) that constitute the matrix \\(\\mathit{A}\\), the diagram includes a translucent plane marking \\(span(\\mathit{A})\\). The goal of projection is, from these givens, to find the model vector (shown in light green). Once the model vector \\(\\vec{x}\\) is known, the residual vector is easy to calculate \\[\\vec{r} \\equiv \\vec{b} - \\hat{b}\\ .\\] Another approach to the problem is to find the residual vector \\(r\\) first, then use that to find the model vector as \\[\\hat{b} \\equiv \\vec{b} - \\vec{r}\\ .\\]\n\n\n\n\nNEED TO PROVIDE LINK AND IMAGE FOR PDF version\n\n\n\n\n\nFigure 30.2: A three-dimensional diagram showing the target vector \\(\\vec{b}\\) and the vectors \\(\\vec{u}\\) and \\(\\vec{v}\\). The subspace spanned by \\(\\vec{u}\\) and \\(\\vec{v}\\) is indicated with a translucent plane. The model vector (green) is the result produced in solving the projection problem.\n\n\n\n\nInterpreting such three dimensional diagrams can be difficult. But there are tricks involving watching the diagram as it is rotated. For instance, how do we know that the translucent plane in Figure 30.2 contains \\(\\vec{u}\\) and \\(\\vec{v}\\)? As the diagram rotates, from time to time you will be looking edge on at the plane, so that the plane appears as a line on the screen. At such times, you can see that vectors \\(\\vec{u}\\) and \\(\\vec{v}\\) disappear. There is no component to \\(\\vec{u}\\) and \\(\\vec{v}\\) that sticks out from the plane."
  },
  {
    "objectID": "Linear-combinations/B5-projection.html#projection-onto-a-single-vector",
    "href": "Linear-combinations/B5-projection.html#projection-onto-a-single-vector",
    "title": "30  Projection & residual",
    "section": "30.2 Projection onto a single vector",
    "text": "30.2 Projection onto a single vector\nAs we said, projection involves a vector \\(\\vec{b}\\) and a matrix \\(\\mathit{A}\\) that defines the model space. We’ll start with the simplest case, where \\(\\mathit{A}\\) has only one column. That column is, of course, a vector. We’ll call that vector \\(\\vec{a}\\), so the projection problem is to project \\(\\vec{b}\\) onto the subspace spanned by \\(\\vec{a}\\).\nGeometrically, the situation of projecting the target vector \\(\\vec{b}\\) onto the model space \\(\\vec{a}\\) is diagrammed in Figure 30.3.\n\n\n\n\n\nFigure 30.3: The geometry of projecting \\(\\vec{b}\\) onto \\(\\vec{a}\\) to produce the model vector \\(\\hat{b}\\).\n\n\n\n\nThe angle between \\(\\vec{a}\\) and \\(\\vec{b}\\) is labelled \\(\\theta\\). You already know how to calculate \\(\\theta\\) from \\(\\vec{b}\\) and \\(\\vec{a}\\) by using the dot product:\n\\[\\cos(\\theta) = \\frac{\\vec{b} \\bullet \\vec{a}}{\\len{b}\\, \\len{a}}\\ .\\] Knowing \\(\\theta\\) and \\(\\len{b}\\), you can calculate the length of the model vector \\(\\hat{b}\\): \\[\\len{s} = \\len{b} \\cos(\\theta) = \\vec{b} \\bullet \\vec{a} / \\len{a}\\ .\\]\nScaling \\(\\vec{a}\\) by \\(\\len{a}\\) would produce a vector oriented in the model subspace, but it would have the wrong length: length \\(\\len{a} \\len{s}\\). So we need to divide \\(\\vec{a}\\) by \\(\\len{a}\\) to get a unit length vector oriented along \\(\\vec{a}\\):\n\\[\\text{model vector:}\\ \\ \\hat{b} = \\left[\\vec{b} \\bullet \\vec{a}\\right] \\,\\vec{a} / {\\len{a}^2} = \\frac{\\vec{b} \\bullet \\vec{a}}{\\vec{a} \\bullet \\vec{a}}\\  \\vec{a}.\\] . \n\nIn R/mosaic, you can calculate the projection of \\(\\vec{b}\\) onto \\(\\vec{a}\\) using %onto%. For instance ::: {.cell layout-align=“center” fig.showtext=‘false’}\nb <- rbind(-1, 2)\na <- rbind(-2.5, -0.8)\ns <- b %onto% a\ns\n##            [,1]\n## [1,] -0.3265602\n## [2,] -0.1044993\n\nHaving found \\(\\hat{b}\\), the residual vector \\(\\vec{r}\\) can be calculated as \\(\\vec{b}- \\hat{b}\\).\n\nr <- b - s\nr\n##            [,1]\n## [1,] -0.6734398\n## [2,]  2.1044993\n\nThe two properties that a projection satisfies are:\n\nThe residual vector is perpendicular to each and every vector in \\(\\mathit{A}\\). Since in this example, \\(\\mathit{A}\\) contains only the one vector \\(\\vec{a}\\), we need only look at \\(\\vec{r} \\cdot \\vec{a}\\) and confirm that it’s zero. ::: {.cell layout-align=“center” fig.showtext=‘false’}\n\nr %dot% a\n## [1] -2.220446e-16\n::: 2. The residual vector plus the model vector exactly equal the target vector. Since we computed r <- b - s, we know this must be true, but still … ::: {.cell layout-align=“center” fig.showtext=‘false’}\n(r+s) - b\n##      [,1]\n## [1,]    0\n## [2,]    0\n:::\nIf the difference between two vectors is zero for every coordinate, the two vectors must be identical. :::"
  },
  {
    "objectID": "Linear-combinations/B5-projection.html#projection-onto-a-set-of-vectors",
    "href": "Linear-combinations/B5-projection.html#projection-onto-a-set-of-vectors",
    "title": "30  Projection & residual",
    "section": "30.3 Projection onto a set of vectors",
    "text": "30.3 Projection onto a set of vectors\nAs we have just seen, projecting a target \\(\\vec{b}\\) onto a single vector is a matter of arithmetic. Now we will expand the technique to project the target vector \\(\\vec{b}\\) onto multiple vectors collected into a matrix \\(\\mathit{A}\\). Whereas in the chapter we used trigonometry to find the component of \\(\\vec{b}\\) aligned with the single vector \\(\\vec{a}\\), now we have to deal with multiple vectors at the same time. The result will be the component of \\(\\vec{b}\\) aligned with the subspace sponsored by \\(\\mathit{A}\\).\nThere is one situation where the projection is easy: when the vectors in \\(\\mathit{A}\\) are mutually orthogonal. In this situation, carry out several one-vector-at-a-time projections: \\[\\vec{p_1} = \\modeledby{\\vec{b}}{\\vec{v_1}}\\\\\n\\vec{p_2} = \\modeledby{\\vec{b}}{\\vec{v_2}}\\\\\n\\vec{p_3} = \\modeledby{\\vec{b}}{\\vec{v_3}}\\\\\n\\text{and so on}\\] The projection of \\(\\vec{b}\\) onto \\(\\mathit{A}\\) will be the sum \\(\\vec{p_1} + \\vec{p2} + \\vec{p3}\\).\n\nTo illustrate the method of projection when the vectors in \\(\\mathit{A}\\) are mutually orthogonal, we can construct such a matrix. ::: {.cell layout-align=“center” fig.showtext=‘false’}\nb  <- rbind( 1,  1,  1, 1)\nv1 <- rbind( 1,  2,  0, 0)\nv2 <- rbind(-2,  1,  3, 1)\nv3 <- rbind( 0,  0, -1, 3)\nA <- cbind(v1, v2, v3)\n\nYou can verify using a dot product that v1, v2, and v3 are mutually orthogonal.\nNow construct the one-at-a-time projections: ::: {.cell layout-align=“center” fig.showtext=‘false’}\np1 <- b %onto% v1\np2 <- b %onto% v2\np3 <- b %onto% v3\n:::\nTo find the projection of \\(\\vec{b}\\) onto the subspace spanned by \\(\\mathit{A}\\), add up the one-at-a-time projections:\n\nb_on_A <- p1 + p2 + p3\n\nNow we’ll confirm that b_on_A really is the projection of b onto A. The strategy is to construct the residual from the projection. ::: {.cell layout-align=“center” fig.showtext=‘false’}\nresid <- b - b_on_A\n::: All that’s needed is to confirm that the residual is perpendicular to each and every vector in A: ::: {.cell layout-align=“center” fig.showtext=‘false’}\nresid %dot% v1\n## [1] 7.771561e-16\nresid %dot% v2\n## [1] -2.220446e-16\nresid %dot% v3\n## [1] 6.661338e-16\n::: :::"
  },
  {
    "objectID": "Linear-combinations/B5-projection.html#a-becomes-q",
    "href": "Linear-combinations/B5-projection.html#a-becomes-q",
    "title": "30  Projection & residual",
    "section": "30.4 A becomes Q",
    "text": "30.4 A becomes Q\nNow that we have a satisfactory method for projecting \\(\\vec{b}\\) onto a matrix \\(\\mathit{A}\\) consisting of mutually orthogonal vectors, we need to develop a method for the projection when the vectors in \\(\\mathit{A}\\) are not mutually orthogonal. The big picture here is that we will construct a new matrix \\(\\mathit{Q}\\) that spans the same space as \\(\\mathit{A}\\) but whose vectors are mutually orthogonal. We’ll construct \\(\\mathit{Q}\\) out of linear combinations of the vectors in \\(\\mathit{A}\\), so we can be sure that \\(span(\\mathit{Q}) = span(\\mathit{A})\\).\nWe introduce the process with an example, involving a vectors in a 4-dimensional space. \\(\\mathit{A}\\) will be a matrix with two columns, \\(\\vec{v_1}\\) and \\(\\vec{v_2}\\). Here’s the setup for the example vectors and model matrix:\n\nb <- rbind(1,1,1,1)\nv1 <- rbind(2,3,4,5)\nv2 <- rbind(-4,2,4,1)\nA <- cbind(v1, v2)\n\nWe start the construction of the \\(\\mathit{Q}\\) matrix by pulling in the first vector in \\(\\mathit{A}\\). We’ll call that vector \\(\\vec{q_1}\\)\n\nq1 <- v1\n\nThe next \\(\\mathit{Q}\\) vector will be constructed to be perpendicular to \\(\\vec{q_1}\\) but still in the subspace spanned by \\(\\left[{\\Large\\strut}\\vec{v_1}\\ \\ \\vec{v_2}\\right]\\). We can guarantee this will be the case by making the \\(\\mathit{Q}\\) vector entirely as a linear combination of \\(\\vec{v_1}\\) and \\(\\vec{v_2}\\).\n\nq2 <- v2 %perp% v1\n\nsince \\(\\vec{q_1}\\) and \\(\\vec{q_2}\\) are orthogonal and define the same subspace as \\(\\mathit{A}\\), we can construct the projection of \\(\\vec{b}\\) onto \\(\\vec{A}\\) by adding up the projections of \\(\\vec{b}\\) onto the individual vectors in \\(\\mathit{Q}\\), like this:\n\nbhat <- (b %onto% q1) + (b %onto% q2)\n\nTo confirm that this calculation of \\(\\hat{\\vec{b}}\\) is correct, construct the residual vector and confirm that it is perpendicular to every vector in \\(\\mathit{Q}\\) (and therefore in \\(\\mathit{A}\\), which spans the same space).\n\nr <- b - bhat\nr %dot% v1\n## [1] 1.110223e-15\nr %dot% v2\n## [1] 2.220446e-16\n\nNote that we defined \\(\\vec{r} = \\vec{b} - \\hat{\\mathbf{b}}\\), so it’s guaranteed that \\(\\vec{r} + \\hat{\\mathbf{b}}\\) will equal \\(\\vec{b}\\).\nThis process can be extended to any number of vectors in \\(\\mathit{A}\\). Here’s the algorithm for constructing \\(\\mathit{Q}\\):\n\nTake the first vector from \\(\\mathit{A}\\) and call it \\(\\vec{q_1}\\).\nTake the second vector from \\(\\mathit{A}\\) and find the residual from projecting it onto \\(\\vec{q_1}\\). This residual will be \\(\\vec{q_2}\\). At this point, the matrix \\(\\left[\\strut \\vec{q_1}, \\ \\ \\vec{q_2}\\right]\\) consists of mutually orthogonal vectors.\nTake the third vector from \\(\\mathit{A}\\) and project it onto \\(\\left[\\strut \\vec{q_1}, \\ \\ \\vec{q_2}\\right]\\). We can do this because we already have an algorithm for projecting a vector onto a matrix with mutually orthogonal columns. Call the residual from this projection \\(\\mathit{q_3}\\). It will be orthogonal to the vectors in \\(\\left[\\strut \\vec{q_1}, \\ \\ \\vec{q_2}\\right]\\), so all three of the q vectors we’ve created are mutually orthogonal.\nContinue onward, taking the next vector in \\(\\mathit{A}\\), projecting it onto the q-vectors already assembled, and finding the residual from that projection.\nRepeat step (iv) until all the vectors in \\(\\mathit{A}\\) have been handled.\n\n\nProject a \\(\\vec{b}\\) that lives in 10-dimensional space onto the subspace sponsored by five vectors that are not mutually orthgonal:\n\nb <- rbind(3,2,7,3,-6,4,1,-1, 8, 2) # or any set of 10 numbers\nv1 <- rbind(4, 7, 1, 0, 3, 0, 6, 1, 1, 2)\nv2 <- rbind(8, 8, 4, -3, 3, -2, -4, 9, 6, 0)\nv3 <- rbind(12, 0, 4, -2, -6, -4, -1, 4, 6, -7)\nv4 <- rbind(0, 3, 9, 6, -4, -5, 4, 0, 5, -4)\nv5 <- rbind(-2, 5, -4, 8, -9, 3, -5, 0, 11, -4)\nA  <- cbind(v1, v2, v3, v4, v5)\n\nYou can confirm using dot products that the v-vectors are not mutually orthogonal.\nNow to construct the vectors in \\(\\mathit{Q}\\).\n\nq1 <- v1\nq2 <- v2 %perp% q1\nq3 <- v3 %perp% cbind(q1, q2)\nq4 <- v4 %perp% cbind(q1, q2, q3)\nq5 <- v5 %perp% cbind(q1, q2, q3, q4)\nQ <- cbind(q1, q2, q3, q4, q5)\n\nSince Q consists of mutually orthogonal vectors, the projection of b onto Q can be done one vector at a time. ::: {.cell layout-align=“center” fig.showtext=‘false’}\np1 <- b %onto% q1\np2 <- b %onto% q2\np3 <- b %onto% q3\np4 <- b %onto% q4\np5 <- b %onto% q5\n# put together the components\nb_on_A <- p1 + p2 + p3 + p4 + p5\n# check the answer: resid should be perpendicular to A\nresid <- b - b_on_A\nresid %dot% v1\n## [1] 1.065814e-14\nresid %dot% v2\n## [1] 4.973799e-14\nresid %dot% v3\n## [1] 7.105427e-15\nresid %dot% v4\n## [1] 0\nresid %dot% v5\n## [1] 1.776357e-14\n\n:::"
  },
  {
    "objectID": "Linear-combinations/B5-projection.html#exercises",
    "href": "Linear-combinations/B5-projection.html#exercises",
    "title": "30  Projection & residual",
    "section": "30.5 Exercises",
    "text": "30.5 Exercises"
  },
  {
    "objectID": "Linear-combinations/B5-target-problem.html",
    "href": "Linear-combinations/B5-target-problem.html",
    "title": "31  The target problem",
    "section": "",
    "text": "“In theory there is no difference between theory and practice, while in practice there is.”\nIn this chapter, we ask you to reconsider a mathematical theory that is universally taught in high-school and to consider augmenting it with newer computational ideas that address the same kind of problems, but which produce useful results even when the mathematical theory insists the “a solution does not exist.”\nThe time-honored theory is that taught in high-school algebra. There’s nothing wrong with that theory except that it is incomplete. It doesn’t address the needs of present-day practice, particularly in data science, statistics, and machine learning.\nAlgebra, in its basic sense, is about generalizing arithmetic to handle situations where some quantities are not yet known numerically and so are represented by symbols. The algebra student learns rules for symbolic expressions that allow the expressions to be re-arranged into other forms that would clearly be valid if replaced by numbers. Some examples of these rules:\ni. \\(ax = b\\) is equivalent to \\(x = a/b\\).\nii. \\(a + x = b\\) is equivalent to \\(x=b-a\\).\niii. \\(a x^2 + b x + c = 0\\) is equivalent to \\(x = \\frac{-b\\pm \\sqrt{\\strut b^2 - 4ac}}{2a}\\).\niv. \\(\\ln(ax) = b\\) is equivalent to \\(x = \\frac{1}{a}\\ln(b)\\).\nA major challenge to the algebra student is to use such rules to re-arrange expressions into a form \\(x=\\) that enables \\(x\\) to be calculated from the numerical values of the other symbols. Unfortunately, students are given little or no insight to the historical origins of algebra techniques and why they are not necessarily appropriate for all tasks.\nIn English, the word “algebra” is seen as early as 1551. It comes from a book written by the Persian Muhammad ibn Musa al-Khwarizmi (780-850), The Compendious Book on Calculation by Completion and Balancing. The book introduced the use of rules familiar to every algebra student. In the original Arabic, the title includes the word “al-jabr,” meaning “completion” or “rejoining.” According to some sources, the literal meaning of “al-jabr” was resetting and rejoining broken bones. That literal meaning correctly conveys the importance of the subject, but also the pain endured by many algebra students. (Incidentally, the “algorithm” comes from the name of the book’s author: al Khwarizmi. He is a major figure in the history of mathematics.)\nThis history may not be of immediate interest to every reader, but there is a good point to it. The roots of algebra are ancient and developed in an era very different from our own. Today’s student learns algebra in order to facilitate the study and practice of physics, chemistry, statistics, engineering, and other fields. None of these fields existed when algebra was being conceived. That is, the theory was developed before the recognition of the problems and calculations that arise in modern practice. Thus, “in practice, theory and practice are different.”\nThis chapter is about re-expressing some basic algebraic theory in order to align it better to today’s practice."
  },
  {
    "objectID": "Linear-combinations/B5-target-problem.html#sec-linear-equations",
    "href": "Linear-combinations/B5-target-problem.html#sec-linear-equations",
    "title": "31  The target problem",
    "section": "31.1 Linear equations",
    "text": "31.1 Linear equations\nThe focus of interest will be the familiar task\n\\[\\ \\ \\ \\ \\ \\ \\text{given}\\ \\ a x = b\\,,\\ \\ \\text{find}\\ \\ x\\ .\\] All algebra students learn that \\(x = b/a\\), with the proviso that if \\(a = 0\\), “there is no solution.”\nA somewhat more advanced algebra task is to work with “simultaneous linear equations,” for example: \\[\\ \\ \\ \\text{given}\\ \\ \\ \\begin{array}{rrrcr}\n3 x & + & 2 y & = & 7\\\\\n-1&+&y&=&4\\end{array}\n\\ \\ \\ \\ \\text{find}\\ \\ x\\,\\&\\,y\\ .\n\\]\nSolving simultaneous linear equations is hard. It involves more arithmetic than \\(ax = b\\) and requires the student to make good choices how to take linear combinations of the two equations to reduce the problem to two equations, one with \\(x\\) as the only unknown and one with \\(y\\). Also, the “there is no solution” proviso is not easy to state, so you can’t know at a glance whether there is indeed a solution.\nThe simultaneous linear equation problem can be more compactly written using matrix and vector notation. \\[\\begin{array}{rrrcr}3x & + &2y & = & 7\\\\-1&+&y&=&4\\end{array} \\ \\ \\text{is the same as}\\ \\ \\left[\\begin{array}{r}3\\\\-1\\end{array}\\right] x + \\left[\\begin{array}{r}2\\\\1\\end{array}\\right] y =\n\\left[\\begin{array}{r}7\\\\4\\end{array}\\right] \\] You can see the vector form as a linear combination of two vectors. Collecting these two vectors into a matrix \\(\\mathit{A}\\), and similarly writing \\(x\\, \\text{and}\\, y\\) as the scalar components of a vector \\(\\vec{x}\\) gives \\[\\left[\\begin{array}{rr}3&2\\\\-1&1\\end{array}\\right]\\ \\vec{x} = \\left[\\begin{array}{r}7\\\\4\\end{array}\\right]\\] Which can be expressed as \\(\\mathit{A} \\vec{x} = \\vec{b}\\).\nA student, recognizing the similarity of \\(\\mathit{A}\\vec{x} = \\vec{b}\\) to \\(a x = b\\) would reasonably suggest the solution \\(\\vec{x} = \\vec{b}/ \\mathit{A}\\). Such a student might be instructed, “No, you can’t do this.” A better response would be, “Good. Now tell me what you mean by \\(\\vec{b}/\\mathit{A}\\)?”\nModern practice often calls for solving \\(\\mathit{A}\\vec{x} = \\vec{b}\\) in settings where a traditional algebra teacher might say, as for \\(0 x = b\\) that “there is no solution.”\nTo illustrate such a setting, recall the problems from Section Section 11.3 of finding the linear combination of the functions \\(f(\\mathtt{time})=1\\) and \\(g(\\mathtt{time}) = e^{-0.019 \\mathtt{time}}\\) that best matches the CoolingWater data:\n\n\n\n \n  \n    time \n    temp \n  \n \n\n  \n    0 \n    98.2 \n  \n  \n    1 \n    94.4 \n  \n  \n    2 \n    91.4 \n  \n  ... and so on ...\n  \n  \n  \n    220 \n    25.9 \n  \n  \n    221 \n    25.8 \n  \n\n\n\n\n\nWe seek scalars \\(C\\) and \\(D\\) such that the function \\(C f(\\mathtt{time}) + D g(\\mathtt{time})\\) gives the best possible match to temp.\nWe can compactly write the problem of finding the best linear combination into matrix form by evaluating \\(f()\\) and \\(g()\\) at the values listed in the time column: \\[\\underbrace{\\left[\\begin{array}{rr}1&1.0000\\\\1&0.9812\\\\1&0.9627\\\\\\vdots\\\\1&0.0153\\\\1&0.0150\\end{array}\\right]}_{\\!\\!\\!\\!\\!\\!\\!\\!{\\large\\mathit{A}} = \\left[\\strut f(\\mathtt{time})\\,,\\ \\ \\ g(\\mathtt{time})\\right]} \\underbrace{\\left[\\begin{array}{r}C\\\\D\\end{array}\\right]}_{\\large\\vec{x}} \\ \\text{is the best match to}\\  \\underbrace{\\left[\\begin{array}{r}\\mathtt{98.2}\\\\\\mathtt{94.4}\\\\\\mathtt{91.4}\\\\\\vdots\\\\\\mathtt{25.9}\\\\\\mathtt{25.8}\\end{array}\\right]}_{\\large\\vec{b}}\\]\nRegrettably, the classical algebraicists did not propose a rule for “is the best match to.” Replacing “is the best match to” with \\(=\\) is not literally correct since “there is no solution” that makes the equality literally true.\nWe’ll use the term target problem to name the task of finding \\(\\vec{x}\\) such that \\(\\mathit{A} \\vec{x}\\) is the best possible match to \\(\\vec{b}\\). This term is motivated by the idea that \\(\\vec{b}\\) is a target, and we seek to use the resources in \\(\\mathit{A}\\) to get as close as possible to the target: choose \\(\\vec{x}\\) such that \\(\\mathit{A} \\vec{x}\\) falls as closely as possible to the target.\nTo address the practical problem in the notation of algebra theory, people write \\[\\mathit{A} \\vec{x} = \\vec{b} - \\vec{r}\\] where \\(\\vec{r}\\) is a vector specially selected to path up \\(\\mathit{A} \\vec{x} = \\vec{b}\\) so that when the best-matching \\(C\\) and \\(D\\) are found, there will be a literal equality solution to \\(\\mathit{A} \\vec{x} = \\vec{b} - \\vec{r}\\).\nAt first glance, \\(\\mathit{A} \\vec{x} = \\vec{b} - \\vec{r}\\) might seem intractable: How are we to find \\(\\vec{r}\\). The answer is that \\(\\vec{r}\\) will be the solution to the projection problem \\(\\vec{b}\\sim\\mathit{A}\\). When \\(\\vec{r}\\) is selected this way, \\(\\vec{r}\\) will be the shortest possible vector that can do the matching up. In other words, by choosing \\(\\vec{r} = \\vec{b} \\sim \\mathit{A}\\) we are implementing the following definition of “is the best match to”: “the best match is the one with the smallest length \\(\\vec{r}\\).”\nIt’s remarkable that one can find \\(\\vec{r}\\) even without knowing \\(\\vec{x}\\). That’s why we introduced and solved the projection problem before taking on the target problem.\nThe part of the target problem that we have still to figure out is how, given \\(\\vec{r}\\), to find \\(\\vec{x}\\). But even at this point you can see that \\(\\mathit{A}\\vec{x} = \\vec{b} - \\vec{r}\\) must have a solution, since \\(\\vec{b} - \\vec{r}\\) is exactly the model vector \\(\\hat{b}\\) which, as we saw in Section 30), must lie in \\(span{\\mathit{A}}\\)."
  },
  {
    "objectID": "Linear-combinations/B5-target-problem.html#visualization-in-a-two-dimensional-subspace",
    "href": "Linear-combinations/B5-target-problem.html#visualization-in-a-two-dimensional-subspace",
    "title": "31  The target problem",
    "section": "31.2 Visualization in a two-dimensional subspace",
    "text": "31.2 Visualization in a two-dimensional subspace\nTo help you create a mental model of the geometry of the target problem, we’ll solve it graphically for a two dimensional subspace. That is, we’ll solve \\(\\left[\\vec{u}, \\vec{v}\\right] \\vec{x} = \\vec{b}\\). For simplicity, the vectors \\(\\vec{u}\\), \\(\\vec{v}\\) and \\(\\vec{b}\\) will have two components. This means that there is no need to project \\(\\vec{b}\\) onto the subspace; it’s already there (so long as \\(\\vec{u}\\) and \\(\\vec{v}\\) have different directions. )\nYou may already have encountered the step (ii) technique in your childhood reading. The problem appears in Robert Louis Stevenson’s famous novel, Treasure Island. The story is about the discovery of a treasure map indicating the location of buried treasure on the eponymous Island. There is a red X on the map labelled “bulk of treasure here,” but that is hardly sufficient to guide the dig for treasure. After all, every buried treasure needs some secret to protect it. On the back of the map is written a cryptic clue to the precise location:\n\nTall tree, Spy-glass shoulder, bearing a point to the N. of N.N.E.\nSkeleton Island E.S.E. and by E.\nTen feet.\n\nSkeleton Island is clearly marked on the map, as is Spy-glass Hill. The plateau marked by the red X “was dotted thickly with pine-trees of varying height. Every here and there, one of a different species rose forty or fifty feet clear above its neighbors.” But which of these was the “tall tree” mentioned in the clue?\n\n\n\n\n\nFigure 31.1: The map of Treasure Island. The heading ‘E.S.E. and by E.’ is marked with a solid black line starting at Skeleton Island. The heading ‘N. of N.N.E.’ is marked by dotted lines, one of which is positioned to point at the shoulder of Spy-glass Hill. Where the bearing from Skeleton Island meets the bearing to Spy-glass Hill will be the Tall tree.\n\n\n\n\nWith your new-found background in vectors, you will no doubt recognize that “N. of N.N.E” is the direction of a vector as is “E.S.E. and by E.” Pirate novels seem always to use the length unit of “pace,” which we’ll use here as well. The target is the shoulder of Spy-glass Hill. Or, in vector terms, \\(\\vec{b}\\) is the vector with Skeleton Island as the tail and the should of Spy-glass Hill as the tip. The vectors are \\(\\vec{u} = \\text{N. of N.N.E.}\\) and \\(\\vec{v} = \\text{E.S.E. and by E.}\\) We need to \\[\\text{solve} \\ \\ \\underbrace{\\left[\\vec{u}, \\vec{v}\\right]}_{\\Large\\strut\\mathit{A}} \\underbrace{\\small\\left[\\begin{array}{c}C\\\\D\\end{array}\\right]}_{\\Large\\vec{x}} = \\vec{b}\\ \\ \\text{for}\\ \\ {\\small\\left[\\begin{array}{c}C\\\\D\\end{array}\\right]}\\ .\\]\nLong John Silver, obviously an accomplished mathematician, starts near Skeleton Island, moving on along the vector that keeps Skeleton Island to the compass bearing one point east of east-south-east. While on the march, he keeps a telescope trained on the shoulder of Spy-glass Hill. When that telescope points one point north of north-north-east, they are in the vicinity of a tall tree. That’s the tree matching the clue.\nThe vectors in Treasure Island were perpendicular to one another, that is, mutually orthogonal. The more general situation is that the vectors in \\(\\mathit{A}\\) will be somewhat aligned with one another: not mutually orthogonal. Figure 31.2 illustrates the situation: \\(\\vec{v}\\) is not perpendicular to \\(\\vec{u}\\). The task, still, is to find a linear combination of \\(\\vec{u}\\) and \\(\\vec{v}\\) that will match \\(\\vec{b}\\). The diagram shows the \\(\\vec{u}\\) vector and the subspace aligned with \\(\\vec{u}\\), and similarly for \\(\\vec{v}\\)\n\n\n\n\n\nFigure 31.2: The telescope method of solving projection onto two vectors.\n\n\n\n\nThe algorithm is based in Long John Silver’s technique. Pick either \\(\\vec{u}\\) or \\(\\vec{v}\\), it doesn’t matter which. In the diagram, we’ve picked \\(\\vec{v}\\). Align your telescope with that vector. Now march along the other vector, \\(\\vec{u}\\), carefully keeping the telescope on the bearing aligned with \\(\\vec{v}\\). From the diagram, you can see that when you’ve marched to \\(\\frac{1}{2} \\vec{u}\\), the telescope does not yet have \\(\\vec{b}\\) in view. Similarly, at \\(1 \\vec{u}\\), the target \\(\\vec{b}\\) isn’t yet visible. Marching a little further, to about \\(1.6 \\vec{u}\\) brings you to the point in the \\(\\vec{u}\\)-subspace where the target falls into view. This tells us that the coefficient on \\(\\vec{u}\\) will be 1.6.\nTo find the coefficient on \\(\\vec{v}\\), you’ll need to march along the line of the telescope, taking steps of size \\(\\|\\vec{v}\\|\\). In the diagram, we’ve marked the march with copies of \\(\\vec{v}\\) to make the counting easier. We’ll need to march opposite the direction of \\(\\vec{v}\\), so the coefficient will be negative. Taking 2.8 steps of size \\(\\|\\vec{v}\\|\\) brings us to the target. Thus:\n\\[\\vec{b} = 1.6 \\vec{u} - 2.8 \\vec{v}\\ .\\]\nTo handle vectors in spaces where telescopes are not available, we need an arithmetic algorithm. In R, that algorithm is packaged up as qr.solve(). We will pick this up again the next section.\n\nIn 3-dimensional space, visualization of the solution to the target problem is possible, at least for those who have the talent of rotating three-dimensional objects in their head. For the rest of us, a physical model can help; take three pencils labeled \\(\\vec{u}\\), \\(\\vec{v}\\), and \\(\\vec{b}\\) and bury their tails in a small ball of putty. (Chemistry molecular construction kits are a good alternative.)\nIn case putty, pencils, or a molecular model kit are not available, use the interactive diagram in Figure 31.3. This diagram also includes \\(\\hat{b}\\) and \\(\\vec{r}\\) with the hope that this will guide you into orienting the diagram appropriately to see where the solution comes from.\n\n\n\n\nNEED TO PROVIDE A SHORT LINK for PDF version\n\n\n\n\n\nFigure 31.3: Showing the relative orientation of the three vectors \\(\\vec{u}\\), \\(\\vec{v}\\) and \\(\\vec{b}\\). Drag the image to rotate it.\n\n\n\n\n\\(\\vec{u}\\) and \\(\\vec{v}\\) are fixed in length. However, their lengths will appear to change as you rotate the space. This might be called the “gun-barrel” effect; a tube looks very short when you look down it’s longitudinal axis, but looks longer when you look at it from the side. Rotate the space until both \\(\\vec{u}\\) and \\(\\vec{v}\\) reach their maximum apparent length. The viewpoint that accomplishes this is looking downward perpendicularly onto the \\(\\left[\\vec{u},\\vec{v}\\right]\\)-plane. In this orientation, you will be looking down the barrel of the \\(\\vec{r}\\) gun. Vector \\(\\vec{b}\\) is not in that plane, as you can confirm by rotating the plot a bit out of the \\(\\left[\\vec{u},\\vec{v}\\right]\\)-plane. Returning to the perspective looking down perpendicularly on the place, you can see how \\(\\vec{b}\\) corresponds to \\(\\hat{b}\\), the point in the plane where the projection of \\(\\vec{b}\\) will fall.\nTo find the scalar multiplier on \\(\\vec{v}\\), rotate the space until the vector \\(\\vec{u}\\) is pointing straight toward you. You’ll see only the arrowhead of \\(\\vec{u}\\). Vectors \\(\\vec{v}\\) and \\(\\hat{b}\\) will appear parallel to each other, but that’s because you are looking at the plane edge on. In this orientation, \\(\\hat{b}\\) will appear just a little longer than \\(\\vec{v}\\), perhaps 1.2 times longer. So 1.2 is the scalar multiplier on \\(\\vec{v}\\).\nTo figure out the scalar multiplier on \\(\\vec{u}\\), follow the same procedure as in the previous paragraph, but looking down the barrel of \\(\\vec{v}\\). From this perspective, \\(\\vec{u}\\) appears longer than \\(\\hat{b}\\); the scalar multiplier on \\(\\vec{u}\\) will be about 0.9. In terms of \\(\\mathit{A} \\vec{x} = \\hat{b}\\), the solution is \\[\\vec{x} = \\left[\\begin{array}{r}0.9\\\\1.2\\end{array}\\right]\\ .\\]"
  },
  {
    "objectID": "Linear-combinations/B5-target-problem.html#properties-of-the-solution",
    "href": "Linear-combinations/B5-target-problem.html#properties-of-the-solution",
    "title": "31  The target problem",
    "section": "31.3 Properties of the solution",
    "text": "31.3 Properties of the solution\nAs you might expect, there is a known solution to the target problem. We’ll start by using a computer implementation of this solution to demonstrate some simple properties of the solution. As an example, we’ll use three vectors \\(\\vec{u}\\), \\(\\vec{v}\\), and \\(\\vec{w}\\) in a 5-dimensional space as the “screen” to be projected onto, and another vector \\(\\vec{b}\\) as the object being projected.\nThe matrix \\(\\mathit{A}\\) is: \\[{\\mathbf A} \\equiv \\left[\\strut \\begin{array}{ccc}|&|&|\\\\\\vec{u} & \\vec{v} & \\vec{w}\\\\|&|&|\\end{array}\\right]\\]\nFor the sake of example, we’ll make up some vectors. In your own explorations, you can change them to anything you like.\n\n# the three vectors\nu <- rbind(6, 4, 9, 3, 1)\nv <- rbind(1, 5,-2, 0, 7)\nw <- rbind(3,-5, 2, 8, 4)\nA <- cbind(u, v, w)\n# the target\nb <- rbind(8, 2,-5, 7, 0)\n\nThe operator %onto% model vector and from that we can calculate the residual vector.\n\ns <- b %onto% A \nr <- b - s\n\nThose two simple commands constitute a complete solution to the projection problem, where see seek to model vector and the residual vector.\nIn the target problem we want more: How to express \\(\\hat{b}\\) as a linear combination of the columns in \\(\\mathit{A}\\). At the risk of being repetitive, this means finding \\(\\color{magenta}{\\vec{x}}\\) in \\[\\mathit{A}\\ \\color{magenta}{\\large\\vec{x}} = \\vec{b}\\] where \\(\\mathit{A}\\) and \\(\\vec{b}\\) are given.\nThe function qr.solve() finds \\(\\vec{x}\\). ::: {.cell layout-align=“center” fig.showtext=‘false’}\nx <- qr.solve(A, b)\n::: ::: {.cell layout-align=“center” fig.showtext=‘false’}\n##            [,1]\n## [1,] 0.03835171\n## [2,] 0.33478133\n## [3,] 0.48849968\n:::\nHow can we confirm that this really is the solution to the target problem for this set of vectors? Easy! Just multiply \\(\\mathit{A}\\) by the \\(\\vec{x}\\) that we found. The result should be the target vector \\(\\hat{b}\\):\n\nA %*% x\n##            [,1]\n## [1,]  2.0303906\n## [2,] -0.6151849\n## [3,]  0.6526021\n## [4,]  4.0230526\n## [5,]  4.3358197\ns\n##            [,1]\n## [1,]  2.0303906\n## [2,] -0.6151849\n## [3,]  0.6526021\n## [4,]  4.0230526\n## [5,]  4.3358197\n\n\nYou should add qr.solve() to your computational toolbox of R functions."
  },
  {
    "objectID": "Linear-combinations/B5-target-problem.html#application-of-the-target-problem",
    "href": "Linear-combinations/B5-target-problem.html#application-of-the-target-problem",
    "title": "31  The target problem",
    "section": "31.4 Application of the target problem",
    "text": "31.4 Application of the target problem\nIn Section 31.1 we translated into vector/matrix form the problem, originally stated in Block 1, of finding the best linear combination of \\(f(\\mathtt{time}) \\equiv 1\\) and \\(g(\\mathtt{time}) \\equiv e^{-0.019 \\mathtt{time}}\\). Let’s solve that problem now.\n\nEarlier we introduced rbind() for the purpose of making column vectors, as in ::: {.cell layout-align=“center” fig.showtext=‘false’}\nrbind(3,7,-1)\n##      [,1]\n## [1,]    3\n## [2,]    7\n## [3,]   -1\n\nNow we are going to work with columns of data stored in the CoolingWater data frame. A good way to extract a column from a data frame is using the with() function. For instance,\n\nb <- with(CoolingWater, temp)\ntime <- with(CoolingWater, time)\nA <- cbind(1, exp(-0.019 * time))\nhead(A)\n##      [,1]      [,2]\n## [1,]    1 1.0000000\n## [2,]    1 0.9811794\n## [3,]    1 0.9627129\n## [4,]    1 0.9445941\n## [5,]    1 0.9268162\n## [6,]    1 0.9093729\n\nNotice that cbind() automatically translated 1 into the vector of all ones.\nWe’re all set up to solve the target problem:\n\nx <- qr.solve(A, b)\n\n\n## [1] 25.92024 61.26398\n\nHow good an answer is the x calculated by qr.solve()? Judge for yourself!\n\ngf_point(temp ~ time, data = CoolingWater, size=0) %>%\n  slice_plot(25.92 + 61.26*exp(-0.019*time) ~ time,\n             color=\"blue\")\n\n\n\n\n\n\n\n\nYou may recall from Block 1 the explanation for the poor match between the model and the data for early times: that the water cooled quickly when poured into the cool mug, but the mug-with-water cooled much slower into the room air.\nLet’s augment the model by adding another vector with a much faster exponential cooling, say, \\(e^{-0.06 \\mathtt{time}}\\).\n\nnewA <- cbind(A, exp(-0.06*time))\nqr.solve(newA, b)\n## [1] 26.82297 53.27832 12.67486\n\n\ngf_point(temp ~ time, data = CoolingWater, size=0) %>%\n  slice_plot(26.82 + 53.28*exp(-0.019*time) +\n               12.67*exp(-0.06*time) ~ time,\n             color=\"green\")\n\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "Linear-combinations/B5-target-problem.html#exercises",
    "href": "Linear-combinations/B5-target-problem.html#exercises",
    "title": "31  The target problem",
    "section": "31.5 Exercises",
    "text": "31.5 Exercises"
  },
  {
    "objectID": "Linear-combinations/B5-stat-modeling.html",
    "href": "Linear-combinations/B5-stat-modeling.html",
    "title": "32  Statistical modeling and R2",
    "section": "",
    "text": "So far, we’ve been looking at linear combinations from a distinctively mathematical point of view: vectors, collections of vectors (matrices), projection, angles and orthogonality. We’ve show a few applications of the techniques for working with linear combinations, but have always expressed those techniques using mathematical terminology. In this Chapter, we will take a detour to get a sense of the perspective and terminology of another field: statistics.\nIn the quantitative world, including fields such as biology and genetics, the social sciences, business decision making, etc. there are far more people working with linear combinations with a statistical eye than there are people working with the mathematical form of notation. Statistics is a far wider field than linear combination, so this chapter is not an attempt to replace the need to study statistics and data science. The purpose is merely to show you how a mathematical process can be used as part of a broader framework to provide useful information to decision-makers.\nIt often happens that a model is needed to help organize complex, multivariate data for purposes such as prediction. As a case in point, consider the data available in the Body_fat data frame, which consists of measurements of characteristics such as height, weight, chest circumference, and body fat on 252 men.\nBody fat, the percentage of total body mass consisting of fat, is thought by some to be a good measure of general fitness. To what extent this theory is merely a reflection of general societal attitudes toward body shape is unknown.\nWhatever its actual utility, body fat is hard to measure directly; it involves submerging a person in water to measure total body volume, then calculating the persons mass density and converting this to a reading of body-mass percentage. For those who would like to see body fat used more broadly as a measure of health and fitness, this elaborate procedure stands in the way. And so they seek easier ways to estimate body fat along the lines of the Body Mass Index (BMI), which is a simple arithmetic combination of easily measured height and weight. (Note that BMI is also controversial as anything other than a rough description of body shape. In particular, the label “overweight,” officially \\(25 \\leq \\text{BMI}\\leq 30\\) has at best little connection to actual health.)\nHow can we construct a model, based on the available data, of body fat as a function of the easy-to-measure characteristics such as height and weight? You can anticipate that this will be a matter of applying what we know about the target problem \\[\\text{Given}\\ \\mathit{A}\\  \\text{and}\\ \\vec{b}\\text{, solve } \\mathit{A} \\vec{x} = \\vec{b}\\ \\text{for}\\ \\vec{x}\\]where \\(\\vec{b}\\) is the column of body-mass measurements and \\(\\mathit{A}\\) is the matrix of all the other columns in the data frame.\nIn statistics, the target \\(\\vec{b}\\) is called the response variable and \\(\\mathit{A}\\) is the set of explanatory variables. You can also think of the response variable as the output of the model we will build and the explanatory variables as the inputs to that model.\nAlthough application of the target problem is an essential part of constructing a statistical model, it is far from the only part. For instance, statisticians find is useful to think about “how much” of the response variable is explained by the explanatory variables. Measuring this requires a definition for “how much.” In defining “how much,” statisticians focus not on how much variation there is among the values in the response variable. The standard way to measure this is with the variance, which was introduce in Section 28.4 and can be thought of as the average of pair-wise differences among the elements in \\(\\vec{b}\\).\nIn order to support this focus on the variance of \\(\\vec{b}\\), statisticians typically augment \\(\\vec{A}\\) with a column of ones, which they call the intercept.\nTo move forward, we’re going to extract the response variable from the data and construct \\(\\vec{A}\\), adding in the vector of ones. We’ll show the vector/matrix commands for doing this, but you don’t have to remember them because statisticians have a more user-friendly interface to the calculations.\nOnce we have b and A, we find the coefficients on the linear model in the usual way, with qr.solve().\nHaving applied qr.solve(), \\(\\vec{x}\\) now contains the coefficients on the “best” linear combination of the columns in \\(\\vec{A}\\). One of the ways in which the R language is designed to support statistics, is that it keeps track of names of columns, so the elements of \\(\\vec{x}\\) are labelled with the name of the column the element applies to.\nBased on the above, the model for body fat as a function of the explanatory variables is: \\[\\text{body fat} = -40.5 + 0.253\\, \\mathtt{wrist} - 0.818\\, \\mathtt{thigh} - 0.223\\, \\mathtt{forearm}\\ .\\] ::: {.Rmosaic data-latex=““} In invoking qr.solve(A, b) we have created a vector x containing the solution to the target problem. We can use that vector x in the usual ways, for example to construct the model vector (A %*% x) or the residual vector (b - A %*% x).\nSometimes you will want to create a function that implements the linear combination described by x. For convenience, the R/mosaic makeFun() knows how to accept a vector input and create the corresponding function. For example:\n:::"
  },
  {
    "objectID": "Linear-combinations/B5-stat-modeling.html#how-good-a-model",
    "href": "Linear-combinations/B5-stat-modeling.html#how-good-a-model",
    "title": "32  Statistical modeling and R2",
    "section": "32.1 How good a model?",
    "text": "32.1 How good a model?\nThere are so many ways to construct a linear-combination model from a data frame—all the different combinations of columns plus possibly interaction terms and other transformations—that it’s natural to ask, “What’s the best model?”\nAs always, “best” depends on the purpose for your model is to be used. This requires thought on the part of the modeler.\nBut there is a far more limited way to address “best” that doesn’t require any substantial thought. Consequently—and perhaps unfortunately—this method is often used in practice. It’s called R-squared and usually written R2. There’s nothing wrong with the R2 method, so long as the correct statistical interpretation is given it. We’ll mention it here without going into the statistics, just so that you will have seen the relevant names and the mathematics (but not statistics!) behind them.\nThe basic question addressed by R2 is: How much of the variation in the response variable b is accounted for by the columns of the matrix A.\nThe standard way to measure the “amount of variation” in a variable is the variance. In R, you calculate that with\n\nvar(b)\n##          bodyfat\n## bodyfat 70.03582\n\n\nHow good is the model?\nCould we make a better model?\n\nTo answer these questions, we’ll have to develop a measure of how good the model is. And this depends on the purpose for which we’ll be using the model.\nWe can also look at the variation in the model vector, \\(\\hat{b}\\).\n\nbhat <- A %*% x\nvar(bhat)\n##          bodyfat\n## bodyfat 22.16637\n\nR2 is simply the ratio of these two variances:\n\nvar(bhat) / var(b)\n##           bodyfat\n## bodyfat 0.3165004\n\nThis result, 31.7%, is interpreted as the fraction of the variance in the response variable that is accounted for by the model. Near synonyms for “accounted for” is explained by or can be attributed to.\nIn the same spirit, we can ask how much of the variance in the response variable is unexplained by, or unaccounted by the explanatory variables. To answer this, look at the size of the residual:\n\nvar(b - bhat) / var(b)\n##           bodyfat\n## bodyfat 0.6834996\n\nNotice that the amount of variance explained, 68.3%, plus the amount remaining unexplained, 31.7%, add up to 100%. This is no accident. It is the reason why statisticians use the variance as a measure of variability."
  },
  {
    "objectID": "Linear-combinations/B5-stat-modeling.html#machine-learning",
    "href": "Linear-combinations/B5-stat-modeling.html#machine-learning",
    "title": "32  Statistical modeling and R2",
    "section": "32.2 Machine learning",
    "text": "32.2 Machine learning\nIf you pay attention to trends, you will know about advances in artificial intelligence and the many claims—some hype, some not—about how it will change everything from animal husbandry to warfare. Services such as Google Translate are based on artificial intelligence, as are many surveillance technologies. (Whether the surveillance is for good or ill is a serious matter.)\nSkills in artificial intelligence are currently a ticket to lucrative employment.\nLike so many things, “artificial intelligence” is a branding term. In fact, what all the excitement is about is not mostly artificial intelligence at all. The advances, by and large, have come over 50 years of development in a field called “statistical learning” or “machine learning,” depending on whether the perspective is from statistics or computer science.\nA major part of the mathematical foundation of statistical (or “machine”) learning is linear algebra. Many workers in “artificial intelligence” are struggling to catch up because they never took linear algebra in college or, if they did, they took a proof-oriented course that didn’t cover the elements of linear algebra that are directly applicable. We’re trying to do better in this course.\nSo if you’re diligent, and continue your studies to take actual statistical/machine learning courses, you’ll find yourself at the top of the heap. Even xkcd, the beloved techno-comic, gets in on the act, as this cartoon reveals:\n\n\n\n\n\n\n\n\n\nLook carefully below the paddle and you’ll see the Greek letter “lambda”, \\(\\lambda\\). You’ll meet the linear algebra concept signified by \\(\\lambda\\)—eigenvalues and eigenvectors—in Block 6.\n\nWe’ve been using the R/mosaic function df2matrix() to construct the A and b matrices used in linear model from data. This is mainly for convenience: we need a way to carry out the calculations that lets you see the x vector, and calculate the model vector and the residual in the way described in Section 31.\nIn practice, statistical modelers use other software. The most famous of these in R is the lm() family of functions. This does all the work of creating the b vector and the A matrix, QR solving, etc. We call it a “family” of functions because the output of lm() is not simply the vector of coefficients x but includes many other features that support statistical inference on the models created.\nIn a statistics course using R, you are very likely to encounter lm(). You’ll never hear about df2matrix() outside of this book."
  },
  {
    "objectID": "Linear-combinations/B5-stat-modeling.html#exercises",
    "href": "Linear-combinations/B5-stat-modeling.html#exercises",
    "title": "32  Statistical modeling and R2",
    "section": "32.3 Exercises",
    "text": "32.3 Exercises"
  },
  {
    "objectID": "Linear-combinations/B5-functions.html",
    "href": "Linear-combinations/B5-functions.html",
    "title": "33  Functions as vectors",
    "section": "",
    "text": "Starting with ?sec-vectors, we have been working with the dot product, an operation that combines two vectors to produce a scalar. \\[\\vec{b}\\bullet\\vec{a} \\equiv\n\\left[\\begin{array}{c}b_1\\\\b_2\\\\\\vdots\\\\b_n\\end{array}\\right] \\bullet\n\\left[\\begin{array}{c}a_1\\\\a_2\\\\\\vdots\\\\a_n\\end{array}\\right] \\equiv b_1 a_1 + b_2 a_2 + \\cdots b_n a_n\\] The dot product enables us to use arithmetic to calculate geometric properties of vectors, even in high dimensional spaces that are out of reach of a ruler or protractor. For instance\nWe used such operations to solve the target problem: finding the best approximation of a vector \\(\\vec{b}\\) as a linear combination of a set of vectors in a matrix \\(\\mathit{A}\\).\nAs early as Block 1, we constructed functions as a linear combination of other functions, for example: \\[g(t) \\equiv A + B \\sin\\left(\\frac{2 \\pi}{P} t\\right)\\] where \\(A\\) is the scalar multiplier for the function \\(\\text{one}(t) \\equiv 1\\) and \\(B\\) the scalar multiplier for the sinusoid of period \\(P\\).\nWe’re going to revisit the idea of linear combinations of functions using our new tools of length, included angle, and projection. To do this, we need to have a definition of the dot product suitable for application to functions."
  },
  {
    "objectID": "Linear-combinations/B5-functions.html#dot-product-for-functions",
    "href": "Linear-combinations/B5-functions.html#dot-product-for-functions",
    "title": "33  Functions as vectors",
    "section": "33.1 Dot product for functions",
    "text": "33.1 Dot product for functions\nGiven two functions, \\(f(t)\\) and \\(g(t)\\) defined over some domain \\(D\\), we’ll compute the dot product of the functions as a sum of the product of the two functions, that is: \\[f(t) \\bullet g(t) \\equiv \\int_{D} f(t)\\,g(t)\\,dt\\ .\\] ::: {.example data-latex=““} Suppose that our two functions are \\(\\text{one}(t) \\equiv 1\\) and \\(\\text{identity}(t) \\equiv t\\) on the domain \\(0 \\leq t \\leq 1\\). Find the length of each function and the included angle between them.\n\nLength: \\(\\|\\text{one}(t)\\| = \\left[\\int_0^1 1 \\cdot 1\\,dt\\right]^{1/2} = \\left[\\ \\strut t\\left.{\\large\\strut}\\right|_0^1\\ \\right]^{1/2} = 1\\)\nLength: \\(\\|\\text{identity}(t)\\| = \\left[\\int_0^1 t \\cdot t\\,dt\\right]^{1/2} = \\left[\\ \\strut \\frac{1}{2}t^2\\left.{\\large\\strut}\\right|_0^1\\ \\right]^{1/2} = \\frac{1}{\\sqrt{2}}\\)\nIncluded angle: \\[\\cos(\\theta) = \\frac{\\text{one}(t) \\bullet \\text{identity}(t)}{\\|\\strut\\text{one}(t)\\| \\, \\|\\text{identity}(t)\\|}  =\n\\sqrt{2}\\ \\int_0^1 t\\, dt = \\sqrt{\\strut 2} \\left.{\\Large\\strut}\\frac{1}{2} t^2\\right|_0^1 = \\sqrt{\\frac{1}{2}}\\] Since \\(\\cos(\\theta) = \\sqrt{1/2}\\), the angle \\(\\theta\\) is 45 degrees. :::\n\n\nProject \\(f(t) \\equiv t^2\\) onto \\(g(t) = \\text{one}(t)\\) over the domain \\(-1 \\leq t \\leq 1\\).\nThe projection of \\(f(t)\\) onto \\(g(t)\\) will be \\[\\widehat{f(t)} = \\frac{f(t) \\bullet g(t)}{g(t) \\bullet g(t)}\\ g(t)\\]\n\n\\(f(t) \\bullet g(t) \\equiv \\int_{-1}^{1} t^2 dt = \\frac{1}{3} \\left.{\\Large \\strut}t^3\\right|_{-1}^{1} = \\frac{2}{3}\\)\n\\(g(t) \\bullet g(t) \\equiv \\int_{-1}^1 \\ dt = 2\\)\n\nThus, \\[\\widehat{f(t)} = \\frac{1}{3} \\text{one(t)} = \\frac{1}{3}\\ .\\]\n\nThe left panel of Figure 33.1 shows the functions \\(f(t) \\equiv t^2\\) and \\(\\color{magenta}{\\widehat{f(t)} \\equiv 1/3}\\) on the domain. The center panel shows the residual function, that is \\(f(t) - \\widehat{f(t)}\\). The right panel gives the square of the length of the residual function, which is \\(\\int_{-1}^1 \\left[f(t) - \\widehat{f(t)}\\right]^{1/2}\\, dt\\) as indicated by the area shaded in \\(\\color{blue}{\\text{blue}}\\).\n\n\n\n\n\nFigure 33.1: Projecting \\(f(t) \\equiv t^2\\) onto \\(g(t) \\equiv \\text{one}(t)\\).\n\n\n\n\n\nThe table links to audio files recorded by a human speaker voicing various vowels. Play the sounds to convince yourself that they really are the vowels listed. (It may help to use the controls to slow down the playback.)\nVowel | Player\n------|-------\n\"o\" as in \"stone\" | <audio controls><source src = \"https://linguistics.ucla.edu/people/hayes/103/Charts/VChart/o.wav\" type = \"audio/wav\"></audio>\n\"e\" as in \"eel\" | <audio controls><source src = \"https://linguistics.ucla.edu/people/hayes/103/Charts/VChart/y.wav\" type = \"audio/wav\"></audio>\nAs you may know, the physical stimuli involved in sound are rapid oscillations in air pressure. Our standard model for oscillations is the sinusoid function, which is parameterized by its period and it’s amplitude. The period of a sound oscillation is short: between 0.3 and 10 milliseconds. The amplitude is small. To get a sense for how small, consider the change in air pressure when you take an elevator up 10 stories in a building. The pressure amplitude of sound at a conversational level of loudness corresponds to taking that elevator upward by 1 to 10 mm.\nThe shapes of the “e” (as in “eel”) and “o” (as in “stone”) sound waves—in short, the waveforms—are drawn in ?fig-sound-waves.\n\n\n\n\n\nThe waveforms of two vowel sounds. Only about five hundredths of a second is shown.\n\n\n\n\nThe function resembles none of our small set of pattern-book functions. It is more complicated, more detailed, more irregular than any of the basic modeling functions featured in this book.\nFor many tasks it’s helpful to have a modeling approach that’s well suited to such detailed and irregular functions. For example, we might want to identify the speaker from a recording, or to play the recording slower or faster without changing the essence of the sound, or to tweak the function to have additional properties such as being exactly on tune while maintaining its individuality as a sound.\nA remarkable aspect of the waveforms in ?fig-sound-waves is their periodicity. The 0.05 sec graphics domain shown includes roughly seven repetitions of a basic waveform. That is, each cycle lasts about \\(\\frac{0.05 \\text{s}}{7} \\approx 7 \\text{ms}\\). what distinguishes the “e” waveform from the “o” waveform is the shape of the waveform that’s being repeated. The individual cycle of the “o” has three peaks of diminishing amplitude. The “e” cycle has two main peaks, high then low. It also has a very fast wiggle superimposed on the two peaks.\nAn important strategy for modeling such complicated oscillations is to decompose (synonym: analyze) them into a linear combination of simpler parts."
  },
  {
    "objectID": "Linear-combinations/B5-functions.html#sinusoids-as-vectors",
    "href": "Linear-combinations/B5-functions.html#sinusoids-as-vectors",
    "title": "33  Functions as vectors",
    "section": "33.2 Sinusoids as vectors",
    "text": "33.2 Sinusoids as vectors\nThe sinusoid is our fundamental model of periodic phenomena. To get started with using sinusoids as vectors, we’ll start with a simple setting: a single sinusoid of a specified frequency.\nFigure 33.2 shows three sinusoids all with the same frequency, but shifted somewhat in time:\n\n\n\n\n\nFigure 33.2: Three sinusoids with a frequency of \\(\\omega=3\\) cycles per second.\n\n\n\n\nSince we have a dot product for functions, we can treat each of the three sinusoids as a vector. For instance, consider the length of waveforms A and B and the included angle between them.\n\n\n## vector lengths \nlengthA <- Integrate(waveA(t) * waveA(t) ~ t, domain(t=0:1)) %>% sqrt() \nlengthA\n## [1] 0.7071068\nlengthB <- Integrate(waveB(t) * waveB(t) ~ t, domain(t=0:1)) %>% sqrt()\nlengthB\n## [1] 0.7071068\nlengthC <- Integrate(waveC(t) * waveC(t) ~ t, domain(t=0:1)) %>% sqrt()\nlengthC\n## [1] 0.7071068\n## dot products\ndotAB   <- Integrate(waveA(t) * waveB(t) ~ t, domain(t=0:1)) \ndotAB\n## [1] -3.984443e-18\ndotAC   <- Integrate(waveA(t) * waveC(t) ~ t, domain(t=0:1))\ndotAC\n## [1] -0.1545085\ndotBC   <- Integrate(waveB(t) * waveC(t) ~ t, domain(t=0:1))\ndotBC\n## [1] -0.4755283\n\n\nThe cosine of the included angle \\(\\theta\\) between functions A and B is calculated using the the dot product formula: \\[\\cos(\\theta) = \\frac{A\\bullet B}{\\|A\\|\\, \\|B\\|}\\] or, computationally\n\ndotAB / (lengthA * lengthB)\n## [1] -7.968886e-18\n\nSince \\(\\cos(\\theta) = 0\\), wave A and B are orthogonal. Admittedly, there is no right angle to be perceived from the graph, but the mathematics of angles gives this result.\nThe graphical presentation of orthogonality between waveforms A and B is easier to appreciate if we plot out the dot product itself: the integral of waveform A times waveform B. Figure 33.3 shows this integral using colors, blue for positive and orange for negative. The integral is zero, since the positive (blue) areas exactly equal the negative (orange) areas.\n\n\n\n\n\nFigure 33.3: The dot product between waveforms A and B, graphically.\n\n\n\n\nIn contrast, waveform A is not orthogonal to waveform C, and similarly for waveform B. ?fig-AC-BC shows this graphically: the positive and negative areas in the two integrals do not cancel out to zero.\n\n\n\n\n\nThe dot products between waveforms A and C (top panel) and between B and C (bottom panel).\n\n\n\n\nWe can project waveform C onto the 2-dimensional subspace spanned by A and B. Since waveforms A and B are orthogonal, This can be done simply by projecting C onto each of A and B one at a time. Here’s a calculation of the scalar multipliers for A and for B and the model vector (that is, the component of C in the A-B subspace):\n\nA_coef <- dotAC / lengthA^2\nB_coef <- dotBC / lengthB^2\nmod_vec <- makeFun(A_coef*waveA(t) + B_coef*waveB(t) ~ t)\n# length of mod_vec\nIntegrate(mod_vec(t)*mod_vec(t) ~ t, domain(t=0:1)) %>% sqrt()\n## [1] 0.7071068\n\nYou can see that the length of the model vector is the same as the length of the vector being projected. This means that waveform C lies exactly in the subspace spanned by waveforms A and B.\nA time-shifted sinusoid of frequency \\(\\omega\\) can always be written as a linear combination of \\(\\sin(2\\pi\\omega t)\\) and \\(\\cos(2\\pi\\omega t)\\). The coefficients of the linear combination tell us both the amplitude of the time-shifted sinusoid and the time shift.\n\nConsider the function \\(g(t) \\equiv 17.3 \\sin(2*pi*5*(t-0.02)\\) on the domain \\(0 \\leq t \\leq 1\\) seconds. The amplitude is 17.3. The time shift is 0.02 seconds. Let’s confirm this using the coefficients on the linear combination of sine and cosine of the same frequency.\n\ng <- makeFun(17.3 * sin(2*pi*5*(t-0.02)) ~ t)\nsin5 <- makeFun(sin(2*pi*5*t) ~ t)\ncos5 <- makeFun(cos(2*pi*5*t) ~ t)\nA_coef <- Integrate(g(t) * sin5(t) ~ t, domain(t=0:1)) /\n  Integrate(sin5(t) * sin5(t) ~ t, domain(t=0:1))\nA_coef\n## [1] 13.99599\nB_coef <- Integrate(g(t)*cos5(t) ~ t, domain(t=0:1)) /\n  Integrate(cos5(t) * cos5(t) ~ t, domain(t=0:1))\nB_coef\n## [1] -10.16868\n\nThe amplitude of \\(g(t)\\) is the Pythagorean sum of the two coefficients: ::: {.cell layout-align=“center” fig.showtext=‘false’}\nsqrt(A_coef^2 + B_coef^2)\n## [1] 17.3\n\nThe time delay involves the ratio of the two coefficients:\n\natan2(B_coef, A_coef) / (2*pi*5) \n## [1] -0.02\n\nFor our purposes here, we’ll need only the Pythagorean sum and will ignore the time delay. :::\n?fig-cello-seg (top) shows the waveform of a note played on a cello. The note lasts about 1 second. The bottom panel zooms in on the waveform, showing 82 ms (that is, 0.082 s).\n\n\n\n\n\nWaveform recorded from a cello.\n\n\n\n\nThe whole note starts with a sharp “attack,” followed by a long period called a “sustain,” and ending with a “decay.” Within the sustain and decay, the waveform is remarkably repetitive, seen best in the bottom panel of the figure.\nIf you count carefully in the bottom panel, you’ll see that the waveform completes 9 cycles in the 0.082 s graphical domain. This means that the period is 0.082 / 9 = 0.0091 s. The frequency \\(\\omega\\) is the reciprocal of this: 1/0.0091 = 109.76 Hz. That is, the cello is vibrating about 110 times per second.\nIn modeling the cello waveform as a linear combination of sinusoids, the frequencies we use ought to respect the period of the cello vibration. Figure 33.4 shows the original waveform as well as the projection of the waveform onto a sinusoid with a frequency of 109.76 Hz. The figure also shows the residual from the projection, which is simply the original waveform minus the projected version.\n\n\n\n\n\nFigure 33.4: Top: The cello waveform and its projection onto a sinusoid with frequency \\(\\omega = 109.76\\) Hz. Bottom: The residual from the projection.\n\n\n\n\nThe sinusoid with \\(\\omega = 109.76\\) is not the only one that will repeat every 0.0091 s. So will a sinusoid with frequency \\(2\\omega = 219.52\\), one with frequency \\(3\\omega = 329.28\\) and so on. These multiples of \\(\\omega\\) are called the harmonics of that frequency. In Figure 33.5 (top) the cello waveform is projected onto \\(\\omega\\) and its first harmonic \\(2\\omega\\). In the middle panel, the projection is made onto \\(\\omega\\) and its first three harmonics. In the bottom panel, the projection is onto \\(\\omega\\) and its first eight harmonics.\n\n\n\n\n\nFigure 33.5: ?(caption)\n\n\n\n\nAs the number of harmonics increases, the approximation gets better and better.\nUntil now, all the plots of the cello waveform have been made in what’s called the time domain. That is, the horizontal axis of the plots has been time, as seems natural for a function of time.\nThe decomposition into sinusoids offers another way of describing the cello waveform: the frequency domain. In the frequency domain, we report the amplitude and phase of the projection onto each frequency, plotting that versus frequency. Figure 33.6 shows the waveform in the frequency domain.\n\n\n\n\n\nFigure 33.6: The frequency domain description of the cello waveform.\n\n\n\n\nFrom the amplitude graph in Figure 33.6, you can see that only a handful of frequencies account for almost all of the signal. Thus, the frequency domain representation is in many ways much more simple and compact than the time domain representation.\nThe frequency domain description is an important tool in many fields. As you’ll see in Block 6, models of many kinds of systems, from the vibrations of buildings during an earthquake, aircraft wings in response to turbulence, and the bounce of a car moving over a rutted road have a very simple form when stated in the frequency domain. Each sinusoid in the input (earthquake shaking, air turbulence, rutted road) gets translated into the same frequency sinusoid in the output (building movement, wing bending, car bound): just the amplitude and phase of the sinusoid is altered.\nThe construction of the frequency domain description from the waveform is called a Fourier Transform, one of the most important techiques in science.\n\nAn important tool in chemistry is molecular vibrational spectroscopy in which a sample of the material is illuminated by an infrared beam of light. The frequency of infrared light ranges from about \\(300 \\times 10^7\\) Hz to \\(400 \\times 10^{10}\\) Hz, about 30 million to 40 billion times faster than the cello frequency.\nInfrared light is well suited to trigger vibrations in the various bonds of a molecule. By measuring the light absorbed at each frequency, a frequency domain picture can be drawn of the molecules in the sample. This picture can be compared to a library of known molecules to identify the makeup of the sample.\nThe analogous procedure for stringed musical instruments such as the cello or violin would be to rap on the instrument and record the hum of the vibrations induced. The Fourier transform of these vibrations effectively paint a picture of the tonal qualities of the instrument."
  },
  {
    "objectID": "Linear-combinations/B5-functions.html#exercises",
    "href": "Linear-combinations/B5-functions.html#exercises",
    "title": "33  Functions as vectors",
    "section": "33.3 Exercises",
    "text": "33.3 Exercises"
  },
  {
    "objectID": "accumulation-part.html",
    "href": "accumulation-part.html",
    "title": "Accumulation",
    "section": "",
    "text": "This is where I’ll explain what the block is about and the overall goals."
  },
  {
    "objectID": "Accumulation/27-intro.html",
    "href": "Accumulation/27-intro.html",
    "title": "34  Change & accumulation",
    "section": "",
    "text": "Every 10 years, starting in 1790, the US Census Bureau carries out a constitutionally mandated census: a count of the current population. The overall count as a function of year is shown in Figure 34.1. [Source]\nIn the 230 years spanned by the census data, the US population has grown 100-fold, from about 4 million in 1790 to about 330,000,000 in 2020.\nIt’s tempting to look for simple patterns in such data. Perhaps the US population has been growing exponentially. A semi-log plot of the same data suggests that the growth is only very roughly exponential. A truly exponential process would present as a curve with a constant derivative, but the derivative of the function in the graph is decreasing over the centuries.\nInsofar as the slope over the semi-log graph is informative, it amounts to this quantity: \\[\\partial_t \\ln(\\text{pop}(t)) = \\frac{\\partial_t\\, \\text{pop}(t)}{\\text{pop}}\\] This is the per-capita rate of growth, that is, the rate of change in the population divided by the population. Conventionally, this fraction is presented as a percentage: percentage growth in the population per year, as in Figure 34.2.\nThe dots in the graph are a direct calculation from the census data. There’s a lot of fluctuation, but an overall trend stands out: the population growth rate has been declining since the mid-to late 1800s. The deviations from the trend are telling and correspond to historical events. There’s a relatively low growth rate seen from 1860 to 1870: that’s the effect of the US Civil War. The Great depression is seen in the very low growth from 1930 to 1940. Baby Boom: look at the growth from 1950-1960. The bump from 1990 to 2000? Not coincidentally, the 1990 Immigration Act substantially increased the yearly rate of immigration.\nIf the trend in the growth rate continues, the US will reach zero net growth about 2070, then continue with negative growth. Of course, negative growth is just decline. A simple prediction from Figure 34.2 is that the argmax of the US population—that is, the year that the growth rate reaches zero—will occur around 2070.\nHow large will the population be when it reaches its maximum?\nIn Block 2, we dealt with situations where we know the function \\(f(t)\\) and want to find the rate of change \\(\\partial_t f(t)\\). Here, we know the rate of change of the population and we need to figure out the population itself, in other words to figure out from a known \\(\\partial_t f(t)\\) what is the unknown function \\(f(t)\\).\nThe process of figuring out \\(f(t) \\longrightarrow \\partial_t f(t)\\) is, of course, called differentiation. The opposite process, \\(\\partial_t f(t) \\longrightarrow f(t)\\) is called anti-differentiation.\nIn this block we’ll explore the methods for calculating anti-derivatives and some of the settings in which anti-derivative problems arrive."
  },
  {
    "objectID": "Accumulation/27-intro.html#accumulation",
    "href": "Accumulation/27-intro.html#accumulation",
    "title": "34  Change & accumulation",
    "section": "34.1 Accumulation",
    "text": "34.1 Accumulation\nImagine a simple setting: water flowing out of a tap into a basin or tank. The amount of water in the basin will be measured in a unit of volume, say liters. Measurement of the flow \\(f(t)\\) of water from the tap into the tank has different units, say liters per second. If volume \\(V(t)\\) is the volume of water in the tank as a function of time, \\(f(t)\\) at any instant is \\(f(t) = \\partial_t V(t)\\).\nClearly there is a relationship between the two functions \\(f(t)\\) and \\(V(t)\\). With derivatives, we can give a good description of that relationship: \\[f(t) = \\partial_t V(t)\\] This description will be informative if we have measured the volume of water in the basin as a function of time and want to deduce the rate of flow from the tap. Now suppose we have measured the flow \\(f(t)\\) and want to figure out the volume. The volume at any instant is the past flow accumulated to that instant. As a matter of notation, we write this view of the relationship as \\[V(t) = \\int f(t) dt,\\] which you can read as “volume is the accumulated flow.”\nOther examples of accumulation and change:\n\nvelocity is the rate of change of position with respect to time. Likewise, position is the accumulation of velocity over time.\nforce is the rate of energy with respect to position. Likewise energy is the accumulation of force as position changes.\ndeficit is the rate of change of debt with respect to time. Likewise, debt is the accumulation of deficit over time."
  },
  {
    "objectID": "Accumulation/27-intro.html#notation-for-anti-differentiation",
    "href": "Accumulation/27-intro.html#notation-for-anti-differentiation",
    "title": "34  Change & accumulation",
    "section": "34.2 Notation for anti-differentiation",
    "text": "34.2 Notation for anti-differentiation\nFor differentiation we are using the notation \\(\\partial_x\\) as in \\(\\partial_x f(x)\\). Remember that the subscript on \\(\\partial\\) names the with-respect-to input. There are three pieces of information this notation:\n\nThe \\(\\color{magenta}{\\partial}\\) symbol which identifies the operation as partial differentiation.\nThe name of the with-respect-to input \\(\\partial_{\\color{magenta}{x}}\\) written as a subscript to \\(\\partial\\).\nThe function to be differentiated, \\(\\partial_x \\color{magenta}{f(x)}\\).\n\nFor anti-differentiation, our notation must also specify the three pieces of information. It might be tempting to use the same notation as differentiation but replace the \\(\\partial\\) symbol with something else, perhaps \\(\\eth\\) or \\(\\spadesuit\\) or \\(\\forall\\), giving us something like \\(\\spadesuit_x f(x)\\).\nConvention has something different in store. The notation for anti-differentiation is \\[\\large \\int f(x) dx\\] 1. The \\(\\color{magenta}{\\int}\\) is the marker for anti-differentiation. 2. The name of the with-respect-to input is contained in the “dx” at the end of the notation: \\(\\int f(x) d\\color{magenta}{x}\\) 3. The function being anti-differentiated is in the middle \\(\\int \\color{magenta}{f(x)} dx\\).\nFor those starting out with anti-differentiation, the conventional notation can be confusing, especially the \\(dx\\) part. It’s easy confuse \\(d\\) for a constant and \\(x\\) for part of the function being anti-differentiated.\nThink of the \\(\\int\\) and the \\(dx\\) as brackets around the function. You need both brackets for correct notation, the \\(\\int\\) and the \\(dx\\) together telling you what operation to perform.\nRemember that just as \\(\\partial_x f(x)\\) is a function, so is \\(\\int f(x) dx\\)."
  },
  {
    "objectID": "Accumulation/27-intro.html#rmosaic-notation",
    "href": "Accumulation/27-intro.html#rmosaic-notation",
    "title": "34  Change & accumulation",
    "section": "34.3 R/mosaic notation",
    "text": "34.3 R/mosaic notation\nRecall that the notation for differentiation in R/mosaic is D(f(x) ~ x). The R/mosaic notation for anti-differentiation is very similar:\nD(f(x) ~ x)\nThis has the same three pieces of information as \\(\\partial_x f(x)\\)\n\nD() signifies differentiation whereas antiD() signifies anti-differentiation.\n~ x identifies the with-respect-to input.\nf(x) ~ is the function on which the operation is to be performed.\n\nRemember that just as D(f(x) ~ x) creates a new function out of f(x) ~ x, so does antiD(f(x) ~ x)."
  },
  {
    "objectID": "Accumulation/27-intro.html#dimension-and-anti-differentiation",
    "href": "Accumulation/27-intro.html#dimension-and-anti-differentiation",
    "title": "34  Change & accumulation",
    "section": "34.4 Dimension and anti-differentiation",
    "text": "34.4 Dimension and anti-differentiation\nThis entire block will be about anti-differentiation, its properties and its uses. You already know that anti-differentiation (as the name suggests) is the inverse of differentiation. There is one consequence of this that is helpful to keep in mind as we move on to other chapters. This being calculus, the functions that we construct and operate upon have inputs that are quantities and outputs that are also quantities. Every quantity has a dimension, as discussed in Chapter 16. When you are working with any quantity, you should be sure that you know its dimension and its units.\nThe dimension of the input to a function does not by any means have to be the same as the dimension of the output. For instance, we have been using many functions where the input has dimension time and the output is position (dimension L) or velocity (dimension L/T) or acceleration (dimension L/T\\(^2\\)).\nImagine working with some function \\(f(y)\\) that’s relevant to some modeling project of interest to you. Returning to the bracket notation that we used in Chapter 16, the dimension of the input quantity will be [\\(y\\)]. The dimension of the output quantity is [\\(f(y)\\)]. (Remember from 16 that [\\(y\\)] means “the dimension of quantity \\(y\\)” and that [\\(f(y)\\)] means “the dimension of the output from \\(f(y)\\).”)\nThe function \\(\\partial_y f(y)\\) has the same input dimension \\([y]\\) but the output will be \\([f(y)] / [y]\\). For example, suppose \\(f(y)\\) is the mass of fuel in a rocket as a function of time \\(y\\). The output of \\(f(y)\\) has dimension M. The input dimension \\([y]\\) is T.\nThe output of the function \\(\\partial_y f(y)\\) has dimension \\([f(y)] / [y]\\), which in this case will be M / T. (Less abstractly, if the fuel mass is given in kg, and time is measured in seconds, then \\(\\partial_y f(y)\\) will have units of kg-per-second.)\nHow about the dimension of the anti-derivative \\(F(y) = \\int f(y) dy\\)? Since \\(F(y)\\) is the anti-derivative of \\(f(y)\\) (with respect to \\(y\\)), we know that \\(\\partial_y F(y) = f(y)\\). Taking the dimension of both sides \\[[\\partial_y F(y)] = \\frac{[F(y)]}{[y]} = \\frac{[F(y)]}{\\text{T}} = [f(y)] = \\text{M}\\] Consequently, \\([F(y)] = \\text{M}\\).\nTo summarize:\n\nThe dimension of derivative \\(\\partial_y f(y)\\) will be \\([f(y)] / [y]\\).\nThe dimension of the anti-derivative \\(\\int f(y) dy\\) will be \\([f(y)]\\times [y]\\).\n\nOr, more concisely:\n\nDifferentiation is like division, anti-differentiation is like multiplication.\n\nPaying attention to the dimensions (and units!) of input and output can be a boon to the calculus student. Often students have some function \\(f(y)\\) and they are wondering which of the several calculus operations they are supposed to do: differentiation, anti-differentiation, finding a maximum, finding an argmax or a zero. Start by figuring out the dimension of the quantity you want. From that, you can often figure out which operation is appropriate.\nTo illustrate, imagine that you have constructed \\(f(y)\\) for your task and you know, say, \\[[f(y)] = \\text{M       and} \\  \\ \\ \\ \\ [y] = \\text{T}\\ .\\] Look things up in the following table:\n\n\n\nDimension of result\nCalculus operation\n\n\n\n\nM / T\ndifferentiate\n\n\nM T\nanti-differentiate\n\n\nM\nfind max or min\n\n\nT\nfind argmax/argmin or a function zero\n\n\nM T\\(^2\\)\nanti-differentiate twice in succession\n\n\nM / T\\(^2\\)\ndifferentiate twice in succession\n\n\n\nFor example, suppose the output of the accelerometer on your rocket has dimension L / T\\(^2\\). You are trying to figure out from the accelerometer reading what is your altitude. Altitude has dimension L. Look up in the table to see that you want to anti-differentiate acceleration twice in succession."
  },
  {
    "objectID": "Accumulation/27-intro.html#sec-preliminary-terrors",
    "href": "Accumulation/27-intro.html#sec-preliminary-terrors",
    "title": "34  Change & accumulation",
    "section": "34.5 From Calculus Made Easy",
    "text": "34.5 From Calculus Made Easy\nCalculus Made Easy, by Silvanus P. Thompson, is a classic, concise, and elegant textbook from 1910. It takes a common-sense approach, sometimes lampooning the traditional approach to teaching calculus.\n\nSome calculus-tricks are quite easy. Some are enormously difficult. The fools who write the textbooks of advanced mathematics—and they are mostly clever fools—seldom take the trouble to show you how easy the easy calculations are. On the contrary, they seem to desire to impress you with their tremendous cleverness by going about it in the most difficult way. — From the preface\n\nThompson’s first chapter starts with the notation of accumulation, which he calls “the preliminary terror.”\n\nThe preliminary terror … can be abolished once for all by simply stating what is the meaning—in common-sense terms—of the two principal symbols that are used in calculating.\nThese dreadful symbols are:\n\n\\(\\Large\\  d\\) which merely means “a little bit of.”\n\nThus \\(dx\\) means a little bit of \\(x\\); or \\(du\\) means a little bit of \\(u\\). Ordinary mathematicians think it more polite to say “an element of,” instead of “a little bit of.” Just as you please. But you will find that these little bits (or elements) may be considered to be indefinitely small.\n\n\\(\\ \\ \\large\\int\\) which is merely a long \\(S\\), and may be called (if you like) “the sum of.”\n\nThus \\(\\ \\int dx\\) means the sum of all the little bits of \\(x\\); or \\(\\ \\int dt\\) means the sum of all the little bits of \\(t\\). Ordinary mathematicians call this symbol “the integral of.” Now any fool can see that if \\(x\\) is considered as made up of a lot of little bits, each of which is called \\(dx\\), if you add them all up together you get the sum of all the \\(dx\\)’s, (which is the same thing as the whole of \\(x\\)). The word “integral” simply means “the whole.” If you think of the duration of time for one hour, you may (if you like) think of it as cut up into \\(3600\\) little bits called seconds. The whole of the \\(3600\\) little bits added up together make one hour.\nWhen you see an expression that begins with this terrifying symbol, you will henceforth know that it is put there merely to give you instructions that you are now to perform the operation (if you can) of totaling up all the little bits that are indicated by the symbols that follow.\n\n\nThe next chapter shows what it means to “total up all the little bits” of a function."
  },
  {
    "objectID": "Accumulation/27-intro.html#exercises",
    "href": "Accumulation/27-intro.html#exercises",
    "title": "34  Change & accumulation",
    "section": "34.6 Exercises",
    "text": "34.6 Exercises"
  },
  {
    "objectID": "Accumulation/28-visualizing.html",
    "href": "Accumulation/28-visualizing.html",
    "title": "35  Totaling the little bits",
    "section": "",
    "text": "Many students wonder how it is possible to reconstruct a function \\(F(x)\\) from its derivative \\(f(x)\\). The point of this short chapter is to help you develop some intuition about anti-differentiation.\nYou already know the notation meaning “\\(F(x)\\) is the anti-derivative of \\(f(x)\\)”: \\[\\large \\int f(x)\\, dx\\ .\\] In drawing a graph of \\(F(x)\\), we will of course want to use coordinate axes where the quantity \\(x\\) is represented on the horizontal axis and the quantity of the output \\(F(x)\\) is on the vertical axis:\nIt’s premature to have drawn a segment of \\(F(x)\\) because we haven’t yet undertaken to compute \\(F(x) = \\int f(x)\\, dx\\). At this point in the process, all we know is \\(f(x)\\), not \\(F(x)\\). Still, since we know \\(f(x)\\), we do know the slope of the little segment of \\(F(x)\\). We just don’t know where that segment should be located vertically in each of the \\(dx\\) regions that make up the whole domain.\nWe can’t draw \\(f(x)\\) in the ordinary way as a curve wending its way across the domain of the graph. Why not? Because the vertical axis of the graphics frame is in terms of \\(F(x)\\) and has a different dimension than the output of \\(f(x)\\).\nBut we can draw \\(f(x)\\) in terms of the slope of a segment of horizontal extent \\(dx\\), so long as we accept that the vertical position of that segment means nothing: \\(f(x)\\) gives information only about the slope of the segment. The best we can do at this point is to graph \\(f(x)\\) in terms of sloping segments, as in Figure 35.2.\nEach of the segments in Figure 35.2 has the same horizontal extent, namely \\(dx\\). When we draw a sloping segment over the tiny bit \\(dx\\) of the domain, the vertical extent of the segment will be the product of the width \\(dx\\) and the slope \\(f(x)\\). That is, the vertical extent will be the product \\(f(x) dx\\).\nWhenever we know a function \\(f(x)\\) and have chosen a size for \\(dx\\) we can draw a graph of \\(f(x)\\) in the form shown in Figure 35.2. We’re drawing it in this unusual way because we want the graphics frame to be all ready for drawing the graph of \\(F(x)\\) in the normal fashion after we have figured out what \\(F(x)\\) results from accumulating/summing-up all the little \\(f(x) dx\\) segments. When we write \\(\\large\\int\\) in the notation \\[\\large \\int f(x)\\, dx\\] we mean, “sum up all the \\(f(x) dx\\) segments.”\nLet’s now consider how to “sum up all the segments.” We’ll start in Figure 35.3 with an example where we already know \\(F(x)\\). That way, we can see of our sum of the \\(f(x) dx\\) segments really does reconstruct \\(F(x)\\).\nNow imagine that we sliced up \\(F(x)\\) over small sub-domains of \\(x\\), as in Figure 35.3 (bottom). That is, we approximated \\(F()\\) piecewise locally. But we’ve broken the continuity of \\(F(x)\\) by moving each slice up or down so that the left-most point has value 0.\nCan you reconstruct \\(F(x)\\) from the local segments?\nStart by reading off the function value from the last point in the left-most segment. That’s been marked in Figure 35.3 with a blue dot. The function value at that dot is 7.072.\nNow take the second segment. The idea is to move that segment upward until it joins the first segment at the blue dot. You can do that by adding 7.072 to the second segment. The result is shown in Figure 35.4(top).\nNow read off the new value at the end of the second segment, it’s 4.198. Add this amount to the third segment as in Figure 35.4(bottom).\nContinue this process until you have reconstructed \\(F(x)\\) from the local segments.\nYou may object: “Of course you can reconstruct \\(F(x)\\) from the local segments, but this isn’t the same as reconstructing \\(F(x)\\) from its derivative \\(\\partial_x F(x)\\).” My answer: “That depends on how many segments you use.”\nWhen we make the segment width \\(h\\) smaller and smaller, the individual segments become more and more like straight lines. Figure 35.5 shows the segments for smaller and smaller \\(h\\).\nNotice that many of the segments are straight lines. That’s understandable, since any function looks like a straight line over a small enough domain.\nEach of those straight-line segments is drawn over a domain \\(x_i < x < x_i+dx\\) that has width \\(dx\\). For \\(dx\\) small enough, the segment is well approximated by a straight line with slope \\(\\partial_x F(x_i)\\). Multiplying slope by width \\(dx\\) gives the segment height: \\(\\left[{\\large\\strut}\\partial_x F(x_i)\\right]\\ dx\\). Of course, remember that \\(\\partial_x F(x) = f(x)\\) helps us see that each of the little segments is \\(f(x_i)\\ dx\\).\nLets review. The standard notation for anti-differentiation can be interpreted in terms of putting together segments, or, in the words of Prof. Thompson in Calculus Made Easy, “totaling up all the little bits.” (See Section 34.5.)\nAltogether, we have:\n\\[\\large \\underbrace{\\underbrace{\\Large\\color{magenta}{\\int}}_{\\color{magenta}{\\text{assemble}}} \\underbrace{\\Large \\overbrace{f(x)}^{\\small\\text{slope of F(x)}}\\ \\  \\overbrace{\\strut dx}^{\\small \\text{bits of}\\ x}}_{\\color{blue}{\\text{the slope segments}}}}_{\\text{giving}\\ {\\Large F(x)+C}\\ \\text{altogether.}}\\]"
  },
  {
    "objectID": "Accumulation/28-visualizing.html#exercises",
    "href": "Accumulation/28-visualizing.html#exercises",
    "title": "35  Totaling the little bits",
    "section": "35.1 Exercises",
    "text": "35.1 Exercises"
  },
  {
    "objectID": "Accumulation/29-integration.html",
    "href": "Accumulation/29-integration.html",
    "title": "36  Integration",
    "section": "",
    "text": "Anti-derivatives are useful when you know how a quantity is changing but don’t yet know the quantity itself.\nIt’s important, of course, to keep track of which is the “quantity itself” and which is the “rate of increase in that quantity.” This always depends on context and your point of view. It’s convenient, then, to set some fixed examples to make it easy to keep track of which quantity is which.\nWe’ll also adopt a convention to make it simpler to recognize which quantity is the “quantity itself” and which is the “rate of increase in that quantity.” We will use CAPITAL LETTERS to name functions that are the quantity itself, and lower-case letters for the rate of increase in that quantity. For example, if talking about motion, an important quantity is momentum and how it changes over time. The momentum itself will be \\({\\mathbf M}(t)\\) while the rate of increase of momentum will be \\(m(t)\\).1 The amount of money a business has on hand at time \\(t\\) is \\({\\mathbf S}(t)\\) measured, say, in dollars. The rate of increase of that money is \\(s(t)\\), in, say, dollars per day.\nNotice that we’re using the phrase “rate of increase” rather than “rate of change.” That’s because we want to keep straight the meaning of the sign of the lower-case function. If \\(m(t)\\) is positive, the momentum is increasing. If \\(m(t)\\) is negative then it’s a “negative rate of increase,” which is, of course, just a “decrease.”\nFor a business, money coming in means that \\(s(t)\\) is positive. Expenditures of money correspond to \\(s(t)\\) being negative. In the fuel example. \\({\\mathbf F}(t)\\) is the amount of fuel in the tank. \\(f(t)\\) is the rate of increase in the amount of fuel in the tank. Of course, engines burn fuel, removing it from the tank. So we would write the rate at which fuel is burned as \\(-f(t)\\): removing fuel is a negative increase in the amount of fuel, an expenditure of fuel.\nThe objective of this chapter is to introduce you to the sorts of calculations, and their notations, that let you figure out how much the CAPITAL LETTER quantity has changed over an interval of \\(t\\) based on what you already know about the value over time of the lower-case function.\nThe first step in any such calculation is to find or construct the lower-case function \\(f(t)\\) or \\(c(t)\\) or \\(m(t)\\) or whatever it might be. This is a modeling phase. In this chapter, we’ll ignore detailed modeling of the situation and just present you with the lower-case function.\nThe second step in any such calculation is to compute the anti-derivative of the lower-case function, giving as a result the CAPITAL LETTER function. You’ve already seen the notation for this, e.g. \\[{\\Large  F(t) = \\int f(t) dt}\\ \\ \\ \\ \\ \\text{or}\\ \\ \\ \\ \\ {\\Large G(t) = \\int g(t) dt}\\ \\ \\ \\ \\text{and so on.}\\] In this chapter, we will not spend any time on this step; we will assume that you already have at hand the means to compute the anti-derivative. (Indeed, you already have antiD() available which will do the job for you.) Later chapters will look at the issues around and techniques for doing the computations by other means.\nThe remaining steps in such calculations are to work with the CAPITAL LETTER function to compute such things as the amount of that quantity, or the change in that quantity as it is accumulated over an interval of \\(t\\)."
  },
  {
    "objectID": "Accumulation/29-integration.html#net-change",
    "href": "Accumulation/29-integration.html#net-change",
    "title": "36  Integration",
    "section": "36.1 Net change",
    "text": "36.1 Net change\nPerhaps it goes without saying, but once you have the CAPITAL LETTER function, e.g. \\(F(t)\\), you can evaluate that function at any input that falls into the domain of \\(F(t)\\). If you have a graph of \\(F(t)\\) versus \\(t\\), just position your finger on the horizontal axis at input \\(t_1\\), then trace up to the function graph, then horizontally to the vertical axis where you can read off the value \\(F(t_1)\\). If you have \\(F()\\) in the form of a computer function, just apply \\(F()\\) to the input \\(t_1\\).\nIn this regard, \\(F(t)\\) is like any other function.\nHowever, in using and interpreting the \\(F(t)\\) that we constructed by anti-differentiating \\(f(t)\\), we have to keep in mind the limitations of the anti-differentiation process. In particular, any function \\(f(t)\\) does not have a unique anti-derivative function. If we have one anti-derivative, we can always construct another by adding some constant: \\(F(t) + C\\) is also an anti-derivative of \\(f(t)\\).\nBut we have a special purpose in mind when calculating \\(F(t_1)\\). We want to figure out from \\(F(t)\\) how much of the quantity \\(f(t)\\) has accumulated up to time \\(t_1\\). For example, if \\(f(t)\\) is the rate of increase in fuel (that is, the negative of fuel consumption), we want \\(F(t_1)\\) to be the amount of fuel in our tank at time \\(t_1\\). That cannot happen. All we can say is that \\(F(t_1)\\) is the amount of fuel in the tank at \\(t_1\\) give or take some unknown constant C.\nInstead, the correct use of \\(F(t)\\) is to say how much the quantity has changed over some interval of time, \\(t_0 \\leq t \\leq t_1\\). This “change in the quantity” is called the net change in \\(F()\\). To calculate the net change in \\(F()\\) from \\(t_0\\) to \\(t_1\\) we apply \\(F()\\) to both \\(t_0\\) and \\(t_1\\), then subtract:\n\\[\\text{Net change in}\\ F(t) \\ \\text{from}\\ t_0 \\ \\text{to}\\ t_1 :\\\\= F(t_1) - F(t_0)\\]\n\nSuppose you have already constructed the rate-of-change function for momentum \\(m()\\) and implemented it as an R function m(). For instance, \\(m(t)\\) might be the amount of force at any instant \\(t\\) of a car, and \\({\\mathbf M}(t)\\) is the accumulated force, better known as momentum. We’ll assume that the input to m() is in seconds, and the output is in kg-meters-per-second-squared, which has the correct dimension for force.\nYou want to find the amount of force accumulated between time \\(t=2\\) and \\(t=5\\) seconds.\n\n\n\n\n# You've previous constructed m(t)\nM <- antiD(m(t) ~ t)\nM(5) - M(2)\n## [1] -1.392131\n\nTo make use of this quantity, you’ll need to know it’s dimension and units. For this example, where the dimension [\\(m(t)\\)] is M L T\\(^{-2}\\), and [\\(t\\)] = T, the dimension [\\({\\mathbf M}(t)\\)] will be M L T\\(^{-1}\\). In other words, if the output of \\(m(t)\\) is kg-meters-per-second-squared, then the output of \\(V(t)\\) must be kg- meters-per-second."
  },
  {
    "objectID": "Accumulation/29-integration.html#the-definite-integral",
    "href": "Accumulation/29-integration.html#the-definite-integral",
    "title": "36  Integration",
    "section": "36.2 The “definite” integral",
    "text": "36.2 The “definite” integral\nWe have described the process of calculating a net change from the lower-case function \\(f(t)\\) in terms of two steps:\n\nConstruct \\(F(t) = \\int f(t) dt\\).\nEvaluate \\(F(t)\\) at two inputs, e.g. \\(F(t_2) - F(t_1)\\), giving a net change, which we’ll write as \\({\\cal F}(t_1, t_2) = F(t_2) - F(t_1)\\).\n\nAs a matter of notation, the process of going from \\(f(t)\\) to the net change is written as one statement. \\[{\\cal F}(t_1, t_2) = F(t_2) - F(t_1) = \\int_{t_1}^{t_2} f(t) dt\\]\nThe punctuation \\[\\int_{t_1}^{t_2} \\_\\_\\_\\_ dt\\] captures in one construction both the anti-differentiation step (\\(\\int\\_\\_dt\\)) and the evaluation of the anti-derivative at the two bound \\(t_2\\) and \\(t_1\\).\nSeveral names are used to describe the overall process. It is important to become familiar with these.\n\n\\(\\int_a^b f(t) dt\\) is called a definite integral of \\(f(t)\\).\n\\(a\\) and \\(b\\) are called, respectively, the lower bound of integration and the upper bound of integration, although given the way we draw graphs it might be better to call them the “left” and “right” bounds, rather than lower and upper.\nThe pair \\(a, b\\) is called the bounds of integration.\n\nAs always, it pays to know what kind of thing is \\({\\cal F}(t_1, t_2)\\). Assuming that \\(t_1\\) and \\(t_2\\) are fixed quantities, say \\(t_1 = 2\\) seconds and \\(t_2 = 5\\) seconds, then \\({\\cal F}(t_1, t_2)\\) is itself a quantity. The dimension of that quantity is [\\(F(t)\\)] which in turn is [\\(f(t)\\)]\\(\\cdot\\)[\\(t\\)]. So if \\(f(t)\\) is fuel consumption in liters per second, then \\(F(t)\\) will have units of liters, and \\({\\cal F}(t_1, t_2)\\) will also have units of liters.\nRemember also an important distinction:\n\n\\(F(t) = \\int f(t) dt\\) is a function whose output is a quantity.\n\\(F(t_2) - F(t_1) = \\int_{t_1}^{t_2} f(t) dt\\) is a quantity, not a function.\n\nOf course, \\(f(t)\\) is a function whose output is a quantity. In general, the two functions \\(F(t)\\) and \\(f(t)\\) produce outputs that are different kinds of quantities. For instance, the output of \\(F(t)\\) is liters of fuel while the output of \\(f(t)\\) is liters per second: fuel consumption. Similarly, the output of \\(S(t)\\) is dollars, while the output of \\(s(t)\\) is dollars per day.\nThe use of the term definite integral suggests that there might be something called an indefinite integral, and indeed there is. “Indefinite integral” is just a synonym for “anti-derivative.” In this book we favor the use of anti-derivative because it’s too easy to leave off the “indefinite” and confuse an indefinite integral with a definite integral. Also, “anti-derivative” makes it completely clear what is the relationship to “derivative.”\nSince 1700, it’s common for calculus courses to be organized into two divisions:\n\nDifferential calculus, which is the study of derivatives and their uses.\nIntegral calculus, which is the study of anti-derivatives and their uses.\n\nMathematical notation having been developed for experts rather than for students, very small typographical changes are often used to signal very large changes in meaning. When it comes to anti-differentiation, there are two poles of fixed meaning and then small changes which modify the meaning. The poles are:\n\nAnti-derivative: \\(\\int f(t) dt\\), which is a function whose output is a quantity.\nDefinite integral \\(\\int_a^b f(t) dt\\), which is a quantity, plain and simple.\n\nBut you will also see some intermediate forms:\n\n\\(\\int_a^t f(t) dt\\), which is a function with input \\(t\\).\n\\(\\int_a^x f(t) dt\\), which is the same function as in (a) but with the input name \\(x\\) being used.\n\\(\\int_t^b f(t) dt\\), which is a function with input \\(t\\).\nLess commonly, \\(\\int_x^t f(t) dt\\) which is a function with two inputs, \\(x\\) and \\(t\\). The same is true of \\(\\int_x^y f(t) dt\\) and similar variations."
  },
  {
    "objectID": "Accumulation/29-integration.html#initial-value-of-the-quantity",
    "href": "Accumulation/29-integration.html#initial-value-of-the-quantity",
    "title": "36  Integration",
    "section": "36.3 Initial value of the quantity",
    "text": "36.3 Initial value of the quantity\nRecall that we’re interested in a real quantity \\({\\mathbf F}(t)\\), but we only know \\(f(t)\\) and from that can calculate an anti-derivative \\(F(t)\\). The relationship between them is \\[{\\mathbf F}(t) = F(t) + C\\] where \\(C\\) is some fixed quantity that we cannot determine directly from \\(f(t)\\).\nStill, even if we cannot determine \\(C\\), there’s one way we can use \\(F(t)\\) to make definite statements about \\({\\mathbf F}(t)\\). Consider the net change from \\(t_1\\) to \\(t_2\\) in the real quantity \\({\\mathbf F}\\). This is \\[{\\mathbf F}(t_2) - {\\mathbf F}(t_1) =  \\left[F(t_2) + C\\right] - \\left[F(t_1) + C\\right] = F(t_2) - F(t_1)\\] In other words, just knowing \\(F(t)\\), we can make completely accurate statements about net changes in the value of \\({\\mathbf F}(t)\\).\nLet’s develop our understanding of this unknown constant \\(C\\), which is called the constant of integration. To do so, watch the movie in Figure 36.1 showing the process of constructing the anti-derivative \\[F(t) = \\int_2^t f(t) dt\\ .\\]\n\n\n\n\n\nFigure 36.1: Constructing the anti-derivative \\(F(t)\\) by reading the slope from \\(f(t)\\) and using that slope to extend the picture of \\(F()\\)\n\n\n\n\n\nFocus first on the top graph. The function we are integrating, \\(f(t)\\), is known before we carry out the integration, so it is shown in the top graph.\n\n\\(f(t)\\) is the rate of increase in \\(F(t)\\) (or \\({\\mathbf F}(t)\\) for that matter). From the graph, you can read using the vertical axis the value of \\(f(t)\\) for any input \\(t\\). But since \\(f(t)\\) is a rate of increase, we can also depict \\(f(t)\\) as a slope. That slope is being drawn as a \\(\\color{magenta}{\\text{magenta}}\\) arrow. Notice that when \\(f(t)\\) is positive, the arrow slopes upward and when \\(f(t)\\) is negative, the arrow slopes downward. The steepness of the arrow is the value of \\(f(t)\\), so for inputs where the value of \\(f(t)\\) is far from zero the arrow is steeper than for values of \\(f(t)\\) that are near zero.\n\nNow look at both graphs, but concentrate just on the arrows in the two graphs. They are always the same: carbon copies of one another.\nFinally the bottom graph. We’re starting the integral at \\(t_1=2\\). Since nothing has yet been accumulated, the value \\(F(t_1 = 2) = 0\\). From (1) and (2), you know the arrow shows the slope of \\(F(t)\\). So as \\(F(t>2)\\) is being constructed the arrow guides the way. When the slope arrow is positive, \\(F(t)\\) is growing. When the slope arrow is negative, \\(F(t)\\) is going down.\n\nIn tallying up the accumulation of \\(f(t)\\), we started at time \\(t=2\\) and with \\(F(t=2) = 0\\). This makes sense, since nothing can be accumulated over the mere instant of time from \\(t=2\\) to \\(t=2\\). On the other hand, it was our choice to start at \\(t=2\\). We might have started at another value of \\(t\\) such as \\(t=0\\) or \\(t=-5\\) or \\(t=-\\infty\\). If so, then the accumulation of \\(f(t)\\) up to \\(t=2\\) would likely have been something other than zero.\nBut what if we knew an actual value for \\({\\mathbf F}(2)\\). This is often the case. For instance, before taking a trip you might have filled up the fuel tank. The accumulation of fuel consumption only tells you how much fuel has been used since the start of the trip. But if you know the starting amount of fuel, by adding that to the accumulation you’ll know instant by instant how much fuel is in the tank. In other words, \\[{\\mathbf F}(t) = {\\mathbf F}(2) + \\int_2^t f(t) dt\\ .\\] This is why, when we write an anti-derivative, we should always include mention of some constant \\(C\\)—the so-called constant of integration—to remind us that there is a difference between the \\(F(t)\\) we get from anti-differentiation and the \\({\\mathbf F}(t)\\) of the function we’re trying to reconstruct. That is, \\[{\\mathbf F}(t) = F(t) + C = \\int f(t) dt + C\\ .\\] We only need to know \\({\\mathbf F}(t)\\) at one point in time, say \\(t=0\\), to be able to figure out the value of \\(C\\): \\[C = {\\mathbf F}(0) - F(0)\\ .\\]\nAnother way to state the relationship between the anti-derivative and \\({\\mathbf F}(t)\\) is by using the anti-derivative to accumulate \\(f(t)\\) from some starting point \\(t_0\\) to time \\(t\\). That is: \\[{\\mathbf F}(t) \\ =\\  {\\mathbf F}(t_0) + \\int_{t_0}^t f(t)\\, dt\\  = \\\n{\\mathbf F}(t_0) + \\left({\\large\\strut}F(t) - F(t_0)\\right)\\]\n\nA famous legend has Galileo at the top of the Tower of Pisa around 1590. The legend illustrates Galileo’s finding that a light object (e.g. a marble) and a heavy object (e.g. a ball) will fall at the same speed. Galileo published his mathematical findings in 1638 in Discorsi e Dimostrazioni Matematiche, intorno a due nuove scienze. (English: Discourses and Mathematical Demonstrations Relating to Two New Sciences)\nIn 1687, Newton published his world-changingPhilosophiae Naturalis Principia Mathematica. (English: Mathematical Principles of Natural Philosophy)\nLet’s imagine the ghost of Galileo returned to Pisa in 1690 after reading Newton’s Principia Mathematica. In this new legend, Galileo holds a ball still in his hand, releases it, and figures out the position of the ball as a function of time.\nAlthough Newton famously demonstrated that gravitational attraction is a function of the distance between to objects, he also knew that at a fixed distance—the surface of the Earth—gravitational acceleration was constant. So Galileo was vindicated by Newton. But, although gravitational acceleration is constant from top to bottom of the Tower of Pisa, Galileo’s ball was part of a more complex system: a hand holding the ball still until release. Acceleration of the ball versus time is therefore approximately a Heaviside function:\n\\[\\text{accel}(t) \\equiv \\left\\{\\begin{array}{rl}0 & \\text{for}\\ t \\leq 3\\\\\n{-9.8}  & \\text{otherwise}\\end{array}\\right.\\]\n\naccel <- makeFun(ifelse(t <= 3, 0, -9.8) ~ t)\n\nAcceleration is the derivative of velocity. We can construct a function \\(V(t)\\) as the anti-derivative of acceleration, but the real-world velocity function will be \\[{\\mathbf V}(t) = {\\mathbf V}(0) + \\int_0^t \\text{accel}(t) dt\\]\n\nV_from_antiD <- antiD(accel(t) ~ t)\nV <- makeFun(V0 + (V_from_antiD(t) - V_from_antiD(0)) ~ t, V0 = 0)\n\nIn the computer expression, the parameter V0 stands for \\({\\mathbf V}(0)\\). We’ve set it equal to zero since, at time \\(t=0\\), Galileo was holding the ball still.\nVelocity is the derivative of position, but the real-world velocity function will be the accumulation of velocity from some starting time to time \\(t\\), plus the position at that starting time: \\[x(t) \\equiv x(0) + \\int_0^t V(t) dt\\] We can calculate \\(\\int V(t) dt\\) easily enough with antiD(), but the function \\(x(t)\\) involves evaluating that anti-derivative at times 0 and \\(t\\):\n\nx_from_antiD <- antiD(V(t) ~ t)\nx <- makeFun(x0 + (x_from_antiD(t) - x_from_antiD(0)) ~ t, x0 = 53)\n\nWe’ve set the parameter x0 to be 53 meters, the height above the ground of the top balcony on which Galileo was standing for the experiment.\n\n\n\n\n\nFigure 36.2: The acceleration, velocity, and position of the ball as a function of time in Galileo’s Tower of Pisa experiment. The ball is released at time \\(t_0\\).\n\n\n\n\n\nIn the (fictional) account of the 1690 experiment, we had Galileo release the ball at time \\(t=0\\). That’s a common device in mathematical derivations, but in a physical sense it’s entirely arbitrary. Galileo might have let go of the ball at any other time, say, \\(t=3\\) or \\(t=14:32:05\\).\nA remarkable feature of integrals is that it doesn’t matter what we use as the lower bound of integration, so long as we set the initial value to correspond to that bound.\n\nFor a while you were writing integrals like this: \\(\\int_a^b f(t) dt\\). Then you replaced \\(b\\) with the input name \\(t\\) to get \\(\\int_a^t f(t) dt\\). But then you switched everything up by writing \\(\\int_a^t f(x) dx\\). Is that the same as \\(\\int_a^t f(t) dt\\)? If so, why do you get to replace the \\(t\\) with \\(x\\) in some places but not in others?\nRecall from Chapter 5 that the names used for inputs to a function definition don’t matter so long as they are used consistently on the left and right sides of \\(\\equiv\\). For instance, all these are the same function:\n\n\\(f(x) \\equiv m x + b\\)\n\\(g(t) \\equiv m t + b\\)\n\\(h(\\text{wind}) \\equiv m \\text{wind} + b\\)\n\nNow think about the integral \\(\\int_a^b f(t) dt\\): \\[\\int_a^b f(t) dt = F(b) - F(a)\\ .\\]\nOn the left-hand side, the input name \\(t\\) is prominant, appearing in two places: \\(f(\\color{magenta}{t}) d\\color{magenta}{t}\\). But \\(t\\) is nowhere on the right-hand side. We could have equally well written this as \\(\\int_a^b f(x) dx\\) or \\(\\int_a^b f(\\text{wind}) d\\text{wind}\\). The name we use for the input to \\(f()\\) doesn’t matter so long as it is consistent with the name used in the \\(d\\_\\_\\) part of the notation. Often, the name placed in the blanks in \\(\\int f(\\_\\_) d\\_\\_\\) is called a dummy variable.\nWriting \\(\\int_a^t f(t) dt\\) is perfectly reasonable, but many authors dislike the multiple appearance of \\(t\\). So they write something like \\(\\int_a^t f(x) dx\\) instead."
  },
  {
    "objectID": "Accumulation/29-integration.html#integrals-from-bottom-to-top",
    "href": "Accumulation/29-integration.html#integrals-from-bottom-to-top",
    "title": "36  Integration",
    "section": "36.4 Integrals from bottom to top",
    "text": "36.4 Integrals from bottom to top\nThe bounds of integration appear in different arrangements. None of these are difficult to derive from the basic forms:\n\nThe relationship between an integral and its corresponding anti-derivative function: \\[\\int_a^b f(x) dx = F(b) - F(a)\\] This relationship has a fancy-sounding name: the second fundamental theorem of calculus.\nThe accumulation from an initial-value \\[{\\mathbf F}(b)\\  =\\  {\\mathbf F}(a) + \\int_a^b f(x) dx\\  = \\ {\\mathbf F}(a) + F(b) - F(a)\\] For many modeling situations, \\(a\\) and \\(b\\) are fixed quantities, so \\(F(a)\\) and \\(F(b)\\) are also quantities; the output of the anti-derivative function at inputs \\(a\\) and \\(b\\). But either the lower-bound or the upper-bound can be input names, as in \\[\\int_0^t f(x) dx = F(t) - F(0)\\]\n\nNote that \\(F(t)\\) is not a quantity but a function of \\(t\\).\nOn occasion, you will see forms like \\(\\int_t^0 f(x)dx\\). You can think of this in either of two ways:\n\nThe accumulation from a time \\(t\\) less than 0 up until 0.\nThe reverse accumulation from 0 until time \\(t\\).\n\nReverse accumulation can be a tricky concept because it violates everyday intuition. Suppose you were harvesting a row of ripe strawberries. You start at the beginning of the row—position zero. Then you move down the row, picking strawberries and placing them in your basket. When you have reached position \\(B\\) your basket holds the accumulation \\(\\int_0^B s(x)\\, dx\\), where \\(s(x)\\) is the lineal density of strawberries—units: berries per meter of row.\nBut suppose you go the other way, starting with an empty basket at position \\(B\\) and working your way back to position 0. Common sense says your basket will fill to the same amount as in the forward direction, and indeed this is the case. But integrals work differently. The integral \\(\\int_B^0 s(x) dx\\) will be the negative of \\(\\int_0^B s(x) dx\\). You can see this from the relationship between the integral and the anti-derivative: \\[\\int_B^0 s(x) dx \\ = \\ S(0) - S(B) \\ =\\ -\\left[{\\large\\strut}S(B) - S(0)\\right]\\ = \\ -\\int_0^B s(x) dx\\]\nThis is not to say that there is such a thing as a negative strawberry. Rather, it means that harvesting strawberries is similar to an integral in some ways (accumulation) but not in other ways. In farming, harvesting from 0 to \\(B\\) is much the same as harvesting from \\(B\\) to 0, but integrals don’t work this way.\nAnother property of integrals is that the interval between bounds of integration can be broken into pieces. For instance:\n\\[\\int_a^c f(x) dx \\ = \\ \\int_a^b f(x) dx + \\int_b^c f(x) dx\\] You can confirm this by noting that \\[\\int_a^b f(x) dx + \\int_b^c f(x) dx \\ = \\ \\left[{\\large\\strut}F(b) - F(a)\\right] + \\left[{\\large\\strut}F(c) - F(b)\\right] = F(c) - F(a) \\ = \\ \\int_a^c f(x) dx\\ .\\]\nFinally, consider this function of \\(t\\): \\[\\partial_t \\int_a^t f(x) dx\\ .\\] First, how do we know it is a function of \\(t\\)? \\(\\int_a^t f(x) dx\\) is a definite integral and has the value \\[\\int_a^t f(x) dx = F(t) - F(a)\\  .\\] Following our convention, \\(a\\) is a parameter and stands for a specific numerical value, so \\(F(a)\\) is the output of \\(F()\\) for a specific input. But according to convention \\(t\\) is the name of an input. So \\(F(t)\\) is a function whose output depends on \\(t\\). Differentiating the function \\(F(t)\\), as with every other function, produces a new function.\nSecond, there is a shortcut for calculating \\(\\partial_t \\int_a^t f(x) dx\\): \\[\\partial_t \\int_a^t f(x) dx\\ =\\ \\partial_t \\left[{\\large\\strut}F(t) - F(a)\\right]\\ .\\] Since \\(F(a)\\) is a quantity and not a function, \\(\\partial_t F(a) = 0\\). That simplies things. Even better, we know that the derivative of \\(F(t)\\) is simply \\(f(t)\\): that’s just the nature of the derivative/anti-derivative relationship between \\(f(t)\\) and \\(F(t)\\). Put together, we have: \\[\\partial_t \\int_a^t f(x) dx\\ =\\ f(t)\\ .\\]\nThis complicated-looking identity has a fancy name: the first fundamental theorem of calculus.\n\nBacktracking the stars.\nIn the 1920s, astronomers and cosmologists questioned the idea that the large-scale universe is static and unchanging. This traditional belief was undermined both by theory (e.g. General Relativity) and observations. The most famous of these were collected and published by Edwin Hubble, starting in 1929 and continuing over the next decade as improved techniques and larger telescopes became available. In recent years, with the availability of the space telescope named in honor of Hubble data has expanded in availability and quality. Figure 36.3 shows a version of Hubble’s 1929 graph based on contemporary data.\n\n\n\n\n\nFigure 36.3: The relationship between velocity and distance of stars, using contemporary data in the same format at Edwin Hubble’s 1929 publication.\n\n\n\n\nEach dot in Figure 36.3 is an exploding star called a supernova. The graph shows the relationship between the distance of the star from our galaxy and the outward velocity of that star. The velocities are large, \\(3 \\times 10^4 = 30,000\\) km/s is about one-tenth the speed of light. Similarly, the distances are big; 600 Mpc is the same as 2 billion light years or \\(1.8 \\times 10^{22} \\text{km}\\). The slope of the line in Figure 36.3 is \\(\\frac{3.75 \\times 10^4\\, \\text{km/s}}{1.8 \\times 10^{22}\\, \\text{km}} = 2.1 \\times 10^{-18}\\, \\text{s}^{-1}\\). For ease of reading, we’ll call this slope \\(\\alpha\\) and therefore the velocity of a start distance \\(D\\) from Earth is \\[v(D) \\equiv \\alpha D\\ .\\]\nEarlier in the history of the universe each star was a different distance from Earth. We’ll call this function \\(D(t)\\), distance as a function of time in the universe.\nThe distance travelled by each star from time \\(t\\) (billions of years ago) to the present is \\[\\int_t^\\text{now} v(t) dt  = D_\\text{now} - D(t)\\] which can be re-arranged to give \\[D(t) = D_\\text{now} - \\int_t^\\text{now} v(t) dt .\\] Assuming that \\(v(t)\\) for each star has remained constant at \\(\\alpha D_\\text{now}\\), the distance travelled by each star since time \\(t\\) depends on it’s current distance like this: \\[\\int_t^\\text{now} v(t) dt = \\int_t^\\text{now} \\left[ \\alpha D_\\text{now}\\right]\\, dt = \\alpha D_\\text{now}\\left[\\text{now} - t\\right]\\] Thus, the position of each star at time \\(t\\) is \\[D(t) = D_\\text{now} - \\alpha D_\\text{now}\\left[\\text{now} - t\\right] = D(t)\\] or, \\[D(t) = D_\\text{now}\\left({\\large\\strut} 1-\\alpha \\left[\\text{now} - t\\right]\\right)\\]\nAccording to this model, there was a common time \\(t_0\\) when when all the stars were at the same place: \\(D(t_0) = 0\\). This happened when \\[\\text{now} - t_0 = \\frac{1}{\\alpha} = \\frac{1}{2.1 \\times 10^{-18}\\, \\text{s}^{-1}} = 4.8 \\times 10^{17} \\text{s}\\ .\\] It seems fair to call such a time, when all the stars where at the same place at the same time, as the origin of the universe. If so, \\(\\text{now} - t_0\\) corresponds to the age of the universe and our estimate of that age is \\(4.8\\times 10^{17}\\text{s}\\). Conventionally, this age is reported in years. To get that, we multiply by the flavor of one that turns seconds into years: \\[\\frac{60\\, \\text{seconds}}{1\\, \\text{minute}} \\cdot \\frac{60\\, \\text{minutes}}{1\\, \\text{hour}} \\cdot \\frac{24\\, \\text{hours}}{1\\, \\text{day}} \\cdot \\frac{365\\, \\text{days}}{1\\, \\text{year}} = 31,500,000 \\frac{\\text{s}}{\\text{year}}\\] The grand (but hypothetical) meeting of the stars therefore occurred \\(4.8 \\times 10^{17} \\text{s} / 3.15 \\times 10^{7} \\text{s/year} = 15,000,000,000\\) years ago. Pretty crowded to have all the mass in the universe in one place at the same time. No wonder they call it the Big Bang!"
  },
  {
    "objectID": "Accumulation/29-integration.html#exercises",
    "href": "Accumulation/29-integration.html#exercises",
    "title": "36  Integration",
    "section": "36.5 Exercises",
    "text": "36.5 Exercises"
  },
  {
    "objectID": "Accumulation/30-euler.html",
    "href": "Accumulation/30-euler.html",
    "title": "37  Integrals step-by-step",
    "section": "",
    "text": "The setting for anti-differentiation (and it’s close cousin, integration) is that we have a function \\(F(t)\\) which we do not yet know, but we do have access to some information about it: its slope as a function of time \\(f(t) \\equiv \\partial_t F(t)\\) and, perhaps, its value \\(F(t_0)\\) at some definite input value.\nSection 35 showed some ways to visualize the construction of an \\(F(t)\\) by accumulating short segments of slope. The idea is that we know \\(f(t)\\) which tells us, at any instant, the slope of \\(F(t)\\). So, in drawing a graph of \\(F(t)\\), we put our pen to paper at some input value \\(t_0\\) and then move forward in \\(t\\), setting the instantaneous slope of our curve according to \\(f(t)\\).\nIn Section 36, we dealt with one of the limitations of finding \\(F(t)\\) by anti-differentiation of \\(f(t)\\); the anti-derivative is not unique. This is because to start drawing \\(F(t)\\) we need pick a \\(t_0\\) and an initial value of \\(F(t_0)\\). If we had picked a a different starting point \\(t_1\\) or a different initial value \\(F(t_1)\\), the new curve would be different than the one drawn through \\((t_0, F(t_0))\\), although it would have the same shape, just shifted up or down according to our choice. We summarize this situation algebraically by writing \\[\\int f(t) dt = F(t) + C\\ ,\\] where \\(C\\) is the constant of integration, that is, the vertical shift of the curve.\nThe non-uniqueness of \\(F(t)\\) does not invalidate its usefulness. In particular, the quantity \\(F(b) - F(a)\\), will be the same regardless of which starting point we used to draw \\(F(t)\\). We have two names for \\(F(b) - F(a)\\)\nThese two things, the net change and the definite integral, are really one and the same, a fact we describe by writing \\[\\int_a^b f(t) dt = F(b) - F(a)\\ .\\]\nIn this chapter, we’ll introduce a simple numerical method for calculating from \\(f()\\) the net change/definite integral. This will be a matter of trivial but tedious arithmetic: adding up lots of little bits of \\(f(t)\\). Later, in Section 38, we’ll see how to avoid the tedious arithmetic by use of algebraic, symbolic transformations. This symbolic approach has great advantages, and is the dominant method of anti-differentiation found in college-level science textbooks. However, there are many common \\(f(t)\\) for which the symbolic approach is not possible, whereas the numerical method works for any \\(f(t)\\). Even more important, the numerical technique has a simple natural extension to some commonly encountered accumulation problems that look superficially like they can be solved by anti-differentiation but rarely can be in practice. We’ll meet one such problem and solve it numerically, but a broad approach to the topic, called dynamics or differential equations, will have to wait until Block 6."
  },
  {
    "objectID": "Accumulation/30-euler.html#euler-method",
    "href": "Accumulation/30-euler.html#euler-method",
    "title": "37  Integrals step-by-step",
    "section": "37.1 Euler method",
    "text": "37.1 Euler method\nThe starting point for this method is the definition of the derivative of \\(F(t)\\). Reaching back to Chapter 8,\n\\[\\partial_t F(t) \\equiv \\lim_{h\\rightarrow 0} \\frac{F(t+h) - F(t)}{h}\\] To translate this into a numerical method for computing \\(F(t)\\), let’s write things a little differently.\n\nFirst, since the problem setting is that we don’t (yet) know \\(F(t)\\), let’s refer to things we do know. In particular, we know \\(f(t) = \\partial_t F(t)\\).\nAgain, recognizing that we don’t yet know \\(F(t)\\), let’s re-write the expression using something that we do know: \\(F(t_0)\\). Stated more precisely, \\(F(t_0)\\) is something we get to make up to suit our convenience. (A common choice is \\(F(t_0)=0\\).)\nLet’s replace the symbol \\(h\\) with the symbol \\(dt\\). Both of them mean “a little bit of” and \\(dt\\) makes explicit that we mean “a little bit of \\(t\\).”\nWe’ll substitute the limit \\(\\lim_{h\\rightarrow 0}\\) with an understanding that \\(dt\\) will be something “small.” How small? We’ll deal with that question when we have to tools to answer it.\n\nWith these changes, we have \\[f(t_0) = \\frac{F(t_0+dt) - F(t_0)}{dt}\\ .\\] The one quantity in this relationship that we do not yet know is \\(F(t_0 + dt)\\). So re-arrange the equation so that we can calculate the unknown in terms of the known. \\[F(t_0 + dt) = F(t_0) + f(t_0)\\, dt\\ .\\]\n\nLet’s consider finding the anti-derivative of \\(\\dnorm()\\), that is, \\(\\int_0^t \\dnorm(x) dx\\). In one sense, you already know the answer, since \\(\\partial_x \\pnorm(x) = \\dnorm(x)\\). In reality, however, we know \\(\\pnorm()\\) only because it has been numerically constructed by integrating \\(\\dnorm()\\). The \\(\\pnorm()\\) function is so important that the numerically constructed answer has been memorized by software."
  },
  {
    "objectID": "Accumulation/30-euler.html#area",
    "href": "Accumulation/30-euler.html#area",
    "title": "37  Integrals step-by-step",
    "section": "37.2 Area",
    "text": "37.2 Area\nThe quantity \\[\\Large \\color{magenta}{f(t_0)}\\, \\color{orange}{dt}\\] gives rise to a visualization that has been learned by generations of calculus students. The visualization is so compelling and powerful that many students (and teachers, and textbook authors) mistake the visualization for integration and anti-differentiation themselves.\nWe’ll start the visualization with a simple graph of \\(f(t)\\), which is called the integrand in the integral \\(\\int_a^b f(t) dt\\). Figure 37.1 shows the graph of \\(f(t)\\). A specific point \\(t_0\\) has been marked on the horizontal axis. Next to it is another mark at \\(t_0 + dt\\). Of course, the distance between these marks is \\(\\color{orange}{dt}\\).\n\n\n\n\n\nFigure 37.1: Illustrating the interpretation of \\(f(t_0) dt\\) as an “area”.\n\n\n\n\nPer the usual graphical convention, a position along the vertical axis corresponds to a possible output of \\(f(t)\\). The output for \\(t=t_0\\) is \\(\\color{magenta}{f(t_0)}\\). That same quantity corresponds to the length of the vertical orange segment connecting \\((t_0, 0)\\) to \\((t_0, f(t_0))\\).\nThe \\(\\color{orange}{dt}\\) line segment and the \\(\\color{magenta}{f(t_0)}\\) segment constitute two sides of a rectangle, shown as a shaded zone. The “area” of that rectangle is the product \\(\\color{magenta}{f(t_0)}\\ \\color{orange}{dt}\\).\nIn this sort of visualization, an integral is the accumulation of many of these \\(f(t) dt\\) rectangles. For instance, Figure 37.2 the visualization of the integral \\[\\int_{0}^3 f(t) dt\\ .\\]\n\n## Warning in is.na(x): is.na() applied to non-(list or vector) of type\n## 'expression'\n\n## Warning in is.na(x): is.na() applied to non-(list or vector) of type\n## 'expression'\n\n## Warning in is.na(x): is.na() applied to non-(list or vector) of type\n## 'expression'\n\n\n\n\nFigure 37.2: Visualizing the integral \\(\\int_0^3 f(t) dt\\) as the total “area” of several \\(f(t) dt\\) bars. The width of each of the bars is \\(dt\\). The height depends on the value of the function \\(f(t)\\) at the bar. For illustration, two of the bars are marked with vertical and horizontal line segments.\n\n\n\n\nAs always in calculus, we imagine \\(dt\\) as a “small” quantity. In ?fig-bars-0-3B you can see that the function output changes substantially over the sub-domain spanned by a single rectangle. Using smaller and smaller \\(dt\\), as in ?fig-bars-0-3-small brings the visualization closer and closer to the actual meaning of an anti-derivative.\n\n\n\n\n\nVisualizing the integral \\(\\int_0^3 f(t) dt\\) as the total “area” of several \\(f(t) dt\\) bars. The width of each of the bars is \\(dt\\). The height depends on the value of the function \\(f(t)\\) at the bar. For illustration, two of the bars are marked with vertical and horizontal line segments.\n\n\n\n\n\nWhy do you keep putting “area” in quotes?\nWhen \\(f(t_i) < 0\\), then \\(f(t_i) dt\\) will be negative. There is no such thing as a negative area, but in constructing an integral the \\(f(t_i)dt\\), being negative, diminishes the accumulated area.\n\n\n\n\n\nFigure 37.3: The \\(\\int_{-2}^3 g(t) dt\\) covers subdomains where \\(g(t) > 0\\) and where \\(g(t) < 0\\). In those latter subdomains, the “area” is negative, and shown in light orange here.\n\n\n\n\nAnother problem is that area is a physical quantity, with dimension L\\(^2\\). The quantity produced by integration will have physical dimension \\([f(t)][t]\\), the product of the dimension of the with-respect-to quantity and the output of the function.\n“Area” is an effective metaphor for visualizing integration, but the goal of integration is not to calculate an area but, typically, some other kind of quantity."
  },
  {
    "objectID": "Accumulation/30-euler.html#the-euler-step",
    "href": "Accumulation/30-euler.html#the-euler-step",
    "title": "37  Integrals step-by-step",
    "section": "37.3 The Euler Step",
    "text": "37.3 The Euler Step\nThe previous section a visualization of an integral in terms of an area on a graph. As you know, a definite integral \\(\\int_a^b f(t) dt\\) can also be computed by constructing the anti-derivative \\(F(t) \\equiv \\int f(t) dt\\) and evaluating it at the upper and lower bounds of integration: \\(F(b) - F(a)\\). In this section, we’ll look at the numerical process of constructing an anti-derivative function, which uses many of the same concepts as those involved in finding an integral by combining areas of rectangles.\nA definite integral produces a quantity, not a function. The anti-derivative function constructed by using quantities like \\(f(t) dt\\) will be a series of quantities rather than a formula. In particular, it will have the form of a data table, something like this:\n\n\n\n\n\n\\(t\\)\n\\(F(t)\\)\n\n\n\n\n-2\n10.62\n\n\n-1.5\n6.47\n\n\n-1\n3.51\n\n\n-0.5\n2.02\n\n\n0\n2.4\n\n\n0.5\n3.18\n\n\n1.0\n5.14\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\nTo start, we’ll need to create a series of \\(t\\) values. We’ll do this by specifying a starting value for \\(t\\) and then creating successive values by adding a numerical increment \\(dt\\) to the entries one after the other until we reach a terminating value. For instance, in the above table, the starting value for \\(t\\) is \\(-2\\), the numerical increment is \\(dt=0.5\\), and the terminating value is \\(1\\).\nIn previous chapters of this book we have worked with data tables, but always the data table was given to us, we did not have to construct it.1 Now we need to construct the data frame with the \\(t\\) column containing appropriate values. Computer languages provide many ways to accomplish this task. We’ll use a simple R/mosaic function Picket(), which constructs a data table like the one shown above. You provide two arguments: the domain for \\(t\\), that is, the desired upper and lower bounds of integration; the interval size \\(dt\\) (which is called h in the argument list). For instance, to construct the \\(t\\) column of the table shown above, you can use Picket() this way:\n\nPts <- Picket(domain(t = -2:1), h=0.5)\nPts\n## # A tibble: 6 × 3\n##       t preweight weight\n##   <dbl>     <dbl>  <dbl>\n## 1  -2           1    0.5\n## 2  -1.5         1    0.5\n## 3  -1           1    0.5\n## 4  -0.5         1    0.5\n## 5   0           1    0.5\n## 6   0.5         1    0.5\n\nAs you can see, the data table produced by Picket() has the \\(t\\) column, as well as a second column named weight. We haven’t explained weight yet, but you can see that it is the same value we specified as h.\nThe name Picket() is motivated by the shape of a picket fence. The pickets are evenly spaced, which keeps things simple but is not a requirement.\nNote that the picket does not say anything at all about the function \\(f(t)\\) being anti-differentiated. The picket can be applied to any function although precision might require a smaller \\(dt\\) for functions that have a lot going on in a small domain.\nThe next step in using the picket to perform anti-differentiation is to apply the function \\(f()\\) to the pickets. That is, we’ll add a new column, perhaps called vals to the data table.\nAdding a new column is a common task when dealing with data. We’ll do this with a new function, mutate(), whose specific function is adding new columns (or modifying old ones). Here’s the command to apply \\(f()\\) to t and call the new column vals:\n\n# Find the height of the pickets\nPts <- Pts %>%\n  mutate(vals = f(t))\n\nWith this modification, the data table looks like:\n\n## # A tibble: 6 × 4\n##       t preweight weight  vals\n##   <dbl>     <dbl>  <dbl> <dbl>\n## 1  -2           1    0.5 -9.12\n## 2  -1.5         1    0.5 -7.27\n## 3  -1           1    0.5 -4.50\n## 4  -0.5         1    0.5 -1.46\n## 5   0           1    0.5  1.28\n## 6   0.5         1    0.5  3.31\n\nNow that we know the value of the function at each of the pickets, the next step is to multiply the value by the spacing between pickets. That spacing, which we set with the argument h = 0.5 in our original call to Picket() is in the column called weight. We’ll call the result of the multiplication step. Note that the following R command incorporates the previous calculation of vals; we’re looking to build up a single command that will do all the work.\n\n# Multiply the height by the picket spacing\nPts <- Pts %>%\n  mutate(vals = f(t),\n         step = vals * weight)\n\n\nPts\n## # A tibble: 6 × 5\n##       t preweight weight  vals   step\n##   <dbl>     <dbl>  <dbl> <dbl>  <dbl>\n## 1  -2           1    0.5 -9.12 -4.56 \n## 2  -1.5         1    0.5 -7.27 -3.63 \n## 3  -1           1    0.5 -4.50 -2.25 \n## 4  -0.5         1    0.5 -1.46 -0.732\n## 5   0           1    0.5  1.28  0.639\n## 6   0.5         1    0.5  3.31  1.66\n\nWe used the name step to identify the product of the height and spacing of the pickets to help you think about the overall calculation as accumulating a series of steps. Each step provides a little more information about the anti-derivative that we will now calculate. In terms of the area metaphor for integration, each step is the area of one vertical bar of the sort presented in the previous section.\nWe’ll call these Euler steps, a term that will be especially appropriate when, in Block 6, we use integration to calculate the trajectories of systems—such as a ball in flight—that change in time.\nThe final step in constructing the anti-derivative is to add up the steps. This is simple addition. But we’ll arrange the addition one step at a time. That is, for the second row, the result will be the sum of the first two steps. For the third row, the result will be the sum of the first three steps. And so on. The name for this sort of accumulation of the previous steps is called a cumulative sum. Another name for a cumulative sum is a “running sum”: the sum-so-far as we move down the column of steps. Cumulative sums are computed in R by using cumsum(). Here, we’re calling the result of the cumulative sum F to emphasize that it is the result of anti-differentiating \\(f()\\). But keep in mind that the anti-derivative is not just the F column, but the table with both t and F columns. That is, the table has a column for the input as well as the output. That’s what it takes to be a function.\n\n# Doing everything in one command\nPts <- \n  Picket(domain(t = -2:1), h=0.5) %>%\n  mutate(vals = f(t),\n         step = vals * weight,\n         F = cumsum(step))\n\n\n## # A tibble: 6 × 6\n##       t preweight weight  vals   step      F\n##   <dbl>     <dbl>  <dbl> <dbl>  <dbl>  <dbl>\n## 1  -2           1    0.5 -9.12 -4.56   -4.56\n## 2  -1.5         1    0.5 -7.27 -3.63   -8.19\n## 3  -1           1    0.5 -4.50 -2.25  -10.4 \n## 4  -0.5         1    0.5 -1.46 -0.732 -11.2 \n## 5   0           1    0.5  1.28  0.639 -10.5 \n## 6   0.5         1    0.5  3.31  1.66   -8.88\n\nWe can summarize the steps in this Euler approach to numerical integration graphically:\n\n\n\n\n\n{#fig-euler-integration1, fig-align=‘center’ width=90%}\n\n\n\n\n\n\n\nFigure 37.4: Steps in a numerical construction of an anti-derivative. (1) Create a set of picket locations over the domain of interest. The locations are spread horizontally by amount dt, so each picket will be dt units wide. (2) evaluate the original function at the picket points to give picket heights. (3) Multiply the picket height by the picket width to create an “area”. (4) Starting at zero for the left-most picket, add in successive picket areas to construct the points on the anti-derivative function (green). Note that the vertical axis in (4) has a different dimension and units than in steps (1)-(3). In (4) the vertical scale is in the units of the anti-derivative function output.\n\n\n\n\nFigure 37.5 shows a dynamic version of the process of constructing an anti-derivative by Euler steps. The integrand \\(f(t)\\) is shown in the top panel, the anti-derivative \\(F(t)\\) is shown being built up in the bottom panel. The \\(\\color{magenta}{\\text{magenta}}\\) bar in the top plot is the current Euler step. That step is added to the previously accumulated steps to construct \\(F(t)\\).\n\nPROVIDE LINK TO MOVIE IN PDF version.\n\n\n\n\n\n\nFigure 37.5: A dynamic view of building \\(F(t)\\) from \\(f(t)\\) by accumulating Euler steps.\n\n\n\n\n\nThe following graphic from a well-respected news magazine, The Economist, shows the reported number of cases and deaths from Covid-19 during a two-year period in Russia. (“Sputnik” is the name given to a Russian-developed vaccine, named after the first man-made satellite in Earth orbit, launched by the Soviet Union on Oct. 4, 1957 and precipitating a Cold-War crisis of confidence in in the US.)\n\n\n\n\n\n\n\n\n\nThe figure caption gives information about the units of the quantities being graphed. Notice the word “daily,” which tells us, for example, that in mid-2021 there were about 10,000 new cases of Covid-19 each day and correspondingly about 350 daily deaths.\nHow many total cases and total deaths are reported in the graphic?\nThere are, of course, two distinct ways to present such data which can be easily confused by the casual reader. One important way to present data is as cumulative cases and deaths as a function of date. We’ll call these \\(C(t)\\) and \\(D(t)\\). Another prefectly legitimate presentation is of the rate of change \\(\\partial_t C(t)\\) and \\(\\partial_t D(t)\\) which, following our informal capital/lower-case-letter convention, we could write \\(c(t)\\) and \\(d(t)\\). Since there is no such thing as a “negative” case or death, we know that \\(C(t)\\) and \\(D(t)\\) are monotonic functions, never decreasing. So the graphs cannot possibly be of \\(C(t)\\) and \\(D(t)\\), since the graphs are far from monotonic. Consequently, the displayed graphs are \\(c(t)\\) and \\(d(t)\\), as confirmed by the word “daily” in the caption.\nTo find \\(C(t)\\) and \\(D(t)\\) requires integrating \\(c(t)\\) and \\(d(t)\\). The value of \\(C(t)\\) and \\(D(t)\\) at the right-most extreme of the graph can be found by calculating the “area” under the \\(c(t)\\) and \\(d(t)\\) curves. But care needs to be taken in reading the horizontal axis. Although the axes are labelled with the year, the tick marks are spaced by one month. (Notice “month” does not appear in the caption.) The far right end of the graph is in early July 2021. The far left end, when the graph moves away from zero cases and deaths, is early April 2020.\nYou can do a reasonable job estimating the “area” by extending the tick marks on the horizontal axis and counting the resulting rectangles that fall under the curve.\n\n\n\n\n\n\n\n\n\nFor the graph of cases, the “area” of each rectangle is \\(\\frac{5000\\, \\text{cases}}{\\text{ day}}\\cdot \\text{1 month}\\). This has the right dimension, “cases,” but the units are screwy. So replace 1 month with 30.5 days (or thereabouts) to get an “area” of each rectangle of 172,500 cases. Similarly, the “area” of the rectangles on the right graph is 3050 deaths."
  },
  {
    "objectID": "Accumulation/30-euler.html#better-numerics-optional",
    "href": "Accumulation/30-euler.html#better-numerics-optional",
    "title": "37  Integrals step-by-step",
    "section": "37.4 Better numerics (optional)",
    "text": "37.4 Better numerics (optional)\nExcept as a textbook exercise, you will likely never have to compute a numerical anti-derivative from scratch as we did in the previous section. This is a good thing. To understand why, you have to know one of the important features of modern technical work. That feature is: We never work alone in technical matters. There is always a set of people whose previous work we are building on, even if we never know the names of those people. This is because technology is complicated and it is evidently beyond the reach of any human to master all the salient aspects of each piece of technology being incorporated into the work we consider our own.\nOf course this is true for computers, since no individual can build a useful computer from first principles. It’s also true for software. One detail in particular is relevant to us here. Computer arithmetic of the sort used in the previous section—particularly addition—is prone to error when adding up lots and lots of small bits. This means that it is not always sensible to choose very small \\(dt\\) in order to get a very accurate approximation to the anti-derivative.\nFortunately, there are specialists in numerical mathematics who work on ways to improve the accuracy of calculations for mid-sized \\(dt\\). Their work has been incorporated into the results of antiD() and Integrate() and so the details are, for us, unimportant. But they are only unimportant because they have been taken care of.\nTo illustrate how considerably more accuracy can be gained in calculating an anti-derivative, consider that the rectangular bars drawn in the previous sections are intended to approximate the “area” under the function. With this in mind, we can replace the rectangular bars with more suitable shapes that stay closer to the function over the finite extend of each \\(dt\\) domain. The rectangular bars model the function as piecewise constant. A better job can be done with piecewise linear approximations or piecewise quadratic approximations. Often, such refinements can be implemented merely by changing the weight column in the picket data frame used to start off the process.\nOne widely used method, called Gauss-Legendre quadrature can calculate a large segment of an integral accurately (under conditions that are common in practice) with just five evaluations of the integrand \\(f(t)\\).\n\n\n\nPicket locations and weights For the integral \\(\\int_a^b f(t) dt\\) where \\(c = \\frac{a+b}{2}\\) and \\(w = (b-a)/2\\).\n\n\n\n\n\n\nlocation\nweight\n\n\n\n\n\\(c - 0.90618 w\\)\n\\(0.236927 \\times w\\)\n\n\n\\(c - 0.53847 w\\)\n\\(0.478629 \\times w\\)\n\n\n\\(c\\)\n\\(0.568889 \\times w\\)\n\n\n\\(c + 0.53847 w\\)\n\\(0.478629 \\times w\\)\n\n\n\\(c + 0.90618 w\\)\n\\(0.236927 \\times w\\)\n\n\n\n\n\nThe locations and weights may seem like a wizard parody of mathematics, but those precise values are founded in an advanced formulation of polynomials rooted in the theory of linear combinations to which you’ll be introduced in Block 5. Needless to say, you can hardly be expected to have any idea where they come from. That’s why it’s useful to build on the work of experts in specialized areas. It’s particularly helpful when such expertise is incorporated into software that faithfully and reliably implements the methods. The lesson to take to heart: Use professional software systems that have been extensively vetted."
  },
  {
    "objectID": "Accumulation/30-euler.html#exercises",
    "href": "Accumulation/30-euler.html#exercises",
    "title": "37  Integrals step-by-step",
    "section": "37.5 Exercises",
    "text": "37.5 Exercises"
  },
  {
    "objectID": "Accumulation/31-symbolic.html",
    "href": "Accumulation/31-symbolic.html",
    "title": "38  Symbolic anti-differentiation",
    "section": "",
    "text": "You have already learned how to write down, by sight, the anti-derivative of the many of the pattern-book functions. As a reminder, here is an (almost) complete list of the derivatives and anti-derivatives of the pattern-book functions.\nYou can see that the derivatives and anti-derivatives of the pattern-book functions can be written in terms of the pattern-book functions. The left column contains the symbolic derivatives of the pattern book functions.1 The right column contains the symbolic anti-derivatives. We call them “symbolic,” because they are written down with the same kind of symbols that we use for writing the pattern-book functions themselves.2\nWe’re stretching things a bit by including \\(\\dnorm(x)\\) and \\(\\pnorm(x)\\) among the functions that can be integrated symbolically. As you’ll see later, \\(\\dnorm(x)\\) is special when it comes to integration.\nThink of the above table as the “basic facts” of differentiation and anti-differentiation. It’s well worth memorizing the table since it shows many of the relationships among the functions that are central to this book. For the sinusoids, we’ve used the traditional name \\(\\cos(x)\\) to refer to \\(\\sin(x + \\pi/2)\\) and \\(-\\cos(x)\\) instead of \\(\\sin(x - \\pi/2)\\) since generations of calculus students have been taught to name “cosine” as the derivative of “sine,” and don’t always remember the relationship that \\(\\cos(x) =\\sin(x + \\pi/2)\\).\nFor differentiation, any function that can be written using combinations of the pattern-book functions by multiplication, composition, and linear combination has a derivative that can be written using the pattern-book functions. So a complete story of symbolic differentiation is told by the above table and the differentiation rules:\nThis chapter is about the analogous rules for anti-differentiation. The anti-differentiation rule for linear combination is simple: essentially the same as the rule for differentiation.\n\\[\\int \\left[\\strut a\\, f(x) + b\\, g(x)\\right] dx = a\\!\\int\\! f(x) dx + b\\!\\int\\! g(x) dx\\] How about the rules for function products and for composition? The surprising answer is that there are no such rules. There is no template analogous to the product and chain rules for derivatives, that can consistently be used for anti-derivatives. What we have instead are techniques of integration, somewhat vague rules that will sometimes guide a successful search for the anti-derivative, but often will lead you astray.\nIndeed, there are many functions for which a symbolic anti-derivative cannot be constructed from compositions and/or multiplication of pattern-book functions that can be written using pattern-book functions.3\nFortunately, we already know the symbolic anti-derivative form of many functions. We’ll call those the cataloged functions, but this is not a term in general use in mathematics. For functions not in the catalog, it is non-trivial to find out whether the function has a symbolic anti-derivative or not. This is one reason why the techniques of integration do not always provide a result.\nThe following sections provide an overview of techniques of integration. We start with a description of the cataloged functions and direct you to computer-based systems for looking up the catalog. (These are almost always faster and more reliable than trying to do things by hand.) Then we introduce a new interpretation of the notation for anti-differentiation: differentials. This interpretation makes it somewhat easier to understand the major techniques of integration: substitution and integration by parts. We’ll finish by returning to a setting where symbolic integration is easy: polynomials.\nRemember that, even if we cannot always find a symbolic anti-derivative, that we can always construct a numerical anti-derivative that will be precise enough for almost any genuine purpose."
  },
  {
    "objectID": "Accumulation/31-symbolic.html#sec-cataloged-functions",
    "href": "Accumulation/31-symbolic.html#sec-cataloged-functions",
    "title": "38  Symbolic anti-differentiation",
    "section": "38.1 The cataloged functions",
    "text": "38.1 The cataloged functions\nIn a traditional science or mathematics education, students encounter (almost exclusively) basic functions from a mid-sized catalog. For instance: \\(\\sqrt{\\strut\\_\\_\\_\\ }\\), \\(\\sin()\\), \\(\\cos()\\), \\(\\tan()\\), square(), cube(), recip(), \\(\\ln()\\), \\(\\exp()\\), negate(), gaussian(), and so on. This catalog also includes some functions that take two arguments but are traditionally written without using parentheses. For instance, \\(a+b\\) doesn’t look like a function but is entirely equivalent to \\(+(a, b)\\). Others in this class are \\(\\times(\\ ,\\ )\\), \\(\\div(\\ , \\ )\\), \\(-(\\ ,\\ )\\), and ^( , ).\nThe professional applied mathematician’s catalog is much larger. You can see an example published by the US National Institute of Standards and Technology as the Digital Library of Mathematical Functions. (Almost all of the 36 chapters in this catalog, important though they be, are highly specialized and not of general interest across fields.)\nThere is a considerable body of theory for these cataloged functions, which often takes the form of relating them to one another. For instance, \\(\\ln(a \\times b) = \\ln(a) + \\ln(b)\\) demonstrates a relationship among \\(\\ln()\\), \\(+\\) and \\(\\times\\). Along the same lines of relating the cataloged functions to one another is \\(\\partial_x \\sin(x) = \\cos(x)\\) and other statements about derivatives such as those listed in Chapter Section 20.\nSimply to illustrate what a function catalog looks like, Figure 38.1 shows a page from an 1899 handbook entitled A Short Table of Integrals.\n\n\n\n\n\n\nFigure 38.1: Entries 124-135 from A Short Table of Integrals (1899) by Benjamin Osgood Pierce. The book includes 938 such entries.\n\n\n\nThe use of cataloged functions is particularly prevalent in textbooks, so the quantitatively sophisticated student will encounter symbolic anti-derivatives of these functions throughout his or her studies.\nThe cataloged functions were assembled with great effort by mathematicians over the decades. The techniques and tricks they used to find symbolic anti-derivatives are not part of the everyday experience of technical workers, although many mathematically minded people find them a good source of recreation.\nCalculus textbooks that include extensive coverage of the techniques and tricks should be understood as telling a story of the historical construction of catalogs, rather than conveying skills that are widely used today. In a practical sense, when the techniques are needed, it’s more reliable to access them via computer interface such as WolframAlpha, as depicted in Figure 38.2.\n\n\n\n\n\n\nFigure 38.2: Pierce’s entry 125 as computed by the WolframAlpha system.\n\n\n\nThe systems can do a good job identifying cases where the techniques will not work. In such systems, they provide the anti-derivative as constructed by numerical integration. The R/mosaic antiD() function works in this same way, although its catalog contains only a tiny fraction of the functions found in professional systems. (But then, only a tiny fraction of the professional cataloged function are widely used in applied work.)"
  },
  {
    "objectID": "Accumulation/31-symbolic.html#differentials",
    "href": "Accumulation/31-symbolic.html#differentials",
    "title": "38  Symbolic anti-differentiation",
    "section": "38.2 Differentials",
    "text": "38.2 Differentials\nBreathing some life into the symbol \\(dx\\) will help in understanding the algebra of techniques for anti-differentiating function compositions and products. We’ve thus far presented \\(dx\\) as a bit of notation: punctuation for identifying the with-respect-to input in anti-derivatives. That is, in interpreting a sequence of symbols like \\(\\int f(x,t) dx\\), we’ve parsed the sequence of symbols into three parts:\n\\[\\underbrace{\\int}_{\\text{integral sign}} \\overbrace{f(x, t)}^{\\text{function to be anti-differentiated}} \\underbrace{dx}_{\\text{'with respect to'}}\\]\nBy analogy, the English sentence\n\\[\\text{We loaded up on snacks.}\\]\nconsists of five parts: the five words in the sentence.\nBut you can also see “We loaded up on snacks” as having three parts:\n\\[\\underbrace{\\text{We}}_{\\text{subject}}\\  \n\\overbrace{\\text{loaded up on}}^{\\text{verb}}\\ \\ \\\n\\underbrace{\\text{snacks}}_{\\text{object}}\\]\nLikewise, the integrate sentence can be seen as consisting of just two parts:\n\\[\\underbrace{\\int}_{\\text{integral sign}} \\overbrace{f(x, t) dx}^{\\text{differential}}\\]\nA differential corresponds to the little sloped segments that we add up when calculating a definite integral numerically using the slope function visualization. That is \\[\\underbrace{\\int}_{\\text{Sum}} \\underbrace{\\overbrace{f(x,t)}^\\text{slope of segment}\\ \\  \\overbrace{dx}^\\text{run}}_\\text{rise}\\]\nA differential is a genuine mathematical object and is used, for example, in analyzing the geometry of curved spaces, as in the Theory of General Relativity. But this is well beyond the scope of this introductory calculus course.\nOur use here for differentials will be to express rules for anti-differentiation of function compositions and products.\nYou should be thinking in terms of differentials when you see a sentence like the following:\n\n“In \\(\\int \\sin(x) \\cos(x) dx\\), make the substitution \\(u = \\sin(x)\\), implying that \\(du = \\cos(x) dx\\) and getting \\(\\int u du\\), which is simple to integrate.”\n\nThe table gives some examples of functions and their differentials. “w.r.t” means “with respect to.”\n\n\n?(caption)\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nderivative\nw.r.t.\ndifferential\n\n\n\n\n\\(v(x) \\equiv x\\)\n\\(\\partial_x v(x) = 1\\)\nx\n\\(dv = dx\\)\n\n\n\\(u(x) \\equiv x^2\\)\n\\(\\partial_x u(x) = 2x\\)\nx\n\\(du = 2x dx\\)\n\n\n\\(f(x) \\equiv \\sin(x)\\)\n\\(\\partial_x f(x) = \\cos(x)\\)\nx\n\\(df = \\cos(x)dx\\)\n\n\n\\(u(x) \\equiv e^{3 x}\\)\n\\(\\partial_x u(x) = 3 e^{3 x}\\)\nx\n\\(du = 3 e^{3 x} dx\\)\n\n\n\\(g(x) \\equiv t^3\\)\n\\(\\partial_t v(t) = 3 t^2\\)\nt\n\\(dg = 3 t^2 dt\\)\n\n\n\n\n\n\nAs you can see, the differential of a function is simply the derivative of that function followed by the little \\(dx\\) or \\(dt\\) or whatever is appropriate for the “with respect to” input.\nNotice that the differential of a function is not written with parentheses: The function \\(u(x)\\) corresponds to the differential \\(du\\).\n\nWhat is the differential of \\(\\sin(x)\\)?\nAs we’ve seen, \\(\\partial_x \\sin(x) = cos(x)\\). For form the differential of \\(\\sin()\\), take the derivative and suffix it with a \\(dx\\) (since \\(x\\) is the name of the input):\n\\[\\cos(x)\\ dx\\]"
  },
  {
    "objectID": "Accumulation/31-symbolic.html#u-substitution",
    "href": "Accumulation/31-symbolic.html#u-substitution",
    "title": "38  Symbolic anti-differentiation",
    "section": "38.3 U-substitution",
    "text": "38.3 U-substitution\nThere is little reason to use \\(\\partial_t\\) and \\(\\int \\left[\\right]dt\\) to cancel each other out, but it is the basis of an often successful strategy—u-substitution—for finding anti-derivatives symbolically. Here’s the differentiate/integrate algorithm behind u-substitution.\n\nPick a function \\(f()\\) and another function \\(g()\\). Typically \\(f()\\) and \\(g()\\) belong to the family of basic modeling functions, e.g. \\(e^x\\), \\(\\sin(t)\\), \\(x^n\\), \\(\\ln(x)\\), and so on. For the purpose of illustration, we’ll use \\(f(x) = \\ln(x)\\) and \\(g(t) = \\cos(t)\\).\nCompose \\(f()\\) with \\(g()\\) to produce a new function \\(f(g())\\) which, in our case, will be \\(\\ln(\\cos(t))\\).\nUse the chain rule to find \\(\\partial_t f(g(t))\\). In the example, the derivative of \\(\\ln(x)\\) is \\(1/x\\), the derivative of \\(g(t)\\) is \\(-\\sin(t)\\). By the chain rule, \\[\\partial_t f\\left(\\strut g(t)\\right) = \\partial_t \\underbrace{\\Large\\ln}_{f()}\\left(\\underbrace{\\large\\cos(t)}_{g(t)}\\right) = \\underbrace{\\left[- \\frac{1}{\\cos(t)}\\right]}_{f'(g(t))} \\underbrace{\\left[{\\LARGE\\strut}\\sin(t)\\right]}_{g'(t)} = -  \\frac{\\sin(t)}{\\cos(t)} = - \\tan(t)\\]\n\nIn a sense, we have just watched a function give birth to another through the straightforward process of differentiation. Having witnessed the birth, we know who is the integration parent of \\(\\tan(t)\\), namely \\(\\int \\tan(t) dt = \\ln\\left(\\cos(t)\\right)\\). For future reference, we might write this down in our diary of integrals: \\[\\int \\tan(t) dt = - \\ln(\\cos(t)) + C\\] Saving this fact in your diary is helpful. The next time you need to find \\(\\int \\tan(x) dx\\), you can look up the answer (\\(-\\ln(\\cos(x)) + C\\)) from your diary. If you use \\(\\int \\tan(x) dx\\) a lot, you will probably come to memorize the answer, just as you have already memorized that \\(\\int \\cos(t) dt = \\sin(t)\\) (a fact that you will use a lot in the rest of this course).\nNow for the u-substitution game. The trick is to take a problem of the form \\(\\int h(t) dt\\) and extract from \\(h(t)\\) two functions, an \\(f()\\) and a \\(g()\\). You’re going to do this so that \\(h(t) = \\partial_t F(g(t))\\), where \\(\\partial_x F(x) = f(x)\\) Once you’ve done this, you have an answer to the original integration question: \\(\\int h(t) dt = F(g(t)) + C\\).\n\nTask: Evaluate the definite integral \\(\\int \\frac{\\sin(\\ln(x))}{x} dx\\).\nYou don’t know ahead of time that this is an integral amenable to solution by u-substitution. For all you know, it’s not. So before you start, look at the function to see if it one of those for which you already know the anti-derivative, for example any of the pattern-book functions or their parameterized cousins the basic modeling functions.\n\nIf so, you’ve already memorized the answer and you are done. If not …\n\nAssume for a moment—without any guarantee that this will work, mind you—that the answer can be built using u-substitution. You will therefore look hard at \\(h()\\) and try to see in it a plausible form that looks like the derivative of some \\(f(g(x))\\).\nIn the problem at hand, we can readily see something of the form \\(f(g(x))\\) in the \\(\\sin(\\ln(x))\\). This immediately gives you a candidate for \\(g(x)\\), namely \\(g(x)\\equiv \\ln(x)\\) We don’t know \\(f()\\) yet, but if \\(g()\\) is the right guess, and if u-substitution is going to work, we know that \\(f()\\) has to be something that produces \\(\\sin()\\) when you differentiate it. That’s \\(-\\cos()\\). So now we have a guess \\[h_\\text{guess}(x) = -\\cos(\\ln(x)) \\partial_x \\ln(x) = - \\cos(\\ln(x)) \\frac{dx}{x}\\]\n\nIf this guess matches the actual \\(h()\\) then you win. The answer to \\(\\int h(x) dx\\) will be \\(f(g(x)) = -\\cos(\\ln(x))\\). If not, see if there is any other plausible guess for \\(g(x)\\) to try. If you can’t find one that works, try integration by parts."
  },
  {
    "objectID": "Accumulation/31-symbolic.html#integration-by-parts-standard-presentation",
    "href": "Accumulation/31-symbolic.html#integration-by-parts-standard-presentation",
    "title": "38  Symbolic anti-differentiation",
    "section": "38.4 Integration by parts (standard presentation)",
    "text": "38.4 Integration by parts (standard presentation)\nIf you do a lot of symbolic anti-differentation, you will often come across functions that you don’t recognize as being the derivative of an already known function. Consider, for instance, \\[\\int x \\cos(x) dx\\ .\\]\nEven though the integrand \\(x \\cos(x)\\) is a simple product of two pattern book functions it is likely not a function that you have previously produced by differentiation. Thus, it’s not yet in your diary of anti-derivatives. The purpose of integration by parts is to provide a standard way to re-organize anti-derivatives like \\(\\int x \\cos(x) dx\\), where the integrand is a product of two simple functions, into another form. Being able to do this is no guarantee that the other form will be something you can anti-differentiate, but it’s worth rolling the dice to see if you get lucky.\nThe re-organization rule is based on two fundamental properties of differentiation and anti-differentiation.\n\n\\(\\int f'(x) dx = f(x)\\). This is saying nothing more than if \\(f'(x)\\) is the derivative of \\(f(x)\\), then \\(f(x)\\) must be an anti-derivative of \\(f'(x)\\).\n\\(\\partial_x \\left[\\strut u(x)\\cdot v(x) \\right] = u'(x)\\cdot v(x) + v'(x)\\cdot u(x)\\): the product rule of differentiation.\n\nLet’s integrate both sides of the statement of the product rule. For the left side, applying rule (i), we get a simple result:\n\\[\\int\\left[\\strut\\partial_x \\left[\\strut u(x)\\cdot v(x) \\right]\\right] dx = u(x) \\cdot v(x)\\]\nAs for the right side, all we get is two anti-derivatives: \\[\\int\\left[\\strut u'(x)\\cdot v(x) + v'(x)\\cdot u(x)\\right]dx =\n\\int\\left[\\strut u'(x)\\cdot v(x)\\right]dx + \\int\\left[\\strut u(x)\\cdot v'(x)\\right]dx\\] Putting together the previous two expressions and re-arranging gives: \\[\\int u(x)\\cdot v'(x)\\, dx = u(x) \\cdot v(x) - \\int  v(x)\\cdot u'(x)dx\\ \\ \\ \\mathbf{ \\text{parts re-arrangment}}\\] Now, consider a problem like \\(\\int x \\cdot \\cos(x) dx\\) that we don’t yet know how to solve. Let’s associate this problem with the left side of the parts re-arrangement equation. With luck, we will recognize a problem that we’ll know how to do on the right-hand side.\nTo implement the re-arrangement, we need to split our as yet unknown anti-derivative into two pieces: \\(u(x)\\) and \\(v'(x) dx\\). There are many possible ways to do this but the most obvious is \\[\\int \\underbrace{\\strut x}_{u(x)} \\cdot \\underbrace{\\cos(x) dx}_{v'(x) dx}\\] According to this proposed splitting, we have \\(u(x) = x\\) and \\(v'(x) dx = \\cos(x) dx\\). To plug things into the right side of the parts re-arrangement we’ll need to find \\(v(x)\\) and \\(u'(x) dx\\). Since we know \\(u(x) = x\\) it’s easy to take the differential, \\(du = dx\\). Similarly, we know \\(v'(x) dx = \\cos(x) dx\\) so we can integrate both sides: \\[v(x) = \\underbrace{\\int v'(x) dx = \\int \\cos(x) dx}_{\\text{from }\\ v'(x)\\,dx\\ =\\ \\cos(x)\\,dx} = \\sin(x)\\] Now that we know the \\(v(x)\\) that’s consistent with our original splitting of the anti-derivative into \\(\\int u(x) \\cdot v'(x) dx\\) we can plug in our results to the right side of the parts re-arrangement equation:\n\\[\\int x \\cdot \\cos(x)dx = x \\sin(x) - \\int \\underbrace{\\sin(x)}_{v(x)}\\  \\underbrace{\\ 1\\ dx\\ \\strut}_{u'(x) dx}\\] We’re in luck! We already know the anti-derivative \\(\\int \\sin(x) dx = -\\cos(x)\\). Substituting this result for the \\(\\int v(x) u'(x) dx\\) term, we arrive at \\[\\int x \\cdot \\cos(x)dx = x \\sin(x) + \\cos(x)\\ .\\]\nThe key creative step in using integration by parts effectively is to choose a helpful split of the original integral into the \\(u(x)\\) and \\(v'(x) dx\\) parts. This is usually based on a solid knowledge of derivatives and anti-derivatives of basic functions as well as insight into the downstream consequences of any choice. In this sense, picking \\(u(x)\\) and \\(v'(x)dx\\) is like making a move in chess. Some players can see two or three moves ahead and so can pick the first move to improve their position. Without such foresight, the best most people can do is to pick a first move that seems attractive and accept that their fate might be either victory or checkmate.\nFor the calculus student learning integration by parts, there is an irony. Gaining enough experience to make good choices of \\(u(x)\\) and \\(v'(x)dx\\) means that you will solve, or read about solving, many anti-differentiation problems. You can, of course, enter the solutions into your diary of anti-derivatives, obviating to that extent the need to perform integration by parts in the future.\n\nIn demonstrating that \\[\\int x \\cdot \\cos(x)dx = x \\sin(x) + \\cos(x)\\] we followed a number of steps each of which might be subject to error. Best to confirm our solution before accepting it. This can be done by differentiating both sides of our solution: \\[\\partial_x \\int x \\cdot \\cos(x)dx = x \\cos(x) = \\partial_x \\left[\\strut x \\sin(x) + \\cos(x)\\right] = \\underbrace{\\sin(x) + x \\cos(x)}_{\\partial_x \\left[x\\cdot\\sin(x)\\right]}\\  \\underbrace{- \\sin(x)}_{\\partial_x \\cos(x)}= x\\cos(x)\\]\n\n\nWhat would happen in the previous example if we had made a bad choice for \\(u(x)\\) and \\(v'(x) dx\\)? For instance, we might have split \\(x \\cos(x) dx\\) into \\(u(x) = \\cos(x)\\) and \\(v'(x)\\,dx = x\\, dx\\). Working out \\(u'(x)\\,dx\\) and \\(v(x)\\) is easy: \\(u'(x)\\, dx = -\\sin(x)\\, dx\\) and \\(v(x) = \\frac{1}{2} x^2\\). Plugging into the re-arrangement formula gives:\n\\[\\int x \\cdot \\cos(x)\\,dx = \\frac{1}{2} x^2 \\cos(x) - \\int \\frac{1}{2} x^2 \\left[\\strut - \\sin(x)\\right]\\,dx = \\frac{1}{2} x^2 \\cos(x) + \\int \\frac{1}{2} x^2  \\sin(x)\\,dx\\] Unless you know \\(\\int x^2 \\sin(x) dx\\), this re-arrangement leaves you no better off than at the beginning.\nOn the other hand … if you are in the business of compiling diaries of anti-derivatives, you could use this situation to chalk up another entry based on already knowing \\(\\int x \\cdot \\cos(x) dx\\): \\[\\int x^2 \\sin(x) dx = 2 \\int x\\cdot \\cos(x)dx - x^2\\cos(x) = 2x\\sin(x) + 2\\cos(x) - x^2 \\cos(x)\\]\n\n\nFind \\(\\int x \\ln(x) dx\\).\nLet \\(u(x) = \\ln(x)\\) and \\(v'(x)dx = x dx\\).\nThen, \\(u'(x)dx = \\frac{1}{x} dx\\) and \\(v(x) = \\frac{1}{2} x^2\\).\nUsing the parts re-arrangement formula …\n\\[\\int x \\ln(x) dx = \\frac{1}{2} x^2 \\cdot \\ln(x) - \\int \\frac{1}{2} x^2\\cdot \\frac{1}{x}\\, dx \\\\\n\\frac{1}{2} x^2 \\cdot \\ln(x) - \\frac{1}{4} x^2\\] And don’t forget, after all this work, to add the constant of integration \\(C\\)!"
  },
  {
    "objectID": "Accumulation/31-symbolic.html#integration-by-parts-optional-alternative-presentation",
    "href": "Accumulation/31-symbolic.html#integration-by-parts-optional-alternative-presentation",
    "title": "38  Symbolic anti-differentiation",
    "section": "38.5 Integration by parts (optional alternative presentation)",
    "text": "38.5 Integration by parts (optional alternative presentation)\nIntegration by parts applies to integrals that are recognizably of the form \\[\\int f(x) g(x) dx\\] Step 1: Split up the integrand into an \\(f(x)\\) and a \\(g(x)\\) multiplied together. That is, split the integrand into parts that are multiplied together. The way we wrote the integrand, this was trivial.\nStep 2: Pick one of \\(f(x)\\) or \\(g(x)\\). Typically, you pick the one that has a dead-easy anti-derivative. For our general description, let’s suppose this is \\(g(x)\\) which has anti-derivative \\(G(x)\\) (where we know \\(G()\\)).\nStep 3: Construct a helper function \\(h(x) \\equiv f(x) G(x)\\). This requires no work, since we’ve already identified \\(f(x)\\) and \\(G(x)\\) in step (2).\nStep 4: Find \\(\\partial_x h(x)\\). It’s always easy to find derivatives, and here we just use the product rule: \\[\\partial_x h(x) = \\partial_x f(x) \\cdot G(x) + f(x)\\cdot\\partial_x G(x)\\] We know from the way we constructed \\(G(x)\\) that \\(\\partial_x G(x) = g(x)\\), so the equation is \\[\\partial_x h(x) = \\partial_x f(x) \\cdot G(x) + f(x)\\cdot g(x)\\]\nStep 5: Anti-differentiate both sides of the previous equation. From the fundamental theorem of calculus, we know how to do the left side of the equation. \\[\\int \\partial_x h(x) = h(x) \\equiv f(x)g(x)\\] The right side of the equation has two parts: \\[\\int \\left[{\\large\\strut}\\partial_x f(x) \\cdot G(x) + f(x)\\cdot g(x)\\right]dx = \\underbrace{\\int \\partial_x f(x) \\cdot G(x) dx}_\\text{Some NEW integral!}\\ \\ \\ \\  + \\underbrace{\\int f(x) g(x) dx}_\\text{The original integral we sought!}\\] Putting together the left and right sides of the equation, and re-arranging gives us a new expression for the original integral we sought to calculate: \\[\\text{Integration by parts re-arrangement}\\\\\\underbrace{\\int f(x) g(x) dx}_\\text{The original integral we sought.} = \\underbrace{f(x) g(x)}_\\text{We know this!}  - \\underbrace{\\int \\partial_x f(x) \\cdot G(x) dx}_\\text{Some NEW integral!}\\] It may seem that we haven’t accomplished much with this re-organization. But we have done something. We took a problem we didn’t otherwise know how to solve (that is \\(\\int f(x) g(x) dx\\)) and broke it down into two parts. One is very simple. The other is an integral. If we’re clever in picking \\(g()\\) and lucky, we’ll be able to figure out the new integral and, thereby, we’ll have computed the original integral. But everything depends on cleverness and luck!\n\nTask: Find \\(\\int x \\cos(x) dx\\).\nAn obvious choice for the two parts is \\(x\\) and \\(\\cos(x)\\). But which one to call \\(g(x)\\). We’ll just guess and say \\(g(x)\\equiv \\cos(x)\\) which implies \\(G(x) = \\sin(x)\\). The helper function is \\(h(x) \\equiv f(x) G(x) = x \\sin(x)\\).\nDifferentiating \\(h(x)\\) can be done by the product rule. \\[\\partial_x h(x) = \\sin(x) + x \\cos(x)\\ .\\] Now anti-differentiate both sides of the above, the left side by the fundamental theorem of algebra and the right side by other means: \\[\\int \\partial_x h(x) = h(x) = x \\sin(x)= \\underbrace{\\int\\sin(x)dx}_{-\\cos(x)} + \\underbrace{\\int x \\cos(x) dx}_\\text{The original integral}\\] Re-arranging gives the answer \\[\\underbrace{\\int x \\cos(x) dx}_\\text{The original integral} = x \\sin(x) + \\cos(x) + C\\] The constant of integration \\(C\\) needs to be included to make the equality true.\nTo confirm the result, you can differentiate the right-hand side; differentiation is always easy.\nAlternatively, we can check numerically if \\(\\int x \\cos(x) dx - (x\\sin(x)+cos(x))\\) is a constant. (See @fig-check-constant).)\n\nF1 <- antiD(x*cos(x) ~ x)\nF2 <- makeFun(x*sin(x) + cos(x) ~ x)\nslice_plot(F1(x) - F2(x) ~ x, domain(x=-5:5))\n\n\n\n\n\nFigure 38.3: ?(caption)\n\n\n\n\n\nTask: Find \\(\\int \\ln(x) dx\\).\nThe easy solution is to recognize that the anti-derivative of \\(\\ln(x)\\) is contained in the table at the top of the chapter. But let’s try doing it by parts as an example (and to show you how it got into the table in the first place).\nIt’s hard to see a separate \\(f(x)\\) and \\(g(x)\\) in the integrand \\(\\ln(x)\\). But sometimes you need to be clever. We’ll set \\(f(x) \\equiv \\ln(x)\\) and \\(g(x) \\equiv 1\\). This means that \\(G(x) = x\\). The helper function is therefore \\(h(x) = x\\ln(x)\\)\nDifferentiating the helper function gives (by the product rule): \\(\\partial_x h(x) = \\ln(x) + x \\frac{1}{x} = \\ln(x) + 1\\)\nIntegrating the differentiated helper function, we find \\[\\int \\partial_x h(x) dx = f(x)g(x) = x \\ln(x) = \\underbrace{\\int \\ln(x) dx}_\\text{The original integral} + \\underbrace{\\int 1 dx}_{x}\\] Re-arranging, we have \\[\\underbrace{\\int \\ln(x) dx}_\\text{The original integral} = x \\ln(x) - x\\ \\  =\\ \\  x\\left[\\strut \\ln(x) - 1\\right]\\]\n\n\nTask: Find \\(\\int \\sin(x) e^x dx\\).\nThis isn’t the integral of a pattern book or basic modeling function, and substitution didn’t work, so we try integration by parts.\nThe obvious choice for the two parts is \\(\\sin(x)\\) and \\(e^x\\). Both are really easy to anti-differentiate. Let’s choose \\(g(x) = \\sin(x)\\), giving \\(G(x) = -\\cos(x)\\). The re-arrangement of the original integral will be \\[\\sin(x) e^x + \\int \\cos(x) e^x dx\\] The new integral that we need to compute doesn’t look any friendlier than the original, but who knows? We’ll do \\(\\int cos(x) e^x dx\\) by parts as well and keep our fingers crossed. That integral turns out to be \\[\\int \\cos(x) e^x dx = \\cos(x) e^x - \\int \\sin(x) e^x dx\\] This may look like we’re going in circles, and maybe we are, but let’s put everything together. \\[\\underbrace{\\int \\sin(x) e^x dx}_\\text{The original problem} = \\underbrace{\\sin(x) e^x + \\cos(x) e^x}_\\text{Easy stuff!}\\ \\ \\  - \\underbrace{\\int \\sin(x) e^x dx}_\\text{Also the original problem}\\] Rearranging gives \\[\\int \\sin(x) e^x dx = \\frac{\\sin(x) e^x + \\cos(x) e^x}{2} = \\frac{e^x}{2}\\left[{\\large\\strut} \\sin(x) + \\cos(x)\\right]\\] And don’t forget the constant of integration.\n\n[The presentation of integration by parts in this section was formulated by Prof. Michael Brilleslyper.]"
  },
  {
    "objectID": "Accumulation/31-symbolic.html#didnt-work",
    "href": "Accumulation/31-symbolic.html#didnt-work",
    "title": "38  Symbolic anti-differentiation",
    "section": "38.6 Didn’t work?",
    "text": "38.6 Didn’t work?\nIf integration by parts doesn’t work … and it doesn’t always work! … there is a variety of possibilities such as asking a math professor (who has a much larger set of functions at hand than you), looking through a table of integrals (which is to say, the collective calculus diary of generations of math professors), using a computer algebra system, or using numerical integration. One of these will work.\nIf you have difficulty using u-substitution or integration by parts, you will be in the same league as the vast majority of calculus students. Think of your fellow students who master the topic in the way you think of ice dancers. It’s beautiful to watch, but you need a special talent and it hardly solves every problem. People who would fall on their face if strapped to a pair of skates have nonetheless made huge contributions in technical fields, even those that involve ice.\nProf. Kaplan once had a heart-to-heart with a 2009 Nobel-prize winner who confessed to always feeling bad and inadequate as a scientist because he had not done well in introductory calculus. It was only when he was nominated for the Nobel that he felt comfortable admitting to his “failure.” Even if you don’t master u-substitution or integration by parts, remember that you can integrate any function using easily accessible resources."
  },
  {
    "objectID": "Accumulation/31-symbolic.html#integrating-polynomials",
    "href": "Accumulation/31-symbolic.html#integrating-polynomials",
    "title": "38  Symbolic anti-differentiation",
    "section": "38.7 Integrating polynomials",
    "text": "38.7 Integrating polynomials\nOne of the most famous settings for integration comes from the physics of free fall under gravity.\nHere’s the setting. An object—a ball, let’s imagine—is being held at height \\(x_0\\). At \\(t=0\\) the ball is released. Perhaps the ball is released from a standstill in which case it’s velocity at release is \\(v_0 = v(t=0) =0\\). Or perhaps the ball has been tossed upward so that \\(v_0 > 0\\), or downward so that \\(v_0 < 0\\). Whichever it is, the initial velocity will be labelled \\(v_0\\).\nOn release, the force that held the ball steady is removed and the object moves under the influence of only one factor: gravity. The effect of gravity near the Earth’s surface is easy to describe: it accelerates the object at a constant rate of about 9.8 m/s\\(^2\\).\nAcceleration is the derivative with respect to time of velocity. Since we know acceleration, to find velocity we find an anti-derivative of acceleration: \\[v(t) = \\int -9.8\\ dt = -9.8\\ t + C\\] The constant of integration \\(C\\) is not just a formality. It has physical meaning. In this case, we see that \\(C=v(0)\\), that is, \\(C = v_0\\).\nVelocity is the derivative of position: height in this case. So height is an anti-derivative of velocity. \\[x(t) = \\int v(t) dt = \\int \\left[\\strut -9.8\\ t + v_0\\right]dt = - \\frac{9.8}{2} t^2 + v_0\\ t + C\\] Why is \\(C\\) back again? It’s a convention to use \\(C\\) to denote the constant of integration. Those experienced with this convention know, from context, that the value of \\(C\\) in the integration that produced \\(v(t)\\) has nothing to do with the value of \\(C\\) involved in the production of \\(x(t)\\). The situation is a bit like the presentation of prices in US stores: to the price of the item itself, you must always add “plus taxes.” Nobody with experience would assume that “taxes” is always the same number. It depends on the price and type of the item itself.4 You won’t have to deal with the taxes at the time you pick the item from the shelf, but eventually you’ll see them when you check out of the store. Think of \\(+\\ C\\) as meaning, “plus some number that we’ll have to deal with at some point, but not until checkout.”\nLet’s checkout the function \\(x(t)\\) now. For that, we need to figure out the value of \\(C\\). We can do that by noticing that \\(x(0) = C\\). So in the anti-differentiation producing \\(x()\\), \\(C = x_0\\) giving, altogether the formula for free-fall famous from physics classes \\[x(t) =  - \\frac{9.8}{2} t^2 + v_0\\ t + x_0\\] An important thing to notice about \\(x(t)\\): it’s a polynomial in \\(t\\). Polynomials can be birthed by successive anti-differentiations of a constant. At each anti-differentiation, each of the previous terms is promoted by one order. That is, the previous constant becomes the first order term. The previous first-order term becomes the second order term, with the division by 2 familiar from anti-differentiating \\(t\\). A previous second-order term will become the new third-order term, again with the division by 3 familiar from anti-differentiating \\(t^2\\).\nStated generally, the anti-derivative of a polynomial is\n\\[{\\Large\\int} \\left[\\strut \\underbrace{a + b t + ct^2 + \\ldots}_\\text{original polynomial}\\right] dt = \\underbrace{C + a\\,t + \\frac{b}{2} t^2 + \\frac{c}{3} t^3 + \\ldots}_\\text{new polynomial}\\] By use of the symbol \\(C\\), it’s easy to spot how the constant of integration fits in with the new polynomial. But if we were to anti-differentiate the new polynomial, we had better replace \\(C\\) with some more specific symbol to that we don’t confuse the old \\(C\\) with the one that’s going to be produced in the new anti-differentiation.\n\nIn exercise 26.16, we introduced a Taylor polynomial approximation to the gaussian function. That might have seemed like a mere exercise in high-order differentiation at the time, but there is something more important at work.\nThe gaussian is one of those functions for which the anti-derivative cannot be written exactly in terms of what the mathematicians call “elementary functions.” (See Section 38.1.) Yet integrals of the gaussian are very commonly used in science, especially in statistics where the gaussian is called the normal PDF.\nThe approach we’ve taken in this book is simply to give a name and a computer implementation of the anti-derivative of the gaussian. This is the function we’ve called \\(\\pnorm()\\) with the R computer implementation pnorm().\nWe never told you the algorithm contained in pnorm(). Nor do we really need to. We all depend on experts and specialists to design and build the computers we use. The same is true of software implementation of functions like pnorm(). And for that matter, for implementations of functions like exp(), log(), sin(), and so on. You don’t have to know about semi-conductors in order to use a computer productively, and you don’t need to know about numerical algorithms in order to use those functions.\nOne feasible algorithm for implementing \\(\\pnorm()\\) is to integrate the Taylor polynomial. It’s very easy integrate polynomials. To ensure accuracy, different Taylor polynomials can be computed for different centers, say \\(x=0\\), \\(x=1\\), \\(x=2\\), and so on.\nAnother feasible approach integrates \\(\\dnorm()\\) numerically using an advanced algorithm such as Gauss-Hermite quadrature."
  },
  {
    "objectID": "Accumulation/31-symbolic.html#exercises",
    "href": "Accumulation/31-symbolic.html#exercises",
    "title": "38  Symbolic anti-differentiation",
    "section": "38.8 Exercises",
    "text": "38.8 Exercises"
  }
]